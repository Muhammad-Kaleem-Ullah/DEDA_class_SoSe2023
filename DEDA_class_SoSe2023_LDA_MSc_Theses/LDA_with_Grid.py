import gensim
from gensim.models import CoherenceModel
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis
import matplotlib.pyplot as plt
import pandas as pd
import csv
import numpy as np

class LDA:
    '''
    A custom LDA interface designed to carry out a grid search, find the best model and vizualize it. 
    Grid search carried out per coherence instead of perplexity, as optimizing for the latter may not lead to "human interpretable topics."
    
    Args:
    
        corpus: A bag of words corpus (Already generated by CorpusMaker)
        dictionary: A gensim dictionary (Already generated by CorpusMaker)
        texts: All tokens (Already generated by CorpusMaker)
        
    How to use:
        
        MSc_LDA = LDA(corpus, dictionary, texts) <-- initializes the class
        MSc_LDA.grid_search(n_topics, alphas, betas) <-- conducts grid search
        MSc_LDA.lineplot_scores() <-- plots coherence scores from grid search rounds
        m = MSc_LDA.build_best_model() <-- fits best model
        MSc_LDA.viz() <-- vizualizes best model
        
    '''
    
    def __init__ (self, corpus, dictionary, texts):
        
        # Below the usual def __init__ items: Those selected by the user
        self.corpus = corpus
        self.dictionary = dictionary
        self.texts = texts
        
        # Below the grid search items, that will get overwritten per search iteration
        # Set to none because it gets selected by the algorithm
        self.best_params = None
        
        # Since coherence score ranges from 0 to 1, we should initialize the best score as as anything below 0
        self.best_score = -1
        
        # Set to none due to reason mentioned
        self.best_model = None
        
    def get_coherence_score(self, n, alpha, beta):
        
        '''
        Calculates the coherence score per model using gensim's CoherenceModel.
        
        Args:
            n: Number of topics in one LDA model (per iteration of search)
            alpha: Document-Topic Density
            beta: Topic-Word Density
            
        Returns:
            Coherence score, fitted LDA model.
            
        '''
        
        m = gensim.models.LdaModel(corpus = self.corpus,
                                   id2word = self.dictionary,
                                   num_topics = n,
                                   random_state = 66,  # Custom random state used in our project
                                   update_every = 1,
                                   chunksize = 100,
                                   passes = 10,
                                   alpha = alpha, # Alpha grabbed from function arguments
                                   per_word_topics = True,
                                   eta = beta # beta grabbed from function arguments
        )
        
        cm = CoherenceModel(model = m,
                            texts = self.texts,
                            corpus = self.corpus,
                            dictionary = self.dictionary,
                            coherence = 'c_v'
                           )
        
        # Returns the variables defined above
        # This comes in handy when fitting the best model after grid search
        return cm.get_coherence(), m
        
    def grid_search(self, n_topics, alphas, betas, verbose = False):
        
        '''
        Performs grid search, finding optimal LDA parameters
        
        Args:
            n_topics: list of possible topic number (can use list(range( ,)))
            alphas: list of alpha values (can use np.arange( , , ).tolist())
            betas: list of beta values (can use np.arange( , , ).tolist())
            verbose: If set to true, will give information about the number of topics, alpha and beta values and their coherence score per iteration

        Returns:
            self.scores: Coherence and perplexity scores for all LDA models tried out in grid search. 
        '''
        # Empty container for scores
        self.scores = []
        # Begin loop
        for n in n_topics:
            for alpha in alphas:
                for beta in betas:
                    
                    # Get coherence score and model
                    coherence_score, model = self.get_coherence_score(n, alpha, beta)
                    # Get perplexity score from model
                    perplexity_score = model.log_perplexity(self.corpus)
                    
                    # Append into empty container for scores
                    self.scores.append({'n_topics': n,
                                        'alpha': alpha,
                                        'beta': beta,
                                        'coherence_score': coherence_score,
                                        'perplexity_score': perplexity_score,
                       })
                    
                    # The if statement will always be valid in the first iteration
                    # but it will give the best score at the end of the run
                    
                    if coherence_score > self.best_score:
                        self.best_score = coherence_score
                        self.best_params = (n, alpha, beta)
                        self.best_model = model
                    
                    # This gives a very long print output in case of large grid search
                    # so it is only activated if verbose = True
                    if verbose:
                        print(f'Number of topics: {n}; alpha: {alpha}; beta: {beta}; Achieved coherence score: {coherence_score}')
                        
        scores_df = pd.DataFrame(self.scores)
        scores_df.to_csv('scores_from_search', index = False)
        
        
    def lineplot_scores(self):
        '''
        Constructs line plot with number of topics in LDA model on X axis and respective coherence and peplexity scores on Y axis.
        '''
  
        # Plot configuration 
        plt.style.use('seaborn-v0_8-whitegrid')
        plt.rcParams['figure.figsize'] = [12, 6.75]

        # Use list comprehension to extract the variables we need for plotting 
        topics = [n_topic['n_topics'] for n_topic in self.scores]
        coherence_scores = [score['coherence_score'] for score in self.scores]
        perplexity_scores = [score['perplexity_score'] for score in self.scores]
            
        # Initialize subplots
        fig1, ax1 = plt.subplots()
            
        ax1.set_xlabel('Number of Topics')
        ax1.set_ylabel('Coherence Score')
        ax1.plot(topics, coherence_scores)
        fig1.tight_layout()
        plt.savefig('Coherence_Scores.png', dpi = 300, transparent = True)
        plt.show()
        plt.close()
            
        fig2, ax2 = plt.subplots()       
        ax2.set_xlabel('Number of Topics')
        ax2.set_ylabel('Perplexity')
        ax2.plot(topics, perplexity_scores)
            
        fig2.tight_layout()
        plt.savefig('Perplexity_Scores.png', dpi = 300, transparent = True)        
        plt.show()
        plt.close()
        
        
        # NOTE:
        # This plotting may not be the best option given that we have many variables.
        # We could consider other options. 
        
    
    def build_best_model(self):
        
        '''
        Fits the best model found during the grid search.
        
        Returns:
            The LDA model.
        '''
        
        # A neat line of if statetement included as a flex
        if self.best_params:
            n, alpha, beta = self.best_params
            
            # We do not need the coherence score so leave first blank
            _, model = self.get_coherence_score(n, alpha, beta)
            
            # Save the model to be able to load it via Gensim later
            model.save('MSc_LDA.gensim')
            
            topics = model.print_topics()
            for t in topics:
                print(t)
                
            # Further save the topics as a csv
            
            with open('topics.csv', 'w', newline = '') as f:
                writer = csv.writer(f)
                writer.writerow(['Topic N', 'Keywords'])
                for t in topics:
                    writer.writerow(t)
                    
            return model
         
        else:
            raise Exception('No parameters found for the best model. Make sure you have run the grid search already.')

    
    def viz(self):
        
        '''
        Visualizes the optimal LDA model found by gridsearch using pyLDAvis
        
        Returns:
            The visualizations
        '''
        if self.best_model:
            pyLDAvis.enable_notebook()
            viz = gensimvis.prepare(self.best_model, self.corpus, self.dictionary)
            return viz
        else:
            raise Exception('Could not find best model. Try doing a grid search and building the best model.')
                        