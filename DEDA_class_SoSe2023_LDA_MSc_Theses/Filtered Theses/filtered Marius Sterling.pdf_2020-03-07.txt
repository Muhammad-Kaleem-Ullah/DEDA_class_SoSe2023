master science statistic joint master program master thesis forecasting stock price limit order book data deep neural network submit marius sterling student number supervise prof dr stefan lessmann prof dr wolfgang ha rdle march content list ii list iii introduction property lob preprocessing property lob data aggregation lob tick equidistant tick price transformation log return standardization log return lobdeeppp lob base deep nn price prediction model feature learn module time wise aggregation module output module application model training prediction performance regard different order book level prediction performance regard different time lag conclusion property stock return declaration academic honesty list steplogreturns standardize step log return lobdeeppp architecture sub module output dimension architecture feature learn sub module architecture temporal convolutional network tcn output module identically construct sub module ask bid price pre diction ratio target variable step ask bid return equal zero lob change event lob order book level joint mse lobdeeppp ask bid step standardize return prediction mse test dataset lobdeeppp ask bid step stan dardized return prediction joint mse lobdeeppp ask bid step standardize return prediction mse lobdeeppp ask bid step standardize return prediction sample skewness kurtosis ask bid step log return jarque bera normality test statistic ask bid step log return auto cross correlation ask bid log return ii list lob sample data provide lobster contain ask bid price volume order level amazon stock tick number trading event amazon stock mean event per second per day lob data order book level epoch layer tcn iii introduction today financial sector majority trade record electronically use limit order book lob compare rosu trader view place order contrast traditional marketplace trader obtain market quote publicize market maker field quantitative analysis mathematical finance deal statistical math ematical analysis model property time series financial asset index market series objective use observable information extract valuable actionable information latent variable future value prediction value trend distribution deviation riskiness connectedness market asset entity gain information use understand market behavior enterprise stakeholder market maker generate insight develop strategy base achieve specific target mathematical model mostly base expert generate feature model parameter tune base observation order avoid vast parameter space give availability electronic market quote generate market deploy data driven machine learn tech niques extract learn feature reasonable alternative especially since lob record multitude asset class stock future cryptocurrencies furthermore model fail grasp often irrational behavior financial time series due influence human activity order avoid adverse price effect especially institutional trader pursue less transparent order placement option order conceal trading strategy include placement iceberg order show correct order price fraction order size completely hidden order act hidden liquidity supply demand lob even set within price spread compare hautsch huang influence strategy include strategic order cancellation either shortly place shortly possible execution thus lob record outstanding buy sell order marketplace within lob order organize price level increase level ask price increase bid price decrease market price lobdatacontain order least price volume include additional order related information placement time placement id event type trader id etc therefore lob contain richer multi dimensional data price time series alone however contain data different domain prone noisy neural network nn capable handle lob data input data classification regressiontasks training data infer relationship unseen data train relationship hence nn base model reasonable choice classification regression task base lob data related research target variable depends model purpose tsantekidis qureshi price movement trend mid price predict classification paper class label represent upwards downwards stationary movement move average mid price time point analogously zhang classifies market situation bull bear market situation paper movement predict one future point time ntakaris exploit ability nn predict multiple target variable base trading strategy prediction movement class barely possible since magnitude price change predict predict mid price ask bid price regression task much complex classification task since distribution target variable mapped model therefore related paper consider regression task order predict price ntakaris predicts mid price movement multitask learn approach regression classification task prediction mid price movement suitable foresee future price trend base trading strategy predict mid price possible introduces bias contrast prediction ask bid price mid price per definition less ask price great bid price thus predict mid price underestimate ask overestimate bid price mid price loses information development liquidity situation base definition price spread ask bid price especially since price development price spread change converge diverge ask bid price mid price could constant change little hence mid price suitable overall trend prediction stock price limited value basis trading strategy paper propose nn base model self learn feature jointly predict ask bid price multiple prediction horizon second ahead due noisiness lob data especially certain order level see hautsch huang input dimension order book level time lag length include historical data understood regard influence prediction performance application compare nn model price prediction performance regard mention input dimension able compare model regard feature learn performance ability module featurelearning feature aggregation time output module propose sub module construct way output module architecture independent order book level time lag time wise feature aggregate module architecture depends time lag feature learn weight share time domain time wise aggregate follow time aggregate module section lob input data property preprocessing described nn insection themodel analyze since training propose model complex time consume performance assess fix time lag second different order book level see subsection vice versa fix order book level different time lag see subsection creategraphs com de let symbol legal reason data prohibit publish include property lob preprocessing lob data use research provide lobster lobsterdata com reconstruct lob demand historical exchange message data file provide national association security dealer automate quotation nasdaq stock exchange recon structed lob data base select stock period level detail order book level contain tick data trading hour pm banking day nasdaq stock exchange see haase huang huang polak detail lobster reconstruction algorithm level detail classifies level information granularity lob level data generally consists ask bid price last execution price level data additionally include ask bid volume various order level high level data contain information regard individual order individual submitter id order level order level ask bid ask bid lob sample data provide lobster contain ask bid price volume order level amazon stock tick level lob data provide lobster contain daily data file message file order book file see haase huang message file contains stream lob change event affected order order level small equal order book level buy sell side message file contains order book change event follow information time measure second midnight event type order id order volume order price buy sell indicator lob change event type data provide lobster submission new limit order cancellation partial deletion limit order deletion total deletion limit order execution visible limit order execution hidden limit order cross trade auction trade trading halt detailed haase huang order book file corresponds tick wise message file contain reconstruct lob order book level order book file format two dimensional matrix row contain tick wise vectorized lob data correspond message file compare format application section data amazon stock use calendar week provide lob order book level week total tick record see entire week tick change event order execution total hidden order hidden bid order property lob data financial time series return certain property call stylize fact day lob event event second number trading event amazon stock mean event per second per day lob data order book level lobdeeppp event count comparesection inmcneiletal significant sample frequency stylize fact differ slightly high frequency return compare cont cont shakeel srivastava stylize fact high frequency asset return state non stationarity due heteroscedastic behavior volatility cluster non symmetric two way price quotation skew depends way return ask bid return increase sample frequency absolute skewness increase heavy tail increase leptokurtosis increase sample frequency non gaussian distribute return become normal decrease frequency linear autocorrelated small intraday sample frequency less minute microstructure effect come play trading volume market volatility linear correlate show long memory havior compare lobato velasco amazon stock return show property mention stylize fact differ depend sample frequency compare follow part section data preprocessing described follow notation introduce order price volume order time point order level ask bid side indicator ask bid order order order level regard price increase order level ask price increase pa pa pb pb whenmentioning ask bid price price meant aggregation lob tick equidistant tick related research zhang tsantekidis tsantekidis ntakaris ntakaris price price movement predict select tickhorizon real time horizon prediction horizon measure time due prediction limited value real world trading system since time horizon prediction constant depends liquidity stock might transferable market situation see mean event per second per day application data provide compare prediction performance stock different liquidity also mislead since variability return increase increase prediction time horizon compare time per tick depends order book level since lob order book level subset lob high order book level therefore time per tick decrease high order level remedy concern data transform equidistant time step one second take lob state full second hence tick prediction equal time horizon prediction second independent liquidity order book level trans formation second tick include none multiple change event information concern single tick event order id event type lose application section amazon stock order book level use since nasdaq open trading pm tick one second aggregate lob per day per week hence value total price transformation log return since financial price process non stationary unit root therefore price prediction price level dependent common practice transform price log return log log predict contrast time series analysis single price process different option com pute log return lob price process let define lob log return log log different standardization function option pa pb return standardization lag mid price mean ask bid price thusrepresenting price change time step order level individually return standardization previous step ask bid price thus return reflect price pressure regard late ask bid price research latter standardization method use advantage ask bid return process transform single time series case therefore property step return log log equivalent sum intermediate log return hold standardization log return unconditional standard deviation step ask bid return process amazon stock target variable regression task proportional property associate wiener process independent increment normally distribute zero mean time step variance stylize fact already hint step ask bid return process generally neither normal distribute independent increment amazon stock ask bid return process exception process non symmetric heavy tail increment correlate hypothesis normality reject jarque bera test increment step length compare therefore standardize step return unconditional mean variance close zero one compare nruter snruter gol pet gol fo pet noitaived naem dradnats time step time step unconditional sample left mean right standard deviation ask bid full line step log return dash standardize step log return lobdeeppp property loss function use train model application mean square error mse mse time series process time set corre sponding prediction process since ask bid return jointly predict second ahead learn task reduce joint mse step return prediction model weight train derivative joint loss function joint mse loss equal weight average mse loss different step return process analogue prediction since unconditional variance return process increase proportionally step size loss farther return prediction weight high focus model prediction towards farther prediction ensure equal model focus target variable target variable distribution mean variance close achieve standardize target variable predict model equivalent specific weight joint mse since mse mse mse hold since step price prediction obtain standardize step return prediction current price via exp hr price prediction use synonymous standardize return prediction lobdeeppp lob base deep nn price prediction model design price prediction model base lob data challenge task due complexity dynamic within author simplify prediction task classification task prediction movement mid price assess two three way model input layer feature learn tcn output lobdeeppp architecture sub module output dimension input dimension previous layer output dimension source design class represent upwards downwards stationary movement see zhang ntakaris tsantekidis qureshi focus paper regress ask bid price propose nn base model conceptually base three sub module feature learn time wise aggregate output module see main idea split nn model feature time wise aggregate model analog idea factorization use inception module see szegedy idea factorize convolutional filter size small convolutional filter size concatenate convolutional layer transfer input data dimension time stack order mean time independent feature learn module time wise feature aggregate module aggregate time step independently create feature time output model reduces dimension time wise aggregate module output target value dimension entire model shall call lobdeeppp time lag number lag historical input data order book level batch size input dimension model last dimension represent feature channel predict step ahead ask bid price yield output dimension feature learn module training nn base model learn feature regress target variable base input data training model give data therefore generate feature base expert knowledge necessary lob data ntakaris show nn predictor model could benefit additional expert generate feature research feature self learn feature learn module input layer locconconv relu bn st conv relu bn conv relu bn conv relu bn concatenate reshape inception architecture feature learn sub module setting convolutional layer filter size dilation rate stride st provide explicitly state stride st dilation rate inception module channel dimension reduction module szegedy source design different way model input data use format use time wise stack lob data alternate price volume ask bid order increase level see zhang tsantekidis therefore disregard batch size input dimension number lag time point order book level related research convolutional layer foremost building block especially feature learn convolutional neural network cnn consist series convolutional optional pool layer follow fully connect layer reduce dimension output dimension power convolutional layer show primarily computer vision image base classification task see lecun contrast image lob data different composition since price volume ask bid order alternate therefore setup convolutional layer acknowledge heterogeneous input data hence weight share entire lob data space create mislead feature map setting filter size dilation rate stride adapt order structure previous research cenesizoglu show asymmetric influence order base ontheask bidside orderlevel etc onpricedynamics hence first input data aggregate layer locally connect convolution similar convolutional layer except filter weight distinct independently learn path configuration layer set weight share across time domain independent order book domain hence feature learn convolutional layer aggregate time point independently first input data order wise order level wise ask bid side wise independently feature wise concatenate order wise aggregate feature gain use locally connect convolutional layer filter kernel size stride feature domain time domain see follow convolutional layer base order wise feature map aggregate convolutional layer order wise feature ask bid side hence kernel size number order level dilation rate since ask bid order alternate aggregation ask bid order order level hence kernel size dilation rate anidentitymap compute filter map parallel concatenate concatenate feature map reshaped feature map size number channel reduce channel inception module concept inception module introduce szegedy use googlenet see paper setup module architecture architecture feature learn module independent time lag change different order level therefore number weight feature learn submodule identical different time lag increase increase order book level time wise aggregation module recurrent neural network rnn widely use time series data employ deep learn ing since ungated rnn prone vanish gradient remedied gate rnn feature guide long term memory forgetness long short term memory lstm nn gate recurrent unit gru see hochreiter schmidhuber cho alternative concept recurrent network architecture wavenets temporal convo lutional network tcn introduce oord lea recurrent feature consequently impact vanish gradient lea show tcns capable capture long term dependency train faster compete recurrent nn architecture bai show select prediction task tcns prediction performance comparable well rnns especially lstms tcn combine two concept fully convolutional network causal dilate convolution long introduce fully convolutional network nn building block solely convolutional layer convolutional filter generally map data point surround data point feature map time series context include data point past future unwanted usage future value know information leakage therefore input data convolutional layer one side zero pad time dimension convolution tcn left causal convolution dilation rate filter size causal zero pad right tcn residual block identity mapping source bai architecture causal information future leak past compare left tcn residual block consists two consecutive tcn layer add identity mapping compare bai tcn layer consists dilate causal convolution normalization layer activation function layer dropout layer dilate convolution aggregate low level change time follow normalization activation layer lobdeeppp batch normalization parametric rectify linear unit prelu normalization layer follow convolution layer enhance robustness boost perfor mance see ioffe szegedy batch normalization normalization activation previous layer across batch dimension shift scale feature map batch mean standard deviation close zero one alternative batch normalization layer normalization see ba normal fail model predict return zero bai use weight normalization reparameterizes weight layer neural network norm weight vector separate direction learn since complex normal ization yet implement kera explore research idea parametric rectify linear unit prelu base leaky rectify linear unit base rectify linear unit relu see maas nair hinton relu define input positive part relu max leaky relu zero negative input instead negative input multiply small fix value chosen close zero leakyrelu max min show learn value negative part multiplier yield well model performance relu activation function benefit leakyrelu value neither preselected found cross validation order improve model generalization performance activate feature propa gate dropout layer concept introduce hinton prevent activation become strongly correlate lead overfitting training data spatial dropout show improve dropout convolution see tompson since drop value randomly entire feature space drop entire feature map therefore value strong correlate one differentiate feature gate rnn gate hidden state lstm gru thusthereceptive field time lag information use forecast determine receptive field tcn compute therefore manually select tcn convolutional layer dilation rate filter size receptive field bai recommend dilation rate yield receptive field application section filter size set dilation rate follow recommend structure choosen log simplicity power two consider time lag since input dimension tcn independent order book level structure number parameter depends number time lag output module output model aggregate time wise aggregate feature target dimension regres sion task number target variable generally achieve dimension reduction layer pool inception flatten multidimensional tensor one dimension least one fully connect dense layer without activation function output layer propose output model paper follow approach small adaption due joint price prediction task ask bid multiple prediction horizon output tcn input output model due set tcn necessary extract time wise late output achieve first crop reshape input data output model see left ask bid price predict separately identically construct sub module concatenate necessary output dimension sub model architecture separate ask bid price prediction consists dense layer reduce number channel see right activation function use dense layer parametric rectify linear unit prelu use order boost performance dense layer follow batch normalization see ioffe szegedy flatten layer follow block channel reduce dense layer output sub model propagates zero log return filter dense layer due aggregation raw tick data input layer dense prelu bn dense prelu bn input layer dense prelu bn flatten crop multiplication reshape dense relu dense relu output ask output bid input substract output input concatenation output reshape left output module identically construct sub module ask bid price prediction right every rectangular represent layer sub module name purpose note left part output dimension right state explicitly layer input dimension correspond former layer output first dimension represent batch size last dimension represent number channel dimension inbetween represent feature source design second data log return zero show ratio zero log return real data ask bid log return different prediction horizon especially small prediction horizon prediction task unbalanced towards zero training nn base model dense layer without activation function output layer imbalance cause model ni snruter gol pet orez fo oitar prediction horizon ratio target variable step ask bid return equal zero lob order book level lobdeeppp property predict zero many target variable input data time seriesover couldovercome zero imbalance fully connect dense layer output would add unwanted complexity nn base model approach instead break output layer input positive negative part yield max min max max relu relu thus predict target variable substraction two fully connect dense layer relu activation function one input input prediction domain dense layer without activation function empower model predict zero return easily hence remedy zero imbalance problem nn base solution model lobdeeppp predict ask bid price step ahead output model architecture number weight independent time lag order level thus output model architecture lobdeeppp application time series model base historical return predict base past event include lobdata theinformation partial due possibility hidden order non visible buy sell strategy follow application prediction performance propose nn model lobdeeppp evaluate different input data setting order understand influence time lag order book level since training model complex time consume performance assess grid time lag order level first application number time lag fix model prediction performance assess different order book level second application order book level fix performance evaluate different time lag model training model implement kera tensorflow backend compare chollet abadi train adam optimizer introduce kingma ba loss function mean square error mse order ensure comparability model model train setting except input data dimension change variable time lag order book level data five consecutive bank day split training validation test datasets share full dataset therefore training validation test data validation dataset use certain model choice regard nn epoch learn rate dropout rate epoch learn rate adam optimizer dropout rate spatial dropout layer tcn model train epoch total step wise decrease learn rate spatial dropout rate increase see prediction performance regard different order book level noillim ni stneve gnignahc bol order book level lob change event lob order book level lobdeeppp event count related study number order level input data time lag fix since market maker allow intransparent price manipulation form hidden order iceberg order order cancellation place order shortly execution etc lob data noisy especially thebidside order level high order level meaning high order level introduce information also noise regard information gain high order book level equidistant aggregate amazon stock lob data order book level use application almost lob change event record first five order level first ten see hence majority lob change occur due lob event impact first order level application time lag fix hence data last second use prediction task model lobdeeppp train modelrepresentedbyl information pastaskand bid price due miss order information feature learn submodule slightly reduce model leave locally connect layer ask bid side aggregate esm tset order book level joint mse lobdeeppp ask bid step standardize return prediction second ahead full line represent error training dashdotted line validation dash line test dataset lobdeeppp mse visualisation convolution tcn output module model prediction performance nn model show unexpected behavior training data predict bad validation test data see effect attribute different market situation since few lob change record day validation test data trading day compare however propose model able generalize training data test data ask price place bid side see hautsch huang insight placement hidden order ob level ob level tset esm esm prediction horizon prediction horizon left askand right bidh step standardize return prediction second ahead lobdeeppp mse visualisation book level since best predict lobdeeppp model order book level bid return prediction worsens order include model employ order information predicts joint mse best consider mse individual prediction horizon second ahead time lag see standardize return prediction ask side much second order book level high prediction horizon prediction performance lobdeeppp improve slightly ask prediction substantial bid prediction second ahead bid prediction best predict model without order information longer prediction horizon model information order book level predicts best ask return prediction model stand test data prediction performance regard different time lag esm time lag joint mse lobdeeppp ask bid step standardize return prediction second ahead full line represent error training dashdotted line validation dash line test dataset lobdeeppp mse visualisation application number order level fix influence different time lag prediction performance lobdeeppp analyze filter size time lag select number tcn layer correspond layer overall performance measure joint mse see behavior previous application observe prediction performance test dataset standardize step return well validation training dataset ask prediction well bid prediction joint mse ask price best time lag bid price prediction time lag time lag tset tset esm esm prediction horizon prediction horizon mseforlobdeeppp left askand right bidh prediction respect prediction horizon lobdeeppp mse visualisation mse prediction horizon show prediction property previous application model standardize ask return prediction well bid prediction especially short term prediction second though clear conclusion regard best time lag ask bid prediction drawn except performance ask side closely grouped bid side model lag work best predict bad prediction less second ahead longer prediction horizon model performance becomes closer therefore order book level influence time lag prediction performance standardize return seem rather insignificant propose model conclusion paper nn base model specifically design sub module feature learn time wise feature aggregation dimension reduction propose order jointly predict ask bid price multiple prediction horizon base lob data feature learn module specially design complex structure lob data order wise order level wise ask bid side wise feature learn model order control model receptive field able ass influence number time lag towards prediction price model predict well market situation ask bid price change demonstrate suggest model generalizes well different market situation test dataset since lob price transform equidistant second long tick possible create trading strategy base model price prediction specific prediction horizon show training log return prediction different prediction horizon loss function mse nn focus prediction longer prediction horizon due proportionality unconditional variance prediction horizon log return log return standardize accord property prediction horizon focus equally training application influence model lob data dimension time lag order book level investigate among model time lag model order information predict prediction horizon less second ahead best beyond second test model order information order book level predict best concern ask return prediction apparent influence variable regard test setting could deduce model few input dimension train efficient therefore chosen interest research aspect research well model gener alizes test data future quickly performance deteriorates alternatively model ability use transfer learn explore thus performs stock liquidity market situation addition attention mechanism especially bid side would enhance nns ability ignore noisy information help focus valuable input feature example introduce self attention tcn dai propose acknowledgement want thank supervisor member irtg high dimensional nonstation ary time series chair information system school business economics universit zu continuous fruitful discussion helpful comment regard research abadi agarwal barham brevdo chen citro corrado davis dean devin ghemawat goodgellow harp irving isard jia jozefowicz kaiser kudlur levenberg mane monga moore mur ray olah schuster shlens steiner sutskever talwar tucker van houcke vasudevan viegas vinyals warden wattenberg wicke yu zheng tensorflow large scale machine learn heterogenous system url http www tensorflow org ba kiros hinton layer normalization arxiv stat july url http arxiv org ab arxiv bai kolter koltun empirical evaluation generic convolutional recurrent network sequence model arxiv mar url http arxiv org ab arxiv cenesizoglu dionne zhou effect limit order book price dynamic ssrn electronic journal issn doi ssrn url http www ssrn com cho van merrienboer gulcehre bahdanau bougares schwenk bengio learn phrase representation use rnn encoder decoder statistical machine transla tion arxiv stat sept url http arxiv org ab arxiv chollet kera url http kera io cont empirical property asset return stylize fact statistical issue url http rama cont perso math cnrs fr pdf empirical pdf cont volatility cluster financial market empirical fact agent base model teyssi ere kirman editor long memory economics page springer heidelberg isbn doi url http link springer com dai minciullo garattoni francesca bremond self attention temporal con volutional network long term daily living activity detection th ieee interna tional conference advanced video signal base surveillance av page taipei taiwan sept ieee isbn doi av url http ieeexplore ieee org document haase huang lobster output feb url http lobsterdata com info datastructure php access hautsch huang dark side market identify analyze hidden order placement ssrn electronic journal issn doi ssrn url http www ssrn com zhang ren sun delve deep rectifier surpass human level performance imagenet classification arxiv feb url http arxiv org ab arxiv hinton srivastava krizhevsky sutskever andr salakhutdinov improvingneural network prevent co adaptation feature detector arxiv july url http arxiv org ab arxiv hochreiter schmidhuber long short term memory neural computation nov issn doi neco url http www mitpressjournals org doi neco huang polak lobster limit order book reconstruction system ssrn elec tronic journal issn doi ssrn url http www ssrn com ioffe szegedy batch normalization accelerate deep network training reduce internal covariate shift arxiv mar url http arxiv org ab arxiv kingma ba adam method stochastic optimization arxiv jan url http arxiv org ab arxiv lea flynn vidal reiter hager temporal convolutional network action segmentation detection arxiv nov url http arxiv org ab arxiv lecun bottou bengio haffner gradient base learn apply document recognition proceeding ieee nov issn doi url http ieeexplore ieee org document lobato velasco long memory stock market trading volume journal business economic statistic oct issn doi url http www jstor org stable origin crossref long shelhamer darrell fully convolutional network semantic segmentation arxiv mar url http arxiv org ab arxiv maas hannun anda ng model th international conference machine learn atlanta georgia usa mcneil frey embrechts quantitative risk management concept technique tool princeton series finance princeton press princeton nj revise edition edition isbn oclc ocn moniz branco torgo resampling strategy imbalanced time series ieee international conference data science advanced analytics dsaa page montreal qc canada oct ieee isbn doi dsaa url http ieeexplore ieee org document nair hinton rectify linear unit improve restrict boltzmann machine pro ceedings th international conference international conference machine learn volume page haifa israel doi ntakaris magris kanniainen gabbouj iosifidis benchmark dataset mid price forecasting limit order book data machine learn method journal forecasting dec issn doi url http doi wiley com ntakaris mirone kanniainen gabbouj iosifidis feature engineering mid price prediction deep learn arxiv fin june url http arxiv org ab arxiv oord dieleman zen simonyan vinyals graf kalchbrenner senior andk kavukcuoglu wavenet arxiv sept url http arxiv org ab arxiv qureshi investigate limit order book characteristic short term price prediction machine learn approach arxiv fin dec url http arxiv org ab arxiv rosu liquidity information order driven market ssrn electronic journal issn doi ssrn url http www ssrn com shakeel srivastava stylize fact high frequency financial time series data global business review page dec issn doi url http journal sagepub com doi szegedy liu jia sermanet reed anguelov erhan vanhoucke rabinovich go deeper convolution arxiv sept url http arxiv org ab arxiv szegedy vanhoucke ioffe shlens wojna rethink inception architecture computer vision arxiv dec url http arxiv org ab arxiv tompson goroshin jain lecun bregler efficient object localization use convolutional network arxiv june url http arxiv org ab arxiv tsantekidis passalis tefas kanniainen gabbouj iosifidis forecasting stock price limit order book use convolutional neural network ieee th conference business informatics cbi page thessaloniki greece july ieee isbn doi cbi url http ieeexplore ieee org document tex id tsantekidis forecasting tsantekidis passalis tefas kanniainen gabbouj iosifidis use deep arxiv fin stat oct url http arxiv org ab arxiv zhang zohren robert deeplob deep convolutional neural network limit order book arxiv fin aug url http arxiv org ab arxiv property stock return sample skewness kurtosis amazon stock snruter snruter gol gol pet pet fo ssenwekssnruter fo sisotrukcitsitats time step time step sample left skewness right kurtosis ask bid step log return lobdeeppp property tset gol mron fo noitalerroc areb euqraj time difference time step jarque bera normality test statistic ask bid step log return hypothesis normality sample size reject significance level test statistic great lobdeeppp property snruter gol fo noitalerroc time difference full auto correlation ask bid log return lag dash cross correlation ask bid return lobdeeppp property declaration academic honesty marius sterling hereby declare previously submit present work examination write work independently source include source internet reproduce either unaltered modify form particularly source text graph image acknowledge understand violation principle result proceeding regard deception attempt deception marius sterling