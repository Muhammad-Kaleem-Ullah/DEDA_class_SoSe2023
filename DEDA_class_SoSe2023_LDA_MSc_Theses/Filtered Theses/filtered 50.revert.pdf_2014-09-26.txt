machine learn solution marketing problem recommender system master thesis advisor author prof dr ostap okhrin lix revert prof dr weining wang universit case center apply statistic economics september declaration authorship hereby confirm author master thesis independently without use others indicate resource passage literally general matter take publication resource marked lix revert september essay present marketing problematic along way solve help statistical tool precisely recommender system built boththethe oretical practical aspect use machine learn method underlined focus orient two part first define precisely problem conceptual meaning natural mathematical transla tion second provide analysis anchor business constraint top academic paper highlight make unify notation implementation work design fulfill master thesis quirements advanced statistical approach mathematical background emphasize parallel hand implementation use open source software content introduction marketing problem purpose problem brief description problem detail business condition trade offs datasets machine learn problem notation mathematical problem methodology train test set sample sample error diversity measurement naive method machine learn technique abc recommender system naive bayes linear discriminent analysis near neighbor decision tree multivariate probit method cluster mean support vector machine compare method implementation result data analysis preprocessing result preliminary read chart sample result sample result conclusion room improvement introduction field machine learn boom last decade computer increase capacity rise amount data store machine learn proven acute tool understand structure dataset predict accurately recom mender system explain section us statistical analysis somehow rank product order recommend one top list relevant define purpose currently one many application machine learn still far know everything one know variety data col lected nowadays many company indeed like walmart amazon use technology generate personalize recommendation client may colossal number way matter fact two company use different algorithm since people develop different datasets different well work also provide unicity intend follow exact process company solve problem first description issue second method tackle third final solution remark therefore plan work present different chronological phase process first part discus detail problematics topic sec ond part enumerate exist technique machine learn analyse regard problem last part present result algorithm implement investigate obtain result conclude work marketing problem purpose problem brief description german supermarket wish use new concept customer consists machine printing voucher work connection loyalty card type machine already exists yet concept lie computer system chooses smartly product print coupon give customer precisely aim select product way would increase size customer basket therefore goal recommend product interfere usual customer pur chase shift customer habit extend underlie question arise idea instance incentive cu tomers buy new product would lie price reduction offer one might suppose discount limit price specific product exists every person limit stand turn point customer start purchasing product discount price limit likely differ person another another concern arises discount give product un need certainly result purchase word printing purchase would induced important printing coupon product customer already intention buy result loss supermarket value amount discount consequence idea create personalize coupon unveils com plication concept seem right first place one must careful undesirable consequence lead also incentive use machine learn address problem since provide wide range method tackle type problem problem detail underlined previously idea present different issue concern selection product present customer amount discount one product relative customer obviously two issue link let face problem another way suppose dataset contains tory purchase one customer know exactly retailer bought product price time give one voucher customer product discount pick know exactly product buy supermarket price therefore make sense select product already customer history purchase maximize chance customer use coupon would choose discount make price bit low use supermarket buy product question question select good product simple answer choos ing right amount discount turn voucher purchase tougher one furthermore discount see psychological incentive pick right amount much harder grasp math ematical approach pick product dataset similar fictive one present face exact question product discount problem choose discount complex methodology use fo cused selection product discount still important extends problem far beyond field statistic work suppose already method discounting product recommend practice amount discount would every product pinpoint recommendation method first let define bit entity work retailer supermarket want implement personalize recommendation system retailer provide loyalty card data usual behavior customer retailer shopping restrict retailer ideal goal recommender system shift pur chase customer retailer represent competitor retailer germany data provider provide data history purchase panel household germany discus later core data say earlier aim recommender system make customer purchase product top usual basket product recommend know customer instance different recommender system one netflix amazon number recommendation also restrict practice coupon print per customer per day use terminology recom mender system system built business condition work do business environment actually closely work company wish remain anonymous hand part work last one full month previous meeting prepared ground aim thus create could use directly super market trade offs every machine learn application one determine first degree accuracy need relative factor time relevant factor take account time print coupon customer also time need algorithm train ready case customer generally wait long machine therefore algorithm able provide coupon within second second limit fix project demand context second would limit time training algo rithm however flexible condition one month period overtaken another crucial point diversity recommendation ideally machine would never print twice coupon two dif ferent customer reality unachievable idea would large diversity within coupon constitutes legiti macy machine learn change old marketing method focus product advertising campains target method focus di rectly customer diverse coupon personalize method finally algorithm interpretable error occurs simple enough give why wherefore happen act like black box also scalable dataset grow long run however may necessary study point datasets two datasets available work loyalty card data retailer panel data represent history purchase household germany one year datasets row state one purchase column customerid retailerid forthe panel data product id product category product brand quantity value next present concrete role predict whether product purchase another retailer retailer recommend product recommendating product consist select product high probability ideally would know recommendation product customer buy would make show green arrow real world predict product probability data customer prod bought somewhere esle ucts bought retailer tailer recommend one nothing else high probability machine learn problem marketing problem translate concrete mathematical problem constitute cornerstone work notation ing algorithm compare attribute symbol object data seem clear need make change form modification lead less information origi nally dataset one still create feature information mathematical problem derive follow notation de scribe data contains different product available retailer tailers whole household customer panel data denotes matrix dummy variable resp equal customer purchase product retailer resp retailer zero else nx tobecorrect nx isthenumber different loyalty card nx matrix dummy variable nx equal customer nx th loyalty card purchase product retailer zero else torecap andy fromthepaneldata andone matrix loyalty card data closely related since represent purchase retailer idea predict predict result latter know exactly therefore training model give good prediction unfortunately database true purchase customer loyalty card discus issue within section methodology mathematical problem developed previously challenge predict accurately proba bility give customer purchase product supermarket retailer precede notation pose problem fol low customer nx write th row inter ested evaluate probability customer purchase product problem note rewrite formula do every product write estimation simultaneously vector formula thus first recommendation customer would simply reduce maximization program argmax recommendation make product never bought retailer equation recommendation remove sequentially maximum time case finally coupon recommendation represent notation chosen remind feature set actually contain information combine instance one could include number retailer customer fact product milk product product feature feature likely algorithm accurate however might also lead overfitting something discus later problem clearly pose develop methodology use similar one generally meet machine learn problem methodology train test set say previously dataset corresponds purchase ismissing usual train test set separation fold cross validation train validation test set separation choice make work arbitrary convenient use simple train test cut data kept training set test set convenient due simplicity test phase one run test set give result whereas fold cross validation instance require time test phase cut train validation test often make determine parameter validation set test method parameter test set method correspond type process simple train test seem fair convenient train test cut make pseudo randomly command set seed place computation therefore train traintraintest trai ntest test test train sdlohesuoh train traintraintest tra itnest test test test product product illustration split initial dataset training set test set always get train test separation every algorithm every rerun sample sample error come implement statisti cal model data usual real value one find compare train test raise question compare probability dummy variable usually difference bound say whether purchase estimation step interest first product relevant error mesure need use previous notation state coupon rec ommendations give customer write rate error rate good guess formula suggests implicitly product recommend ex pected purchase somewhere else prediction product count error also splitting datasets two set lead rewrite train train therefore derive sample error rate error train sample error train rate error test research paper last error test call pseudo sample still know value since test data real sample error would write know true purchase customer loyalty card diversity measurement another aspect recommendation diversity point discuss previously define measure diversity algorithm recommendation compute training set test set notation define measure training set also use test set rate diversity proportion different coupon card train train train order within recommendation matter two coupon similar permutation count twice cardinal another way measure diversity count number time product recommend meanwhile see single value array make sense plot chart two measure study detail section result naive method course implement complex algorithm expect good sults benchmark say whether algorithm good turn possible way select product without use statistical method first idea one pick randomly customer prod ucts purchase retailer however method fails since customer usually purchase slender minority total amount product diversity contrary would high still good accuracy prevails good diversity another idea would rank product number time purchase retailer retailer accord product customer purchase retailer recommendation consist select best sell product reason intelligible face context long tail phenomenon product mean item make majority sale method give good result see diversity within coupon low best sell product recommend call strategy top another method would consider best sell product tailer time corresponds less idea predict purchase retailer top strategy naive method prove surprisingly good aim would perform benchmark use work top strategy see next section strategy turn relatively accurate machine learn technique abc recommender system wide variety machine learn technique exists literature prove efficient many case usually method work well particular case case method versatile however ability adapt often lead less accuracy furthermore data cleaning often responsible good result whatever model use step preprocessing paramount get expect accuracy whenever machine learn seem good solution problem one careful use good method precise prior preprocessing step enlarge debate comparison model unveils sensible aspect machine learn complex model necessarily outperform simple one first describe depth researcher understand term recommender system quote frequently usu ally one four branch field machine learn three branch supervise learn sl unsupervised learn ul reinforcement learn rl particular way combination previous aspect people may order preference within set product unlikely exactly know anyone therefore try estimate preference exist genuine set order preference never know locates simply sl ul reinforcement learn also use often explicit feedback recommend product rating comment implicit one purchase click practice many different way recommend product cu tomers non exhaustive list method present collaborative filter us customer past behavior often first choice forecasting two different method make recommendation user base focus look user similar basket purchase see easily matrix hav ing customer row product column rep resentation method look similarity row customer basket product base focus look product similar pattern sale history previous illustration method look similarity column product sale history demographic put interest customer feature instance age sex affinity bio product method cluster customer order make specific recommendation within cluster content base put interest product feature instance product dairy diet bio method cluster product order make recommendation one small number cluster product social connect customer social circle purchase method lie particular database work available data would push use collaborative filter cf method time another way would build matrix product feature creat ing manually feature like dairy quantity vitamin fill product correspond value improve database add feature help get well result instance since one month period already short address issue go straightforward cf method experience netflix prize large number cf method popular focus subset goal estimate accurately vector probability purchase product retailer probability compute step process otherwise complexity high probability may comparable since method compute different thus wish find method compute similarly probability personalize possible naive bayes naive bayes method widespread method us famous bayes formula notation developed previous formula becomes customer product probability customer represent purchase product retailer similarly probability customer purchase product retailer test data simply row unknown vector purchase row correspond customer continue use notation instead illustrate real implementation therefore naive bayes particular method supposes every feature independent others call naive tricky part calculation us term previous equation one rewrite term respect independence condition mean probability customer certain behavior retailer know purchase product another retailer simply product probability individual behavior product retailer term product simple count matrix order compare take logarithm ratio call log log log log obtain vector customer remain task get high know course linear discriminent analysis lda method us assumption independence sup pose however another hypothesis probability pur gaussian mathematical term give multivariate density know multivariate density know prior probability prior probability exp formula first sight quite complex simply state give product probability purchase somewhere else retailer gaussian form linear discriminent compute follow log log clear context work many parameter estimate parameter product model massively overfit since customer product one possible way adapt method would reduce significantly number product low number parameter still product near neighbor assimpleasitis thecoreofthemethod lie fact point data relevance others predict certain us simple idea similar one histogram group data bin predict average bin bin neighborhood size mean give one find neighborhood data predict class purchase purchase dominant neighborhood practice probability purchase product simply sample probability neighborhood therefore near neighbor customer formula really straightforward caveat construct good way find neighbor eventually choose good last point easy continuous variable usual similarity function sample cor relation kendall rank correlation variable discrete similarity function much delicate build func tions cite usually work well similarity function select way find use call cross validation method method attractive practitioner super fast prove quite accurate similarity function chosen smartly also possible use knn method product side decision tree decision tree popular application interpretability paramount medical purpose instance case method could use give probability prediction one small tree create product method could provide quickly vector probability subject make tree may take time usually tree define decision node reach end node aim build short tree predict probability quickly main problem method face tree different accuracy might turn recommendation make rely poor prediction another way use method could build large tree pro vides end node vector probability suppose tree end node different coupon ever give customer therefore provide accuracy diversity tree must sufficiently large might overfit multivariate probit multivariate probit model us fact observe purchase binary like lda method several parameter estimate explanatory variable would purchase retailer explain variable latent variable model null vector length covariance matrix model estimate maximum likelihood developed soft ware provide built likelihood maximization program model turn simple gaussian copula model copula variable marginal distribution model complex copula could consider method cluster divide initial dataset several cluster generally repre sent data correctly cluster method might use customer well product usually one feature back cluster method feature necesarily stand data one want predict instance might relevant use fact product dairy cluster even role final probability computation problem base estimate vector probability length one wish use general cluster method divide data number different class issue loom behind number dif ferent class product class therefore class number way large number customer thus cluster method intend separate class help get structure data last remark make one aim case create personalize recommendation term coupon would ideally different one customer another mapping data cluster gather customer ensemble recommendation make cluster real lack diversity coupon therefore one cluster method use another method apply top former make recom mendations unique mean mean algorithm quite famous field unsupervised learn since one fast way compute cluster data principle method number cluster set algorithm build sequentially cluster parallel center convergence method may simple compute yet underlie motivation choose less clear one cluster might work well though another might perform poorly even center chosen randomly almost chance retrieve exact result twice make good use algorithm require many try lessens fact fast support vector machine method strongly support many paper prove fective case also cluster method supervise learn meaning class know map data like web tends divide class large margin issue class problem therefore method pretend represent every class individually map data give probability purchase time seem clever way consequently method use separate data sev eral cluster chosen smart way instance separate group together one purchase specific product retailer use method perform many blind attempt might take lot time find smart separation compare method plethora method provide real variety statistical technique lie data like knn nb underline main difference adapt adjust low many degree highly vector complexity parameter freedom multivariate knn lda tree nb svm probit adjust vector many parameter simply avoid estimation error low complexity train test degree freedom mean algorithm produce coupon way improve without change completely algorithm touch data adapt highly multivariate point algorithm give relevant result even product customer knn naive bayes mean stand best candidate next section implementation result data analysis preprocessing part described different preprocessing step induced data analysis dataset generate loyalty card kept algorithm built term customer household state unique entity people panel data remain true say otherwise section use general matrix notation customer row product column implicitly notation surmise product offer retailer practice true actually ten thousand different product id dataset interest plot revenue generate sale product show first rational thing remove long tail denotes seasonal product special offer simply scarce product sell minority retailer remove heavy tail interest look retailer revenue also display long tail phenomenon cf discrepancy retailer revenue also due outlier online retailer retailer primarily focus consumer good related food another step preprocessing thus need also encourage company provide data developed special taxonomy product namely recommender id fusion product category brand idea different product id state product category brand example customer purchase small bottle soda retailer good chance know large bottle soda brand therefore value add recommend large bottle since customer could already thought buying moreover revenue generate per product log log scale red oval stress long tail exist set product revenue generate per retailer coupon suggests small large bottle two distinct recommendation maybe would well simply recommend bottle small large another product fusion product category example bottle soda brand seem define clearly recommendation term product lose connotation size format simply refer term recommender id another necessary preprocessing carry keep product available retailer retailer whole select customer purchase least retailer keep household customer retailer even regular one help understand different behavior come retailer retailer plot revenue generate product retailer retailer show chart particularly first one famous law large number appear zipf law log log scale reveals liner relationship indeed word book ulysse james joyce word replace product occurrence sale revenue book replace first chart retailer second retailer aggregate therefore make sense notice almost straight line first chart since one retailer assimilate one book curve line second chart since aggregate retailer different certainly manifest homogeneity set product ranked sale revenue step preprocessing remove possible outlier prod ucts retailer customer long tail phenomenon present three entity induces outlier generally long tail take account data tough choice make keep long tail good accuracy fast algorithm still interest look remains data severe clean ing process customer one purchase least retailer product recommender id common retailer retailer whole retailer acutal competitor retailer latter retailer medium size face cutthroat competition hard discount retailer gather household expense order grasp new datasets display respec revenue generate per revenue generate per product log log scale retailer product log log scale retailer tively number different product bought retailer per customer sale revenue per product also retailer red grey line plot chart stand respectively tail tail noticeable long tail still strong customer side product side may pose problem algorithm since long tail might give less relevant information confuse logic within algorithm complete priliminary data preprocessing necessary know notation developped section line new data actually data never transform matrix sufficient build two contigency one purchase retailer one purchase everywhere else however see huge loss information transform contigency matrix dummy quantity lose becomes boolean variable time project would focus try method use quantity matrix notation require consideration fact notation reveals purchase number different product bought total retailer per customer sale revenue per product retailer customer also stress non purchase zero value matrix new information absence purchase actually wider previous information presence purchase fill matrix raft zero lead sparse data constitutes serious issue reason concern one say whether absence purchase mean customer like product chance buy customer may bought product even product corre spond taste budget multiple possible reason timeframe one year case small capture purchase customer send information data provider incomplete data collection simply retailer retailer display different product set product available retailer en tirely available retailer individually one challenge machine learn problem understand point sparsity disturb algorithm choose method suffer less sparsity issue indeed sparsity met many case instance make choice restrict selection algorithm resilience sparsity instead let tail factor possibility varry order compare performences algorithm different situation sparsity grasp issue display sparsity respec tively tail factor account customer number cust different purchase retailer tail factor account prod product sale revenue retailer remove tail retailer necessarily remove tail retailer result preliminary possible choice machine learn method actually test implementation top strategy constitutes also add method use na bayes mean two type near neaighbors first test training set appart sparsity relative proportion tail keep set customer product retailer sparsity relative percentage tail keep set customer product retailer mean implement test set prior discussion final choice tail factor concern product regard tail factor customer seem adequate make different indeed algorithm use available data customer product could see number variate within problem reduce size gain accuracy caveat implementation side algorithm write appartformk mean thisimpliesthat time spent check confirm validity algorithm time allow implement several change easily instance distance function within near neighbor method read chart result displayed algorithm test training set change two tail factor cust prod increment time total try make represent matrix keep tail tune parameter help understand efficiency method sparsity data varies show use well know contour plot help visualize data three dimension two dimension color red indicates low number light yellow stand high number plot accuracy target light yellow area good prediction display run time complexity target red area low complexity moreover diversity proportion unique coupon would also focus light yellow area high diversity unit first third chart ratio denote rate second second chart black digit indicates average value color area accuracy chart sample result top recommend best sell product top strategy benchmark machine learn algorithm beat fairly simple code idea sort num liat liat morf morf tpek tpek stcudorp liat stcudorp customer kept tail customer kept tail rate good prediction run time complexity top algorithm top algorithm morf tpek stcudorp customer kept tail diversity coupon top algorithm ber sale per product retailer retailer decrease order recommend best sell product pur chase customer retailer algorithm fast complexity serf low bound algorithm contrary accuracy diversity challenged method look result algorithm lead several interest point whatever tail factor accuracy remains stable quite impressive since product method actually performs well might prove difficult beat diversity decrease number customer quite straightforward since coupon similar since best sell product change method turn accurate number different coupon give actually low thus personalize expect neareast neighbor many package provide near neighbor algorithm basic class package instance however thorough analysis would tend rely user make knn datasets vary significantly one application another performance knn may great one case terrible another one make knn could input different distance function customer actually two knn namely knn knn imple mented rely precisely two different distance function similarity func tion since use find optimal number neighbor prove hard use cross validation large datasets involve therefore use advise company provide data course binary variable might best choice distance function yet first try reasonable choice since fast liat liat morf morf tpek tpek stcudorp liat stcudorp customer kept tail customer kept tail run time complexity knn algorithm knn algorithm morf tpek stcudorp customer kept tail diversity coupon knn algorithm compute correlation still make sense even seem best choice result knn algorithm arouse different comment performance bad top around true everywhere suggest model good enough performance increase number customer point actually data accurate algorithm example high variance algorithm data strong impact performance diversity decrease number customer increase effect look result top algorithm surprise therefore room another implementation knn algorithm since performance poor two possibility offer change number neighbor change distance function sample correlation function say distance function appropriate binary variable furthermore seem way built knn find similarity customer give weight dominant data algorithm simply cluster customer purchase point relevant try build distance would focus mainly order group customer purchase interest information simple distance would give customer compute similarity inpractice distance function really distance since similarity function count number common product bought customer customer high number common product purchase reference customer neighbor similarity function lead knn algorithm particularly impressive result theknn especiallythe top benchmark remarkably us fact much liat liat morf morf tpek tpek stcudorp liat stcudorp customer kept tail customer kept tail run time complexity knn algorithm knn algorithm morf tpek stcudorp customer kept tail diversity coupon knn algorithm value look basket similarity customer similarity whole set product knn performance increase number customer remark knn fact algorithm performs well another change high variance attribute data lead well accuracy diversity drop customer long tail present without doubt due fact customer long tail bought much product lead accurate neighborhood border fact customer might purchase kind product instance staple product regular customer retailer induce coupon print run time complexity rise compare two previous algorithm remain reasonable bound naive bayes naive bayes algorithm built scratch explain section run time complexity spiral tail factor increase one main problem nb algorithm try sample nb turn take much time run significantly reduce try sample complete output matrix compute algorithm try actually one algorithm try different datasets different tail factor result reveal different point time complexity high legend different previous chart indeed computation single predic train tion use notation described therefore sample error take compute train low accuracy good interest since prod suggests method work product significant number actually number matter balance information well balance least tipped one side probability compute liat liat morf morf liat liat tpek morf tpek morf tpek tpek stcudorp liat stcudorp liat stcudorp stcudorp cu mers kep fr om tail cu mers kep fr om tail customer kept tail customer kept tail run time complexity naive bayes algorithm naive bayes algorithm morf tpek morf tpek stcudorp stcudorp cu mers kep fr om tail customer kept tail diversity coupon naive bayes algorithm reasonable sample size therefore pertinent conditional probability inclined near unbalance overall probability product conditional probability low rise number customer increase prod cust accuracy fact feature purchase product pertinent stick half product purchase eachcustomer spurchasebrings significance determine real density probability purchase give product low rise number product blind algo cust prod rithm simply opposite last two point product tail tend bring sparsity bias conditional probabili tie therefore bias probability less accurate algorithm sum naive bayes algorithm give good performance relevant product contrary algorithm explodes number product increase due fact number product tail overwhelm number understand well algorithm different performance accord tail factor compelling look plot density log likelihood function recommendation within coupon chart correspond best nb cust prod mean ratio probability purchase probability purchase reasonable value however second chart ratio start bump last chart completely explodes last case nb recommends product never bought retailer retailer concrete probability would zero add one smooth show limit naive bayes help understand implementation nb work recommendation naive bayes cust prod loglikelihood recommendation naive bayes cust prod loglikelihood recommendation naive bayes cust prod loglikelihood ecnairav ecnairav denialpxe denialpxe mean thek infunc tion would use combination nn algorithm instead find ing neighbor across dataset idea find neighbor within one cluster customer cluster would found use near neighbor algorithm across point built mean algorithm mean method always return partition cluster necesarily mean cluster good indeed initial position center induce different result time use repeat time abitrary number validate empirically random start algorithm give easily manage since iteration already built function parameter nstart set top since optimal unknown flexible begin plot explain variance cluster axis function number center axis chart reveals real inaccuracy mean algorithm problem proportion explain variance start straight meaning real improvement point proportion explain variance around indicates data explain cluster sufficently low urge stop dig direction actually issue doubt encounter building first nn algorithm distance function pertinency sample result section serf final phase process building machine learn algorithm building model train specific sample test unknown dataset test set observe actually similar happen training set top strategy performs would strange otherwise knn get poor performance knn good naive bayes infers consequence regard different tail factor display rate good prediction algorithm high performance knn nb remain goal achieve outperform benchmark significant improvement knn nb high diversity teresting chart plot number time product recommend per product algorithm stagger see even machine learn algorithm give diversity recommendation seem provide limited number different recommendation customer also interest see two knn choice product make difference accuracy seem recommend product way red green line yet product recommend different therefore distance function seem play role choice product overall process recommendation diversity lead knn algorithm seem exogenous endogenous sug gests possible improvement diversity use algorithm naive bayes algorithm provide contrary large diversity rec ommendations say long tail recommendation method give fairly similar diversity main product knn top yet develops long tail might infer serendipity often advocate machine learn practitioner liat liat morf morf liat tpek tpek morf tpek stcudorp stcudorp liat stcudorp cu st mers kep fr om tail customer kept tail customer kept tail top algorithm naive bayes algorithm liat morf morf tpek tpek stcudorp stcudorp customer kept tail customer kept tail knn algorithm knn algorithm top knn knn naive bayes product diversity recommendation sample top knn knn naive bayes product diversity recommendation sample conclusion room improve ments recommend product customer challenge purpose machine learn scientist require deep knwoledge available technique order avoid face dead end moreover computer skill highly valuable gain time obtain result short demand utmost interest statistician work algorithm actually test still many different technique could also useful problem section describes non exhaustive list machine learn tool apply fit problem others simple manipulation data could make preceeding statement wrong infinite possibility attack problem work try present objective analysis data available tool modest yet thorough way solve improvement could apply directly algorithm test work give algorithm specific line refinement enlarge discussion open method knn algorithm perform well second similarity function first one suggests choice distance make final result consequence could many possibility refining second similarity function instance customer buy much retailer neighborhood good chance contain many neighbor restrict neighbor chooses abritrarily might efficient choice way solve could select neighbor number purchase retailer similar number purchase customer also neighbor different behavior retailer algorithm would predict well suggest use top method case seem safer recommendation regard naive bayes implementation different suggestion come mind one main drawback time complexity top parallel computation found would shrewder compute conditional probability separately therefore calculation time complexity would lie rapidity retrieve data much facter compute everything another issue majority product brings sparsity completely blind algorithm way tackle implementation cut tail unfortunately see smart way trick evoke possible refinement could cite use linear blend previous algorithm improve accuracy add complexity interest use decision tree model order understand somehow structure data addition predict purchase also consider time data would great asset takethatintoaccount nothing maybe customer buying somewhere else therefore recommendation product would key asset keep make customer purchase product many discussion arise end question efficiency know coupon work somebody may accurate coupon infer purchase know knn nb seem good enough proceed anoth question customer purchase recommend product next coupon remain accurate customer purchase lot retailer value add problem seem address customer purchase irregularly retailer finally similar developed competitor would lead cutthroat price competition complete focus product high chance purchase another retailer also suggest customer variety product similar customer purchase retailer example bibliography xavier amatriain recommender system mls pittsburgh david barber bayesian reason machine learn cambridge press beel langer genzmehr gipp rnberger compar ative analysis offline online evaluation discussion research paper recommender system evaluation acm editor repsys pro ceedings international workshop reproducibility replication recommender system evaluation page david hand classifier technology illusion progress statis tical science june trevor hastie robert tibshirani jerome friedman element statistical learn springer dietmar jannach markus zanker alexander felfernig gerhard friedrich recommender system introduction cambridge univer sity press kevin murphy machine learn probabilistic perspective mit press ricci rokach shapira kantor recommender system handbook springer martin robillard walid maalej robert walker thomas zim mermann recommendation system software engineering springer