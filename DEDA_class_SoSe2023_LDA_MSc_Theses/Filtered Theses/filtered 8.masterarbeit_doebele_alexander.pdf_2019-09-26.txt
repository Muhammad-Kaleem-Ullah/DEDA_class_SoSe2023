blockchain sentiment sector analysis lstm approach master thesis submit prof cathy yi hsuan chen universit zu school business economics institute statistic econometrics chair econometrics dr jens kolbe technische universit faculty vii economics business chair statistic alexander obele partial fulfillment requirement degree master science july inthisthesis blockchain news article depth look model specification choice different model parameter give around article april april gather business section coindesk use dynamic web scraper various method like smote oversampling introduce account imbalanced training dataset model performance compare svm model could show lstm well suit sentiment analysis superior model long term semantic dependency text data furthermore static contemporaneous regression employ explain relationship blockchain sentiment return different sector identify promising sector strong tie blockchain news sentiment regression however hypothesis blockchain sentiment today increase conditional volatility return tomorrow could establish specific sector implement garch model zusammenfassung die folgende arbeit besch aftigt sich mit einer machine learn applikation au dem bereich der neuronalen netze ein lstm model wird genutzt um eine sentiment analyse im bere ich blockchain nachrichten durchzufu hren dabei wird dem leser ein tieferer einblick die model spezifikationen und wahl der parameter gegeben wurden insgesamt ca artikel im zeitraum von april bi april von coindesk mit hilfe eines dy namischen web scraper gesammelt au erdem werden verschiedene methoden vorgestellt die strukturelle probleme im training datensatz wie beispielsweise imbalanced data heben sollen zu diesem zweck wird da konzept de smote oversamplings vorgestellt anschlie end werden die ergebnisse de lstm model mit den ergebnissen eines anderen model au dem bereich machine learn svm verglichen konnte gezeigt werden das sich lstm fu sentiment analysen besser eignet da der lage ist abh angigkeiten ber einenl regression vorgestellt die den zusammenhang zwischen den gesch atzten sentiment indizes und dem ertrag verschiedener sektoren erkl soll nachdem die vielversprechensten sektoren mit st arkeren einflu ssen au dem bereich blockchain identifiziert wurden konnte keine signifikante beziehung zwischen ertr agen au sektoren und blockchain sentiment etabliert werde jedoch konnte die hypothese das blockchain sentiment heute die bedingte varianz sektorertr agen morgen beeinflusst mit hilfe eines garch model best atigt wer den ii content list abbreviation list vi list viii introduction overview contribution thesis content sentiment analysis type sentiment analysis approach sentiment analysis deep learn method algorithm deep neural network dnns architecture convolution neural network cnn recurrent neural network rnns long short term memory lstm support vector machine svm lstm method apply sentiment prediction algorithm setup sentiment prediction model training training data challenge training process confusion matrix news data sentiment construction coindesk preprocessing text data sentiment index construction iii application sentiment index sector data preprocessing farma control variable static contemporaneous regression generalize autoregressive conditional heteroskedasticity garch model conclusion discussion iv list abbreviation nlp natural language processing dnn deep neural network rnn recurrent neural network lstm long short term memory btc bitcoin svm support vector machine relu rectify linear unit cnn convolutional neural network ffnn feedforward neural network gru gate recurrent unit cbow continuous bag word omx office max html hyper text markup language bleu bilingual evaluation understudy sgd stochastic gradient descent smote synthetic minority oversampling technique list confusion matrix base binary classification problem chawla fully connect neural network three input layer one output layer four hidden layer kim illustration hidden layer recurrent neural network salehinejad general architecture rnn nasekin chen structure lstm unit nasekin chen structure gru unit nasekin chen comparison one hot encode word embeddings yin architecture word vec neural model nasekin chen word frequency malo training data agreement rate amongst annotator left standard nn two hidden layer right thin net produce apply dropout cross unit drop srivastava validation training loss differently specify lstm unit concern ing dropout rate distribution malo training data smote oversampling minority class chawla number daily article buisness section coindesk march mai monthly aggregate sentiment estimate monthly aggregate sentiment number article coindesk estimate monthly aggregate sentiment return financial sector period daily return finance sector period estimate monthly aggregate sentiment return energy sector period estimate monthly aggregate sentiment return information technology sector period vi daily return communication sector period daily return consumer discretionary sector period daily return information technology sector period daily return industrial sector period vii list parameter setup lstm peter nagy distribution label phrase bank four subset form base strength majority agreement malo performance metric differently specify lstm model svm model see bommes svm result base training data malo example processing sentence coindesk news static regression result finance sector static regression result information technology sector static regression result energy sector estimate coefficient garch model base daily financial return estimate coefficient garch model base daily return interannotator agreement statistic base subset sentence tag annotator malo performance metric different agreement rate amongst annotator equalliy specify lstm network viii introduction overview last year extent unstructured text data overall internet flourish rapidly therefore demand process extract helpful information crease accordingly every sentence play essential part determine opinion individuum towards particular topic compose semantics single word form expression whole meaning yin interest different technology natural language processing nlp like sentiment analysis social medium gathering knowledge big unstructured data internet increase drastically society relies nlp rise accordingly vast majority nlp problem lead shallow machine learn method focus depth feature engineering renaissance deep neural network dnn less artificial feature yin neural network architecture elevate space recurrent neural network rnn rnns special kind neural network take sequential input produce follow output share parameter time step mikolov benefit rnns show useful natural language processing socher image caption mao speech recognition graf contribution thesis focus one specific field nlp sentence base sentiment analysis opinion mining context blockchain news accord aboody sentiment play crucial role determine price evolution give possible arbitrage op portunity intangible fundamental value therefore article coindesk last six year gather dynamic web scraper blockchain tech nology open ledger coordination record keep irrevocability transaction swan gain traction past couple year due spike price popular application bitcoin carry task specific form rnn longterm short term memory lstm use lstms proven useful model word sequence without assume word order independence powerful learn data long range temporal dependency zhou different perfor mance measure lstm machine learn approach svm analyze compare result estimate sentiment index use determine effect change sentiment towards blockchain technology return conditional variance different sector industry bitcoin nakamoto recently introduce blockchain broader audience one might expect significant influence sector already imply technology planning thesis content remain document organize follow chapter introduces basic concept field sentiment analysis different type concern level analysis present follow short overlook approach sentiment analysis like lexical analysis machine learn analysis chapter introduces reader deep learn method algorithm basic struc ture well advantage disadvantage commonly use neural network show focus recurrent neural network lstm particular architecture lstm outline importance word embeddings emphasise afterwards lstm algorithm setup show detail reason behind choice particular parameter give necessary step training process highlight training dataset introduce problem come model training identify possible solution give chapter concludes confusion matrix show performance result lstm model compare result svm model chapter dataset use sentiment prediction outline method gather data coindesk dynamic web scraper present neces sary preprocessing data described method construct sentiment index base give data show chapter introduces possible application con structed sentiment index first sector data preprocessed control variable isolate effect sentiment sector return introduce afterwards static con temporaneous regression model use estimate impact blockchain sentiment sector return finally influence sentiment conditional volatility return sector analyse use garch model chapter concludes document summarize main point present direction future work sentiment analysis chapter provide overview related concept field sentiment analysis first part focus different type sentiment analysis illustrates critical step process second part two classical approach sentiment analysis intro duced various metric give evaluate performance sentiment analysis area natural language processing nlp determine opinion attitude author towards particular topic rise popularity blog social network opinion mining sentiment analysis become field interest many type research comprehensive overview current work present pang sentiment describes opinion attitude declare individual opinion holder entity target scheible accord liu opinion two key component target sentiment target target entity aspect entity opinion express degree direction sentiment positive neutral negative know sentiment polarity common polarity design focus two category positiveandnegative scale sentiment polarity define way represent rating mechanism found online thumb youtube facebook rating star tripadvisor amazon imdb positive neutral negative ebay however every article comment specific topic express entirely positive negative sentiment koppel schler state importance neutral sentiment additional information gain accounting polarity often use interval perfect negative sentiment perfect positive sentiment accord scale describes perfect neutral sentiment although neutral sentiment carry information literature defines task two category problem dave type sentiment analysis sentiment classification arguably widely study topic field sentiment analysis blitzer aue gamon chesley themostnoticeable difference lay level granularity tripathy document level analysis whether whole document express positive negative sentiment sentence level analysis determines whether sentence express negative posi tive neutral opinion aspect base analysis mainly concerned particular aspect topic product document level sentiment analysis assumes opinionated document express opinion single target opinion belong single person sadegh assumption hold customer review product commonly focus one prod uct write one reviewer movie restaurant main task predict whether reviewer write positive negative review base text review standard text classification problem address pang use machine learn technique maximum entropy classification naive bayes classifier support vector machine svm formally document level classification define follow liu set document give document polarity sentiment towards object determine give document relates object determine orientation oo opinion express discover opinion orientation oo feature quintuple assume know irrelevant assume document product review express opinion single object opinion single opinion holder assumption hold product review might hold forum blog post author express view multiple product liu besides supervise chine learn technique document level sentiment classification unsupervised method use literature well turney first step sentence level classification classify sentence objective sub jective literature task call subjectivity classification sadegh afterwards sentiment polarity individual sentence calculate task liuetal main challenge apply model manual effort label large number training example technique automatically mark training data via bootstrapping approach introduce riloff wiebe note quintuple oo use determine task sentence level classification intermediate step determine target aspect specify sentence main subject classification without target sentence classification sentence useless one fundamental assumption sentence level classification focus number opin ion opinion holder sentence single opinion single opinion holder liu assumption hold simple sentence sin gle opinion panel television excellent complex sentence might violate assumption panel television excellent remote hard use sentence positive panel negative remote mixed opinion aspect level analysis focus identify aspect comment mining opinion text document specific entity aspect provide valuable insight consumer business therefore robust fine grain evaluation opinion express particular topic possible liu ordinary opinionated text author might write positive neg ative aspect object even though overall sentiment towards object may positive negative document level sentence level classification contribute kind information aspect level classification particular interest task divide four step pontiki step aspect term extraction task determine aspect term mention text return spe cific aspect term example sentence room hotel beautiful aspect room hotel step aspect term polarity identify polarity aspect term determine step positive neutral negative step aspect category detection aim determine aspect category discuss particular text com parison aspect term step aspect category finely grain appear term give text analyse entity hotel category could consist room price food service step aspect category polarity identify aspect category use specific text step aim determine polarity positive negative neutral every aspect category approach sentiment analysis different technical approach sentiment analysis roughly divide two area collomb lexical analysis lexical classifier machine learn analysis lexical analysis determines sentiment polarity base semantic orientation word sentence give document therefore semantic orientation opinion particular feature identifies opinion positive negative neutral chong mastrogiovanni commonly use absence label data first step text interest tokenized dictionary consist pre tag lexicon use match token negative positive match influence total pool score text instance horrible negative match dictionary therefore total score text decrease positive tag word score increase hence classification text depends overall score achieves probability text positive compute number melvilleetal text depends predefined threshold text classify positive otherwise classify negative without prior knowledge threshold value found literature melville machine learn approach gain popularity last couple year due accuracy adaptability thakkar patel us several learn algorithm predict sentiment base set training data afterwards test dataset use evaluate performance prediction machine learn approach useful sentiment classification categorize text positive negative neutral category collomb supervise learn technique support vector machine svm naive bayes classifier commonly use sentiment prediction chapter long short term memory lstm model introduce context artificial recurrent neural network predictive performance compare svm model accord pang discriminative classifier like svm superior concern sentiment classification compare generative model distinguish mixed sent ments one document us positive negative word small set training data naive bayes classifier might suitable svm relies extensive training dataset build high quality classifier sadegh polarity entry training data label manually time consume task create model base training dataset use classification previously unseen text model performance evaluate different index accuracy score recall simple classification model two category positive negative index compute base follow confusion matrix confusion matrix base binary classification problem chawla tn truenegatives fn falsenegatives fp falsepositives tp truepositives commonly use performance measure context machine learn algorithm predictive accuracy define follow way chawla tp tn accuracy tp fp tn fn balance dataset error cost error rate accuracy use aperformancemetric chawlaetal error cost among class performance metric like recall precision use tp recall tp fn tp precision tp fp recall summarises capability detect relevant observation dataset precision hand computes proportion instance model declare relevant relevant appropriate choice performance metric task hand motivate section conduct sentence level sentiment analysis coindesk news different machine learn approach introduce next chapter deep learn method algorithm follow chapter architecture several neural network discuss advantage criticalfeatures network like word embeddings explain algorithm setup lstm model sentiment prediction examine detail training process model followedbypossible solution moreover support vector machine svm introduce chapter concludes overview performance result differently specify lstm model svm model cover bommes deep neural network dnns deep neural network dnns first introduce grossberg context mathematical model information processing capability biological brain nowa day similarity biological neuron dnns proven rather weak popularity dnns still rise field pattern classification use solve complex problem like natural language understand neural net computer told solve particular problem instead us observational data learn calculate solution problem yin architecture general architecture fully connect dnn described network small processing unit often call node link weight connection hinton biological use case node represent neuron thenetwork initialize feed input one multiple node advance throughout hole network passing weight connection generate output kawakami eachconnectionis bring information forward one layer next general fully connect dnn consists least three layer type input layer hidden layer output layer yin unit input layer represent feature input data unit output layer represent least one class yin number layer defines depth model maximum size layer defines width model add hidden layer dnn describe highly complex function fully connect neural network three input layer one output layer four hidden layer kim forward pas network describes process result unit activation form input layer get disseminate hidden layer output layer yin mostlynonlinear transform sum activation arrive unit sigmoid logistic activation function rectify linear unit relu softmax function select activation function essential task affect way input data format choice proper activation function look detail later fully connect dnn show several advantage yin less need engineer feature solve natural language processing nlp problem one huge task feature engineering conventional way create fea turesmanually dnn slearntask implement feature engineering automatically parameterize system component input output know fix shallow machine learn system dnns part include input connection output parameterized reliable generalization power dnn vast number trainable parameter extend amount training sample accord zhang generalization error model difference training error test error still quite small largely due fact input represent training dataset test dataset therefore parameterized continuous repre sentations create connection sample stand far away conventional representation scheme convolution neural network cnn convolutional neural network cnns apply layer convolve filter enforce local feature lecun useful nlp task especially semantic parse yang search query retrieval shen sentence model kalchbrenner basic idea behind cnns reduce connection input hidden layer instead fully connect lecun although benefit need train lead large parameter matrix matrix multiplication computationally expensive pool fullyconnected layer kim convolution layer obtain refine data layer applies convolution opera tions input data use kernel fix size filter pool layer stage different vector output convolu tion layer pool relevant vector pass forward fully connect layer cnns least one fully connect layer convo lution layer look output previous layer determines feature correlate particular class part output dropout layer dropout layer optional layer prevent overfitting therefore training process probability use randomly prevent different weight update since output feedforward neural network ffnn multilayer perception like theonesabove network applicable pattern classification task specht deal text input input size often varies exact amount input stream data hard determine therefore different neural network introduce next section make possible share parameter different time step recurrent neural network rnns architecture rnn varies ffnns introduce think rnn extension cnn rnns include cycle pas activation previous period input network base previous period network decide current input sak analyze text remote meaning word give context sentence janssen meaning longer expression word sentence also depends meaning surroundings tang recurrent neural network deal short term dependency sequential data lin part network responsible include accordingtosaketal forasequence input recurrent hidden state update otherwise nonlinear function output rnn notate recurrent hidden state traditionally implement follow form wx uh bound smooth function hyperbolic tangent function sigmoid function thehidden state function previous hidden state show rnns inherently deep time salehinejad sigmoid activation function notate illustrates hidden state equation grey box layer activation function like sigmoid activation function notate equation output sequence length every time interval input hand model output generate act input model next time interval illustration hidden layer recurrent neural network salehinejad natural language text tends long term dependency example try predict next word sentence spent three year argentina speak fluent obviously next word sentence spanish base solely recent infor mation rnn would predict next word name language conclude language correct previous part text include well mikolov address shortcoming rnns deal long term dependency challenge train rnns capture long term dependency gradient either van ish explode cause problem use gradient base optimization method wang base variation gradient magnitude long term dependency hidden effect short term dependency well chungetal theselong polarity meaning document one way deal issue introduction long short term memory network long short term memory lstm task require capture long term dependency speech recognition graf machine translation luong rnns use recurrent unit gain popularity past year one long short term memory lstm unit first introduce hochreiter schmidhuber one call gate recurrent unit introduce recently cho although thesis focus former use practice general architecture rnn nasekin chen show full architecture rnn multiple lstm gru gate compose input sequence embed lookup matrix several layer lstm gru cell output sequence mean pool softmax layer main component rnn lstm gru cell structural example foundinfigure andfigure respectively cell state give network possibility store information previous state lstm cell information store particular cell state guard gate three different gate input gate forget gate output gate first step forget gate regulates much former state willflowintoc andthecurrentinput nasekin chen forget gate similar hidden layer introduce formally notate sigmoid function generate output number cell state exp inthenextstep value cell state tanh layer tanh tanh exp exp exp exp input gate play vital role determine store next cell state control amount information new candidate state inputted similar forget gate previous step generates number every value cell state current period weight sum consist past cell state new candidate state denote element wise multiplication lastly update value hidden state period calculate therefore cell state use tanh function multiply element wise output gate tanh hidden state value disseminate within lstm unit unit across lstm cell also upwards next hidden layer one important distinction make stage one lstm cell unit former illustrate blue box described equation latter group lstm cell highlight black box give depth look architecture lstm unit nasekin chen lstm unit generates output sequence serf input next unit well next layer specific kind architecture grant lstm network model long term dependency efficiently cell state previous period influence one future period well next layer name long short term memory derive network ability balance old new information use recent input event another major advantage feature reduce problem vanish explode gradient address another recurrent unit use context rnns gru gate recurrent unit although feature next part thesis basic architecture functionality shortly outline similar set equation notate gru architecture illustrate consists update gate combination input forget gate lstm unit reset gate besides cell state theupdategatez provide previous hidden state new candidate value building thereset gate establishes amount past information forgotten formally gru feature described follow formula nasekin chen tanh structure lstm unit nasekin chen structure gru unit nasekin chen word embeddings accurate word representation crucial natural language processing often word represent discrete distinct symbol scarce many task lack ability generalization levy goldberg example symbolic representa tion word club mate coffee entirely unconnected even though coffee valid indication verb drink information also indicator club mate successfully train statistical model base symbolic representation data might need therefore representation word capture seman tic syntactic similarity alharbi distribute representation word define real value dense low dimensional vector call word embed liu syntactic semantic property word thereby described dimension word embed vector zhao zhao comparison one hot encode word embeddings yin difference one hot encode word embeddings show one hot representation give vocabulary size every word denote binary vector length value word specific index remain value yin accordingtothe left handsideoffigure distance two word every possible combination however apparent zebra horse similar zebra school pretty analogous animal word embed model use word vec technique right handsideinfigure dictionary via form distribution relationship also know distribute representation nguyen indicate single word embed space represent dimensional vector mostly yin main benefit ability share information similar word related word zebra horse example similar vector value two candidate word share feature value besides second value usage low dimensional dense vector two main benefit one computational gener alization power similarity feature detect worthwhile find representation secure example training word horse observe numerous time word zebra couple time word identify dimension instance horse carry information one zebra dense vector representation vector horse might share property vector zebra enable yin accordingtonasekin chen pre train matrix embed lead faster smoother convergence training algorithm different method pre train word embed weight introduce literature word vec glove popular family method word vec model two method generat ing dense embeddings look particular skip gram continuous bag word cbow bothactaseachother scounterpart thecontextwordsc cc predict give word cbow word predict base context word cc slightly different notation goal skip gram model maxi mizing conditional probability hence choose context word likely condition observe give word probability represent softmax function nasekin chen evc vw cev vw vector embeddings notate parameter conditional probability vocabulary set context particular ci wi form argmax pc task carry equation one major downside summation denominator computationally expensive therefore practical one way overcome problem use negative sample chooses different noise word corpus virtue frequency additionally normal pair second pair noise pair generate note construct whole corpus nasekin chen notation negative sample objective conduct follow way argmax log log witch softmax function detailed look computation negative sample approach found levy goldberg architecture word vec neural model nasekin chen word vec model illustrate graph show shallow neural network one hidden layer contain share weight context word one downside method negative sample lack optimal prediction context word approximation nonetheless another benefit word vec model dense vector representation vocabulary word nasekin chen support vector machine svm svm itcan use sentiment classification well different type nlp extension support vector classifier us kernel function map data point space gelbukh separatingthe data point do use non linear boundary detailed introduction topic please see joachim work multi class classification problem joachim every class binary classification problem set follow way joachim class label th binary learn task accordingly bin binary class label bin independent identical distribute training sample example drawn document vector class label document vector high dimensional vector carry information word document afterwards binary problem use train individual classifier result binary classification rule classification new example output estimate pr inspect classify instance class pr large subdivide multi classification problem binary classification problem often call one versus ovum strategy joachim different less frequently use approach pairwise classification apply strategy lead classification problem bommes implement svm model base ovum approach predict polarity sentence malo dataset next section compare performance svm implement bommes lstm model introduce thesis lstm method apply sentiment prediction algorithm setup sentiment prediction framework lstm model use predict sentiment coindesk dataset base research carry peter nagy aim analysis differs slightly study conduct thesis author goal predict sentiment twit ter post positive negative polarity consider language use twitter post informal include slang point essential remember difference style structure training prediction datasets influence result analysis model fit specific type language may able repli thebase architecture introduce peter nagy still useful thesis since training process model crucial component gather information subject language structure however structure model need specify correctly depend task hand hence change model make give overview parameter setup lstm use peter nagy left hand side right model specification sentiment prediction three category positive neutral negative base malo training data notate throughout chapter reason behind choice different parameter make transparent possible field deep learn rather young therefore might possible identify state art technique every task hand maximum encode message length input length embed dimension embed dim batch size spatialdropout recurrent unit lstm recurrent dropout dropout recurrent layer activation function softmax softmax loss function binary cross entropy categorical crossentropy optimizer adam adam parameter setup lstm peter nagy left side adjust patameter setup right side input length network input length depends maximum vocabulary size sentence training data pennington therefore range statement malo data analyse mean length sentence word sentence length one could check manually sentence carry additional information whether exclude analysis nationality therefore input length fix exclude long sentence density vector input length shorter deeper neural network provide computational benefit note analysis word frequency carry dataset agreement rate optimal input length differs different agreement rate instance support nokia phone include gb touch xpress classic navigator classic navigator xm word frequency malo training data agreement rate amongst annotator embed dimension afterwards embed layer translates input length sequence dense vector dimension embed dim accord britz large embeddings suppose generate high bleu bilingual evaluation understudy score additionally expect result low perplexity model although dimensional embeddings generate best result small surplus even way small embeddings dimensional embed achieve quality result converge almost twice fast furthermore britz observe gradient update small large embed ding significant difference regardless size norm gradient update embed matrix stayed roughly constant training overfitting large embed size observe number trainable parameter increase rapidly size embed dimension training log perplexity stayed approximately britz suggest model make sufficient use additional parameter well optimization technique might need although high embed dimension yield well result slight margin case computational power sparse small dimensionality might beneficial give reason embed dimension set batch size batch size determines number sample per gradient update batch result update model accordingly small batch size contributes faster learn rate keskaretal method algorithm choice us small portion training data generallybetween keskar show large batch size lead significant degradation quality model manifest weaker ability generalize accord keskar may base particular property large batch method tend converge sharp minimizers training function compare model result base different batch size optimal batch size could identify dropout one significant difficulty work neural network habit overfitting training data nns consist multiple non linear hidden layer learn extremely complicate relationship along output however sample noise might reason lot relationship therefore exist training data data use prediction srivastava many method developed attenuate issue one way prevent overfitting implement dropout refers drop unit recurrent neural network drop unit temporarily remove system include incoming outcoming connection deng individual unit cut network probability independent unit srivastava show kim dropout probability optimal scenario one think sample thin neural network original one unit unaffected dropout constitute thin network therefore network unit transform possible thin network process training neural network dropout similar training assemblage thin network srivastava although dropout powerful tool feedforward neural network lack abili tie recurrent neural network lstm accord bayer lack concept overfitting introduce section left standard nn two hidden layer right thin net produce apply dropout cross unit drop srivastava performance due amplify noise produce recurrence result model learn ability store information several period limited possible solu tion problem present zaremba apply dropout connection non recurrent therefore lstm improve dropout without lose ability memorize instance past period dropout percentage suggest peter nagy relatively low compare adropoutrateof andarecurrent dropout rate use compile model mean unit linear transformation input drop subsequently linear transformation recurrent state drop unit well recurrent layer recurrent layer lstm part dense layer connect layer sub sequent layer huang discover regularize effect dense connection reduce overfitting task limited training data output lstm construct take three different value positive neutral negative recurrent layer create three output value one class model introduce peter nagy focus straightforward task sentiment classify two category negative positive task sentiment classification define two category problem many researcher pang lee dave however every comment user article news declare positive negative statement case consist fact towards specific topic without carry sentiment koppelandschler explore thesis neutral document show less potential learn material clearly define sentiment conclude learn positive negative ex amples lead misclassification neutral case use neutral sentiment improves distinction positive nega tive example especially use training data malo neutral sentiment play important role discuss discriminate neutral positive sentiment achieve low pairwise agreement rate amongst annotator hence exclude neutral sentiment analysis may overestimate proportion positive instance distinction two somewhat hard context financial news although three category task complex additional information gain inclusion neutral class make implementation mandatory give reason model consists three recurrent layer model training pretrained word embeddings themodelconsistsof parameter fromwhich aretrainableand non trainable duringtraining training process refer non trainable parameter rather large number constant weight model explain use pre train word embeddings global vector critical question regard word embeddings meaning generate word occurrence meaning represent derive word vector pennington particular feature meaning obtain co occurrence probability pennington illustrate idea introduce small example two individual word carry particular form interest context thermodynamic phase word might ice steam relationship two inspect look ratio co occurrence probability different probe word word link steam ice example vaporous ratio pjk co occurrence probability expect large pik data found http nlp stanford edu project glove hand word link ice steam example solid ratio expect small word related ice steam none rate pjk close one next step information present ratio transform pik word vector space detailed mathematical approach transformation please see pennington result model construct vector space purposeful substructure combine advantage two prevalent model literature global matrix factorization local context window method finally glove corpus consist word associate pre train word embeddings later chapter performance differently specify model compare one focus difference performance model use trainable word embeddings one word representation base unsupervised learn word vec loss function dnn train gradient descent optimization algorithm part loss function often call error function define calculate loss model current state result use update weight next evaluation reduce loss bishop choice loss function important task correspond predictive model problem hand case loss function match classification problem primary loss function use literature classification problem cross entropy function bishop calculate score consist average difference predict probability distribution actual one particular class problem score need minimize optimal cross entropy score hence classification problem two target value implementation categorical cross entropy need binary classification problem see peter nagy binary cross entropy use described choice output layer recurent layer match chosen loss function well result three recurrent layer categorical cross entropy loss function activation function activation function use define output nn specific case put one three sentiment class positive neutral negative generally one distinguishes two group activation function linear non linear activation function hence discuss due characteristic output linear activation function lay range therefore help complexity different parameter benefit non linear activation function lie ability make easy model generalize differentiate output four different function frequently use practice bishop relu rectify linear unit max use relu activation function model ability train give data sinceitsrangeis itcannotmapnegative value accordingly sigmoid logistic activation function exp sigmoid activation function beneficial property result value lay range therefore mainly use estimate probability model output exp exp tanh hyperbolic tangent tanh exp exp itsvalueslaybetween bishop point tanh obtain faster convergence training algorithm compare logistic function reason often use instead logistic function softmax exp exp one see share property sigmoid activation function term value lie ensures enables use cross withq furthermore dunne campbell demonstrate logistic activation function rebuild special form softmax due great flexibility softmax activation function widely use multiclass prediction problem dunne campbell output several probability depend many class label exist data case three way classification problem observation carry three probability one class class high probability accord model likely observe give particular characteristic tomas mikolov optimizer popular widely use optimizer algorithm nlp task adam kingma ba internally tune learn rate parameter different optimizer allows fine tune learn rate stochastic gradient descent sgd sgd outperform adam zhang wallace time choice optimizer motivate rather poorly literature less tune hyperparameters need adam implement literature nlp task analysis base model use adam optimizer training data one major setback training model textual analysis task financial article difference vocabulary expression use medium report company related news article loughran mcdonald availability corporas economic financial domain extremely sparse especially data set include phrase level notation context financial area malo built corpus train sentiment model economic text use english news article omx helsinki company across different industry news source able scrape sentence news article next step random sample sentence pull classify phrase level positive negative neutral category task carry three researcher master student aalto school business sentence corpus analyse five eight participant small subsample sentence malo examine degree overall pairwise agreement tween annotator high agreement rate observe separate positive negative sentence well neutral negative sentence main challenge lie discrimination positive neutral sentiment manifest low pairwise agreement rate accord malo finding might surprising company tend exaggerate positive develop ments detailed overview found base number overlap annotation malo provide four different way define majority vote base gold standard sentence agreement sentence agreement sentence agreement sentence agreement give detailed overview four different agreement specification fall agreement rate amongst annotator relative proportion sentence declare positive slightly increase whereas negative neutral label sentence stay roughly constant every agreement rate majority sentence classify neutral follow positive negative minor change however number sentence increase remarkably few annotator agree sentence include slight increase positive share explain difficulty distinguish neutral positive corporate news number negative neutral positive sentence sentence sentence sentence sentence distribution label phrase bank four subset form base strength majority agreement malo training model performance largely depends training data two factor particular influence result quality training data increase high agreement rate specific dataset vice versa quantity data increase decline agreement rate statistical model tend gather information learn faster substantial dataset todeterminethebest identically specify model train three different agreement rate found result show decrease performance relevant metric low agreement rate amongst annotator increase number sentence use train model positive influence result performance unseen data aggravate low agreement rate therefore training data agreement rate use anotherchallenge many observation declare neutral whereas minority class negative sentence account training model imbalanced dataset challenge might hinder performance hence different method overcome restriction introduce random undersampling synthetic minority sample technique challenge training process overfitting primary goal machine learn method achieve high generalization power high accuracy precision rate achieve training data producible unseen data one reason model delivers near perfect score training data one distinguish two different type overfitting hawkins influence performance worsens performance model forexample apolynomial excessive degree overfitting occurs recurrent connection rnn prevent essential area research nlp one method overcome overfitting already introduce dropout drop individual unit network capable pre vent overfitting different straightforward approach introduce srivastava training process stop soon model performance validation set start decrease significant improvement make early stop implement callback function python first metric check define validation loss afterwards patience argument pass function patience training stop chollet show graphical method check overfitting training validation loss monitor epoch training process increase validation loss validation loss decrease training loss optimally validation training loss small slightly high training loss lstm dropout lstm dropout rate validation training loss differently specify lstm unit concern dropout rate opposite scenario possible well case validation loss significantly low training loss model underfitting underfitting relevant analysis section focus entirely overfitting lstm dropout start overfitting epoch validation loss onwards ontheotherhand thelstm drop rate show first sign overfitting period though display improvement validation loss later epoch model perform equally well training data although unseen data lstm dropout adropoutrateof ischosen basedonperformance metric imbalanced class one problem identify look dataset malo class distribution data number neutral observation compare number negative positive instance skewed show distribution data accord three class neutral positive negative seem easy find neutral news positive negative one roughly observation declare neutral although domain far imbalanced fraud detection domain defraud credit card account per year tom fawcett still without adjust imbalanced nature training data conventional method lean bias favour majority class low accuracy theminorityclass heandgarcia distribution data might lead trivial classifier rank every observation instance majority class since observation minority class consider outlier ignore distribution malo training data identify instance negative positive class relevant analysis different method account imbalanced dataset introduce although accurate way deal depends highly give data common approach outline follow metric training model base imbalanced dataset metric beyond accuracy use chawlaetal instance classify precision relative proportion positive classifica tions genuinely positive optimize algorithm respect accuracy might lead naive classifier predicts majority class every time example classifier two way classification problem instance class case class reach accuracy merely predict class every observation chawla instance switch metric parameter model selection enough achieve require performance detect minority class undersampling popular approach account imbalanced dataset different sample technique tom fawcett sample minority class replacement increase number observation minority class randomly duplicate instance class oversampling lead data mean thevarianceof certain variable might decrease whileit still base onthe type ofobservations furthermore oversampling duplicate number error tom fawcett sample majority class might seem high variance due loss observation depend size dataset sample might optimal choice account imbalanced data small dataset number observation might sufficient anymore provide significant result smote synthetic minority oversampling technique approach sample minority class create synthetic example follow way chawla first difference feature vector interest near neigh bour compute next step difference multiply random number finally value add back feature vector interest detailed look computation performance metric please see hilario give graphical illustration smote technique one significant advantage use smote creation new observation compute randomly base exist one result general decision region minority class tom fawcett smote oversampling minority class chawla adjust class weight another way account imbalanced dataset adjust class weight chine learn library scikit learn pedregosa python offer possibility adjustthe importance ofcertainclasses tomfawcett thedefaultclass weight parameter one class specify lower instance less importance analysis increase class high impact analysis importance class measure error error class costly weight increase penalize misclassifications instance malo dataset class eitherbymanually increase class weight positive factor negative factor orbyusingtheclass learnlibrary imbalanced character data way detect negative positive instance important analysis adjust class weight implement base malo dataset agreement rate positive negative neutral confusion matrix accuracy precision recall score data positive lstm trainable neutral wordembeddings negative weight avg total positive lstm smote neutral ovesampling negative weight avg total positive lstm pre train neutral embeddings negative weight avg total positive svm neutral oversampling negative weight avg total seebommesetal svm result base training data malo thelstmmodels weretrainedwith embed dimensionsof batchsizeof epoch training hence early stop mechanism implement model stop training validation loss show improvement predefined pe riod best model save case early stop ten epoch without improvement chosen metric optimize model self define precision recall default metric accuracy tends overfit model situation imbalanced training data dropout rate balance class weight implement prevent model overfitting function use define compile fit model take kera chollet scikit learn pedregosa additionally minority class positive negative sentiment oversampled ing smote algorithm part imblearn package lema tre model two might seem like result change smote discrimination neutral positive neutral negative show sign improvement overall perfor mance benefit implementation pre train word embeddings word vec discrimination group bad compare smote unfortunately result bommes focus precision recall svm model nevertheless lstm show improvement svm base result well suit three way classification problem hand possible explanation ability lstm model account long term semantic dependency text data go forward final model use prediction lstm model smote oversampling minority class please see full implementation model training http github com adoebele lstm sentiment analysis coindesk brief description smote found news data sentiment construction chapter focus data gather coindesk use sent mentprediction andnecessary areillustrated finally regard construction sentiment index give result briefly analyse coindesk coindesk news website particular focus blockchain bitcoin cryp tocurrencies whole site launch april release close article three different category business market technology since follow analysis available data business category apr april use aim thesis sentiment analysis base article relevant context blockchain category focus mainly price individual cryptocurrencies exclude research market textual data source collect via dynamic web scraper use gather data webpage work load option note request data aggressively website might lead break site get red flag therefore time sleep command include mimic human behaviour one request one website every second turn sufficient total article discuss time frame illustrate show number article per day last six year one observe increase number daily article year significance blockchains popular use case bitcoin undergo corrective phase price decrease asmuchas marked period price discovery time high usd valuation broken every day increase blockchain news article period could indicate grow interest underlie blockchain technology fuel price surge specific cryptocurrencies additionally trading volume bitcoin could related number article grow interest blockchain technology might lead increase research adoption company analysis whether change sentiment towards http github com adoebele lstm sentiment analysis coindesk blob master dynamicwebscraper coindesk py blockchain industry influence return sector perform later number daily article buisness section coindesk march mai preprocessing text data preprocessing data essential assure identical format datasets one use train model form one use prediction otherwise accuracy model might suffer model generate error vector differ length model train context specific dataset context coherence training data prediction data assume moreover processing data cause shrinkage information content assume work financial data separate fruit example like appear rarely benefit lower word still outweigh disadvantage training data malo already split sentence thing left low case sentence keep word alphabetic tokenize result word afterwards data need split training validation set prevent overfitting processing training data found part model training http github com adoebele lstm sentiment analysis coindesk asthefollowing chapter carry sentence base sentiment prediction article coindesk com broken list sentence article data return web scraper csv file contain relevant link coindesk com therefore text extract link gather first step clean afterwards convert link list link content url parse use beautiful soup construction daily sentiment score time stamp article collect well sentiment day one article calculate average sum sentiment give day parse content word convert lowercase non alphabetic character remove final step text divide multiple list sentence instance corresponds one article timestamp content article use make sentiment base prediction follow chapter illustrates process cleaning text scrap parse web processing processing part deal xa shingo part deal shingo lavine founder ceo lavine founder ceo ethos become voyager ethos become voyager chief innovation officer chief innovation officer example processing sentence coindesk news besides lower character remove punctuation expression xa dis miss well dynamic web scraper us htlm tag scrape webpage gather relevant information within html code hence remnant html erase text cleaning apply sentence dataset obtain accurate prediction model lately field nlp shift bag word model word encoding ward word embeddings brownlee already touch benefit use word full implementation extract cleaning text coindesk link found http github com adoebele lstm sentiment analysis coindesk embeddingsoverbag word relative meaning within text also spell punctuation different variation word automatically learn embed space cleaning text might therefore significant performance model classical nlp stemmingwords tomasmikolov vec saiditbestwhen ask prepare text data word vec tomas mikolov universal answer depends plan use vector experience usually good disconnect remove punctuation word sometimes also convert character lowercase one also replace number possibly great constant single token allthesepre valuable content case may true lowercase certain word bush different bush another usually sense another small vocabulary low memory complexity robustly parameter word estimate although preprocessing data specific way might necessary train model decrease vocabulary size carry benefit improve performance model seem right way process data lot depends task need perform doubt one use differently handle data compare metric every model result base score optimal way alter input found sentiment index construction inthisstep final whichisthe coindesk dataset course thing perfect model prediction task one achieve high performance metric use see lstm model smote oversampling final model classification prediction text data discrete represent network one hot encode input vector general total text class sentence class provide network input vector length zero throughout except kth entry whichisone graf accordingtograves pr naturally parameterised softmax function output layer follow way exp pr exp result output vector length include probability associate class particular sentence precisely probability certain input sentence part class prediction coindesk dataset three way classification problem derive output vector length class positive neutral negative include correspond probability sentence part class furthermore sentence predict part category high probability step prediction sentence article classify one three group afterwards sentence base sentiment aggregate conduct polarity article therefore summarize follow way bommes article consists series sentence total article vector contains sentiment every sentence article obtain article base sentiment first fraction polarity calculate notation introduce do chen pf nf one see pf nf denote positive negative fraction sentiment please find python implementation estimate sentiment index http github com adoebele lstm sentiment analysis coindesk see softmax activation function one particular article large number neutral sentence article small correspond value pf nf next step aggregate sentiment measure introduce base fraction define notate antweiler frank log pf log nf ia overall negative polarity give hold nf pf equal ia fraction negative positive sentiment polarity article neutral ia positive polarity observe fraction positive sentiment great fraction negative sentiment ia look scale overall sentiment polarity one observe log log ia pf nf might useful rescale follow way bommes log ia result value hold therefore interpretation intuitive valuescloserto indicate positive sentiment show daily sentiment estimate coindesk article base approach note sentiment curve smoothen aggregate data monthly level monthly aggregate sentiment estimate average daily sentiment observe give period overall polarity monthly aggregate positive might either due optimistic funda mental outlook blockchain technology positive bias coverage blockchain news coindesk another possibility bias estimate base poor performance model result perform training process imbalanced data option discuss bit detail later low value sentiment could observe dip year saw major correction blockchains famous use case bitcoin monthly aggregate sentiment number article coindesk additionally period low sentiment draw much attention blockchain number daily article reach high value time bearish news seem dis cuss frequently high value sentiment lead increase number article publish see application sentiment index part thesis question change sentiment towards blockchain tech nology influence return different sector answer section additionally relationship estimate sentiment period condi tional variance return period analyse generate sentiment estimate coindesk dataset index eight different sector collect spindices sector industry index measure performance widely use global industry classification standard gics sector sub industry spindices financial industry instance consists firm institution provide financial ser vice commercial retail customer bank investment company insurance company accord ho hung investor sentiment play vital role asset pricing model sentiment often mirror investor expectation current market condition future development sector data preprocessing analyse effect sentiment change sector return time horizon sector sentiment data match therefore preprocessing data necessary show number daily publish article quite low sentiment estimate base period might bias due opinion concentrate instance follow analysis sentiment estimate refer time neglect available data extends january march contains daily observation sector index sentiment prediction farma control variable additionally sector index transform log return capture percentage change give day benefit transformation lay normalization character possible measure variable comparable metric find analytic relationship even though come price series different value furthermore lognormality give assumption price distribute lognormally log normally distribute merton sector include industrial communication service consumer discretionary consumer staple financials health care information technology energy real estate task carry found http github com adoebele sector sentiment preprocessing give reason log return chosen follow analysis compute particular way log log log price asset period accordingly period farma control variable analyse dependency two variable value dependent variable might dependent predecessor time certain independent variable could depend past value variable well therefore control variable need introduce distinguish direct indirect effect sentiment sector return control variable introduce thesis base research fama french kanuri mcleod smb small minus big return diversified portfolio small stock minus return diversified portfolio big stock give time interval hml high minus low difference return diversified portfolio high low book market ratio stock rmw robust minus weak difference return diversified portfolio stock robust weak profitability cma conservative minus aggressive difference return diversified portfolio stock low high investment firm mkt rf excess return market difference value weight return nyse amex nasdaq stock one month treasury bill rate famaandfrench therefore isolate effect impulse sentiment sector return data download follow link http mba tuck dartmouth edu page faculty ken french data library factor html share price stock market number share static contemporaneous regression influence sentiment vary different sector every sector equally affected blockchain technology one might expect industry embrace blockchain use case well suit analysis three promising sector finance information technology energy finance sector start implement blockchain solution numerous way clearing settlement procedure prim blockchain setl euroclear citi another prominent use case cross border settlement financial payment need pas numerous bank global transaction make slow expen sive fintech firm ripple introduce xcurrent blockchain base message system coordinate transfer money bank second schwartz use case information technology sector grow fast rate last year due rapid blockchain development concept cloud storage entirely new original form first attempt decentralize cloud storage record include project filecoin benet greco sia ko storj storj lab inc although willingness large tech firm integrate decentralize cloud storage question another significant aspect data verification promising application like proof origin product quality verification new application blockchain technology might indicate grow involvement major sector company space furthermore energy sector might benefit expansion blockchain use case directly increase energy consumption proof work algorithm use blockchain application still consume tremendous amount energy could benefit return energy sector get idea relationship sentiment estimate return financial sector introduce focus recent observation sentiment estimate base substantial amount article per day period finance return sentiment prediction aggregate monthly level smoothen series get clearer picture overall relationship two seem move generally direction besides large gap begin around start graph energy sector found estimate monthly aggregate sentiment return financial sector period middle begin series move slightly mean afterwards small decrease sentiment return observe early high fluctuation clear trend chart hard establish relationship sentiment return financial sector base graph analysis time series data regression model introduce simple static regression model write follow wooldridge dependent variable denotes return financial sector period sentiment estimate period vector control variable consists variable introduce static time series regression model observation dependent variable influence contemporaneous value explanatory variable wooldridge implies interaction variable sum occur immediately within period hence implement model require observation interval long enough allow behavioural adjustment take place wooldridge high frequency data assumption might violate give reason daily observation aggregate weekly observation model base weekly frequency observation implement contains regres sion result base observation weekly return financial sector estimate weekly sentiment control variable finance estimate std error value value intercept sentiment mkt rf hml smb rmw cma adjust static regression result finance sector result show high value coefficient sentiment indicates significantly different zero conclude variable sentiment significant influence return financial sector level expect control variable mkt rf hml cma highly significant level furthermore show similar result energy sector correspond coefficient insignificant high value regression carry dynlm package zeileis therefore base sector data sentiment score estimate coindesk article dependency blockchain sentiment return sector could establish code found http github com adoebele sector sentiment preprocessing estimate std error value value intercept sentiment mkt rf hml smb rmw cma adjust static regression result information technology sector energy estimate std error value value intercept sentiment mkt rf hml smb rmw cma adjust static regression result energy sector generalize autoregressive conditional heteroskedasticity garch model see variable sentiment significant influence return different sector time series section impact sentiment conditional conditional volatility sector return tomorrow test implement garch model garch model suitable case heteroskedasticity hence data error term expect differ region data formally heteroskedasticity define follow way lu tkepohl return time time mean time series variance different variance different time interval heteroskedasticity give simple way check heteroskedasticity plot time series look cluster volatility daily return finance sector period show different volatility cluster time series finance sector turn volatility begin roughly constant one significant spike november afterwards increase volatility begin please find plot sector time series ning observe take structure time series account implementation garch model analyse influence sentiment volatility return necessary due het eroscedasticity garch model introduce bollerslev generalize form arch model able deal heteroscedasticity one particular model use research depict conditional variance function exogenous vari ables financial data garch model fratzscher miah rahman show compare garch model garch thebest time follow form conditional variance depends conditional variance residual return thegarch tion update conditional variance depends past value sentiment estimate justify use sentiment term conditional variance equation garch model sentiment index external regressor estimate purpose package rugarch ghalanos use give overview coefficient estimate base daily financial sector return coefficient highly significant extremely low value manifest significant relationship conditional variance financial return period past sentiment value period blockchain industry although value implementation found http github com adoebele coindesk sentiment application sector coefficient estimate std error value finance estimate coefficient garch model base daily financial return coefficient quite small therefore strength relation relatively weak similar picture developed sector communication industrials consumer discretionary information technology result summarize extremely low stan dard error could explain equal coefficient high value across different sector finding discuss conclusion discussion section coefficient estimate std error value communication industrials consumer discretionary information technology estimate coefficient garch model base daily return conclusion discussion technology forthatpurpose itcouldbeob serve overall metric accuracy score precision vary tremendously dataset implement pre train word embeddings word vec improve performance significantly compare lstm network trainable word embeddings complexity task might high enough benefit additional information gain pre train word embeddings sentence base three way classification problem like one handdoeshaveless predict next word text document number possible outcome way high use word vec might well suit case additionally context analysis play important role word vec train solely financial dataset model benefit trainable word embeddings tailormade business news one method help increase model performance particularly distinguish neutral sentiment positive negative sentiment oversampling minority class negative positive instance great interest research increase performance metric particular case desirable implementation class even slight change model like switch evaluation metric accuracy recall score significant impact overall performance base three way classification problem svm show lstm outperform svm task sentiment analysis context nlp cell state allow lstm model long term semantic dependency efficiently past information influence decision next layer model long term semantic dependency especially use ful nlp although sentence level analysis might make full use long term semantic dependency result show significant improvement use lstm svm excite approach topic would analysis language model task speech recognition compare result lstm svm longer document potential outcome lstm might benefit efficient use long term semantic dependency result could drift apart industry influence return different sector promising sector identi fied static contemporaneous regression perform however result coefficient sentiment estimate show significance hypothesis reject appropriate level additionally garch model introduce examine interaction sentiment estimate today conditional volatility sector return tomorrow although coefficient value rather small could suffer include control model show highly significant value finding indicate explanatory power blockchain sentiment today conditional volatility sector return tomorrow another find need address distribution sentiment estimate could observe estimate positive mean could indi cate potential bias towards positive sentiment classification one possible explanation structural bias news coverage coindesk due close tie blockchain com munity coindesk might incentive report positively indicate overall positive state blockchain market another possible explanation lie structure training prediction data achieve similar result prediction training format underlie data match primarily similarity language use article training prediction affect result tremendously thesis conduct assumption financial news malo similar property blockchain news article coindesk define formal language one could make case formal language use financial news differs one use blockchain news due different interest group might require additional research determine whether structure two alters ultimately outlook blockchain general might optimistic therefore model generates majority positive estimate model deeper neural network every additional layer neural network increase number trainable parameter hence flexibility model task shallow neural network might sufficient addition layer lstm model increase model performance slightly time consume computa tion increase drastically light analysis result justify addition various layer expense computational time point hard tell whether implementation multiple lstm layer would increase overall performance significantly moreover conduct sentiment analysis deeper lstm model could academic interest possibility overcome imbalanced class cover thesis cost sensitive learn regular learn misclassifications treat equally cost sensitive learn one specify cost misclassification particular class ahigherpenalization misclassification customize especially imbalanced datasets increase true positive rate minority class although varies method account imbalanced training data implement monitoring effect cost sensitive learn performance result would interest question remains benefit model achieves outstanding performance metric know training data reproduce result unseen data finally interaction blockchain sentiment volatility sector return demand research garch model implement thesis could expand additional variable isolate effect sentiment volatility return sen timent traditional financial market research field behavioural economics might help explain relationship kahneman study irrationality human establish different cognitive bias may help explain influence sen timent personal investment choice research like one conduct thesis influence blockchain tech nology traditional market still early stage put matter perspective prominent blockchain use case bitcoin market cap around billion dollar side financial sector alone market cap approximately trillion dollar blockchain use case nowhere near mass adoption therefore influence traditional market right questionable impact blockchain technology everyday life task expect rise come year future anal ysis might conduct different result market cap blockchain use case rise accordingly instead focus sector research carry thesis could apply specific firm strong tie blockchain industry identify firm influence blockchain sentiment return might show different result small firm expect react faster news sector conclusion blockchain space still emerge new application launch everyday relevance technology grow result thesis concern return technology development future hard judge increase usabil ity relevance blockchain news traditional market grow accordingly therefore might influence return market much great way outline thesis could say machine learn application neural network particular last year see tremendous increase usage algorithm even problem could cover easily less sophisticated method deep neural network reveal full potential yet research show nlp task long term semantic dependency lstm model well suit carry classification aboody levi wei managerial incentive option cost structure choice review accounting study alharbi metadiscourse tag academic lecture ph thesis sheffield antweiler frank talk noise information content internet stock message board journal finance aue gamon customize sentiment classifier new domain case study proceeding recent advance natural language processing ranlp citeseer vol bayer osendorfer korhammer chen urban van der smagt fast dropout applicability recurrent network arxiv preprint arxiv benet greco filecoin decentralize storage network protoc lab bishop neural network pattern recognition oxford press blitzer dredze pereira biography bollywood boom box blender domain adaptation sentiment classification proceeding th annual meeting association computational linguistics bollerslev generalize autoregressive conditional heteroskedasticity jour nal econometrics bommes chen ha rdle textual sentiment sector specific reaction journal finance data science britz goldie luong le massive exploration neural machine translation architecture arxiv preprint arxiv brownlee machine learn mystery vermont australia chawla bowyer hall kegelmeyer smote synthetic minority sample technique journal artificial intelligence research chen daigler parhizgari persistence volatility future market journal future market future option derivative product chesley vincent xu srihari automatically classify blog sentiment training cho van merrie nboer gulcehre bahdanau bougares schwenk bengio learn phrase representation use rnn encoder decoder statistical machine translation arxiv preprint arxiv chollet kera http github com fchollet kera chong mastrogiovanni handbook research ambient intelli gence smart environment trend perspective information science reference chung gulcehre cho bengio empirical evaluation gate recurrent neural network sequence model arxiv preprint arxiv coindesk blockchain news http www coindesk com category business news last access collomb costea joyeux hasan brunie study rapport de recherche rr liris dave lawrence pennock mining peanut gallery opin ion extraction semantic classification product review proceeding th international conference world wide web acm deng yu deep learn method application foundation trend signal processing dunne campbell pair softmax activation cross entropy penalty function derivation softmax activation function proc th aust conf neural network melbourne citeseer vol fama french five factor asset pricing model journal financial economics fratzscher financial market integration europe effect emu stock market international journal finance economics gelbukh computational linguistics intelligent text processing th inter national conference cicling mexico city mexico february proceed ings vol springer ghalanos rugarch univariate garch model package version graf generate sequence recurrent neural network arxiv preprint arxiv graf mohamed hinton speech recognition deep recurrent neural network ieee international conference acoustic speech signal processing ieee grossberg nonlinear neural network principle mechanism architec tures neural network hawkins problem overfitting journal chemical information computer science garcia learn imbalanced data ieee transaction knowledge data engineering hilario sistemas de clasificacion basados en reglas difusas linguisticas ph thesis hinton osindero teh fast learn algorithm deep belief net neural computation ho hung investor sentiment conditioning information asset pricing journal banking finance hochreiter schmidhuber long short term memory neural compu tation huang liu van der maaten weinberger densely connect convolutional network proceeding ieee conference computer vision pattern recognition janssen frege contextuality compositionality journal logic lan guage information joachim learn classify text use support vector machine vol springer science business medium kahneman think fast slow macmillan kalchbrenner grefenstette blunsom convolutional neu ral network model sentence arxiv preprint arxiv kanuri mcleod sustainable competitive advantage stock performance case wide moat stock apply economics kawakami supervise sequence label recurrent neural network ph thesis keskar mudigere nocedal smelyanskiy tang onlarge arxiv preprint arxiv kim convolutional neural network sentence classification arxiv preprint arxiv kingma ba adam method stochastic optimization arxiv preprint arxiv ko toman spektor whitepaper version koppel schler importance neutral example learn sentiment computational intelligence lecun bottou bengio haffner gradient base learn ing apply document recognition proceeding ieee lema tre nogueira aridas imbalanced learn python toolbox tackle curse imbalanced datasets machine learn journal machine learn research levy goldberg dependency basedwordembeddings inproceedings nd annual meeting association computational linguistics volume short paper vol lin horne tino giles learn long term depen dencies narx recurrent neural network ieee transaction neural network liu sentiment analysis mining opinion sentiment emotion cambridge press liu zhang survey opinion mining sentiment analysis mining text data springer liu handbook natural language processing liu joty meng fine grain opinion mining recurrent inproceedingsofthe method natural language processing loughran mcdonald liability liability textual analysis dictionary journal finance luong sutskever le vinyals zaremba address rare word problem neural machine translation arxiv preprint arxiv lu tkepohl new introduction multiple time series analysis springer science business medium malo sinha korhonen wallenius takala good debt orbaddebt information science technology mao xu yang wang huang yuille deep caption ing multimodal recurrent neural network rnn arxiv preprint arxiv melville gryc lawrence sentiment analysis blog combine lexical knowledge text classification proceeding th acm sigkdd international conference knowledge discovery data mining acm merton option pricing underlie stock return discontinuous journal financial economics miah anda rahman isgarch enough american scientific research journal engineering technology science asrjets mikolov karafia burget ernocky khudanpur recurrent neural network base language model eleventh annual conference international speech communication association nakamoto bitcoin peer peer electronic cash system nasekin chen deep learn base cryptocurrency sentiment construction available ssrn nguyen vo pham nguyen quan deep architecture sentiment analysis news pang lee see star exploit class relationship sentiment categorization respect rating scale proceeding rd annual meeting association computational linguistics association computational linguistics pang lee vaithyanathan thumb sentiment classification use machine learn technique proceeding acl conference empirical method natural language processing volume tic pang lee opinion mining sentiment analysis foundation trend information retrieval pedregosa varoquaux gramfort michel thirion grisel blondel prettenhofer wei dubourg vanderplas pa so cournapeau brucher perrot duchesnay scikit learn machine learn python journal machine learn research pennington socher man glove global vector word representation proceeding conference empirical method natural language processing emnlp peter nagy lstm sentiment analysis kera http www kaggle com ngyptr lstm sentiment analysis kera notebook last access pontiki galanis papageorgiou androutsopoulos manandhar mohammad ayyoub zhao qin de clercq semeval task aspect base sentiment analysis proceeding th inter national workshop semantic evaluation semeval riloff wiebe proceeding conference empirical method natural language processing sadegh ibrahim othman opinion mining sentiment analysis survey international journal computer technology sak senior beaufays long short term memory base recur rent neural network architecture large vocabulary speech recognition arxiv preprint arxiv salehinejad sankar barfett colak valaee recent advance recurrent neural network arxiv preprint arxiv scheible supervise semi supervise statistical model word base sentiment analysis schwartz young britto ripple protocol consensus algorithm ripple lab inc white paper shen gao deng mesnil learn semantic repre sentations use convolutional neural network web search proceeding rd international conference world wide web acm socher lin man ng parse natural scene natural language recursive neural network proceeding th international conference machine learn icml specht probabilistic neural network neural network spindices sector data http eu spindices com index family equity sector industry last access srivastava hinton krizhevsky sutskever salakhutdinov dropout simple way prevent neural network overfitting journal machine learn research storj lab inc storj decentralize cloud storage network framework http github com storj whitepaper last access swan blockchain blueprint new economy reilly medium inc tang qin liu aspect level sentiment classification deep memory network arxiv preprint arxiv thakkar patel approach sentiment analysis twitter state art study arxiv preprint arxiv tom fawcett learn imbalanced class http www svds com learn imbalanced class last access tomas mikolov input document format http group google com forum msg word vec toolkit jpfyp fob tgzzxsco gsj last access tripathy anand rath document level sentiment classifica tion use hybrid machine learn approach knowledge information system turney inproceedings th annual meeting association computational linguistics association computational linguistics wang jiang luo combination convolutional recurrent neural network sentiment analysis short text proceeding coling th international conference computational linguistics technical paper wooldridge introductory econometrics modern approach nelson edu cation yang yih gao deng embed entity relation learn inference knowledge base arxiv preprint arxiv yin deep neural network identification sentential relation ph thesis lmu zaremba sutskever vinyals recurrent neural network regu larization arxiv preprint arxiv zeileis dynlm dynamic linear regression package version zhang bengio hardt recht vinyals understand deep learn require rethink generalization arxiv preprint arxiv zhang wallace sensitivity analysis practitioner guide arxivpreprintarxiv zhao zhao sentiment analysis base requirement evolution predic tion future internet zhou wan xiao attention base lstm network cross lingual sentiment classification proceeding conference empirical method natural language processing estimate monthly aggregate sentiment return energy sector period estimate monthly aggregate sentiment return information technology sector period daily return consumer discretionary sector period daily return industrial sector period average pairwise annotator agreement sentiment category number sentence number annotator overall agreement positive negative negative neutral positive neutral interannotator agreement statistic base subset sentence tag annotator malo accuracy precision recall score data positive lstm neutral agreement rate negative weight avg total positive lstm neutral agreement rate negative weight avg total positive lstm neutral agreement rate negative weight avg total performance metric different agreement rate amongst annotator equalliy specify lstm network declaration authorship hereby confirm author master thesis independently without use others indicate source passage literally general matter take publication source marked july