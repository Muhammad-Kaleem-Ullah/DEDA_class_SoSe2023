leveraged investment strategy use deep reinforcement learn master thesis submit prof dr wolfgang karl ardle universit zu school business economics institute statistic econometrics chair statistic ilyas agakishiev partial ful llment requirement degree master science april hereby would like thank prof dr wolfgang karl ardle supervision bruno spilak help research jiang provide basis python code thesis describes deep reinforcement learn algorithm portfolio management multiple innovation reward function allows investor maximize return regulate risk preference term maximum drawdown risk regulate change portfolio structure increase decrease leverage algorithm consists two convolutional neural network cnns one responsible weight distribution leverage range experiment show bullish market algorithm restricts mostly complies de ned target drawdown bearish market hand algorithmleaves themarket entirely isa majorimprovement comparedto thealgorithm jiang di erent method restrict trading sometimes fails situation ii content introduction methodology convolutional neural network overview structure forward propagation backpropagation regularization reinforcement learn overview markov decision process learn policy gradient optimization technique gradient descent advanced optimization technique model speci problem description portfolio value calculation transaction cost model asset pre selection price tensor reinforcement learn deterministic policy gradient online stochastic batch learn network structure portfolio vector memory result overview data iii experiment experiment experiment discussion research discussion method data quality data generalization model architecture discussion result generative adversarial network conclusion cryptocurrencies price cryptocurrencies monthly return iv introduction portfolio management one important task quantitative nance since portfolio selection paper markowitz rapid advancement eld many current portfolio management strategy require assumption model world however nancial market complex model account everything part solution application machine learn especially deep learn create model complexity deep learn finance still young last year see remarkable progress interest application deep learn eld analysis limit order book sirignano neural network base stochastic volatility model luo use forecasting another achievement machine learn reinforcement learn since discovery one common application teach algorithm play game backgammon checker go silver video game farebrother portfolio management also consider game usage make sense area active research paper jiang liang two example reinforcement learn use portfolio management investor trade return risk desire objective paper study reinforcement learn apply purpose like markowitz portfolio algorithm try maximize return time hold risk metric xed level di erence use maximum drawdown risk metric instead volatility since many investor consider former important similar approach sharpe ratio risk metric attempt necchi inaddition tothat anew approachis introducedthat incorporate leverage option reduce risk increase return structure thesis follow section describes basic method use thesis concept convolutional neural network reinforcement learn section go speci model describes problem quantitative term show speci network structure way reinforcement learn apply section show result experiment describes finally section comment method result suggests topic research methodology convolutional neural network overview convolutional neural network cnns speci type neural network mostly use process grid like data typical example audio picture video processing data temporal spatial goodfellow name give base mathematical operation convolution de ned integral product two function one reverse shift da obviously cnns operate integral instead operate call discrete convolution one dimensional convolution operation however likely con volutional layer two dimensional simplify calculation avoid kernel ipping cross correlation use instead convolution structure forward propagation rough structure convolutional layer look like convolution main operation illustrate note input kernel possible size value kernel weight initialize randomly train addition kernel multiple lters essentially multiple kernel di erent weight well feature extraction multiple layer output illustration discrete convolution operation practice activation function manipulates output ensure non linearity typical ex amples sigmoid function hyperbolic tangent tanh rectus ed linear unit relu practice relu almost always give best performance training process therefore use thesis de ned max pool modi output include value neighbour input cell output cell popular type pool max pool include maximum value within rectangular neighbourhood average pool calcu lates average value within rectangular neighbourhood max pool usually prefer since capture outlier information well pool important task picture processing use invest include thesis asmentionedearlier temporal data also structure cnns allows signi cantly small number feature compare fully connect dense network mean cnns require less memory less training time compare architecture downside way backpropagation backpropagation method use calculate gradient need calculation weight use network goodfellow word essential train network capture pattern problem network backpropagation step calculate use chain rule show illustration backpropagation step green arrow symbolize forward propagation red arrow show backpropagation formula red box gradient show backpropagation work convolutional layer assume input matrix lter kernel output matrix output matrix element calculate backpropagation step give need hij wij calculate formula backpropagation work similar fashion backpropagation relu function give de nition easy compute relu dx otherwise regularization every complex algorithm always issue tting network learns forthat reason regularization apply set measure prevent tting happen three popular regularization method early stop easy way prevent tting force network stop training certain point certain number iteration one method determine number check performance measure loss value validation set rst improve depend model stop improve start get bad high number iteration prechelt tikhonov regularization modi cost function excessive weight actively punish result formula reg number sample batch lambda regularization parameter concept rst introduce tikhonov regularize ill pose problem apply linear regression regression call ridge regression hoerl kennard dropout di erent approach prevent tting arti cially reduce complexity model technique training neuron connection randomly drop weight set reduces number parameter help algorithm generalize well srivastava thesis rst two regularization method use reinforcement learn overview goal reinforcement learn learn action take certain situation state order maximize reward numeric signal sutton barto depend environment come complication case state depend previous state previous action agent others may stochastic process obviously algorithm require lot trial error search order maximize reward reinforcement learn neither supervise learn unsupervised learn rather separate category supervise learn base learn training set correct action give try generalize information unsupervised learn hand objective nding hidden structure unlabelled data correct answer reinforcement learn nal objective unlike supervise learn split small isolated task regard whole also rather learn give data reinforcement learn algorithm learn action make interact environment common problem reinforcement learn call exploration exploitation dilemma algorithm learn way earn reward may use knowledge however ndingamoree addition environment may change make previously optimal policy ine ective result algorithm sometimes deviate optimal policy explorative action see ective update policy necessary main element reinforcement learn explain detail sutton barto policy de ne agent behaves point time basically tell agent action take certain state may deterministic stochastic reward value reinforcement learn algorithm try maximize change policy value function estimate accumulate reward earn give state give algorithm method judging long term success model environment use easy state prediction minimize trial error learn thesis model environment use accurate model nancial time series exists markov decision process formally reinforcement learn follow call markov decision process sutton barto markov decision process stochastic process markov property mean future state depend current state time step environment initial state later agent selects action environment output reward environment output state agent receives reward next state illustration reinforcement learn process therefore interaction loop generates sequence trajectory look like policy function speci action take space policy call optimal policy accumulate discount reward tr max imized reward output follow stochastic process expect reward maxi mized instead argmaxe tr policy optimization process two concept distinguish value function state show expect cumulative reward follow policy state good state tr value function state action expect cumulative reward take action state follow policy good state action pair tr give maximum expect cumulative reward give state action pair maxe tr satis follow bellman equation maxq equation explain way optimal state action value next time step know optimal strategy take action maximizes expect value obviously bellman equation use iteratively eventually nd optimal policy whole state action space however feasible since state action space usually large algorithm would scale properly one solution would application complex algorithm neural network estimate solution call learn learn learn introduce watkins dayan allows estimate action value function function approximator li parameter weight approximator forward propagation step loss function simple square error compute target value since true value calculate use bellman equation maxq afterwards backpropagation step realize update gradient respect parameter function maxq like normal bellman equation equation iterate try bring value close make function optimal optimal policy advantage idea well scalability single learn process possible action state consider mean neural network di erent predictor multiple value output one every action train already mention loss function learn work well scenario number action limited example video game small number di erent input make training do per frame thesis number action per state technically unlimited learn complicate instead policy gradient algorithm apply policy gradient sometimes easy estimate optimal policy function directly instead optimize function li function de ned characterize estimation total reward policy tr objective nd policy parameter maximize argmaxj maximization realize use gradient ascent gradient formula look like policy stochastic easy take gradient transform logarithm do use likelihood ratio formula logx apply policy function continue previous equation log log sequence state action reward men tioned earlier update rule log learn rate notethat inthisthesis butdeterministic whichmakes task nding optimal policy much easy explain thoroughly section since thesis applies policy gradient many common gradient descent ascent algo rithms apply way supervise learn discuss next section optimization technique gradient descent standard gradient descent algorithm minimum found parameter take step proportional negative gradient function one update step iteration learn rate gradient cost function whichisslowandine cient well approach use subsample also call mini batch per iteration approach call stochastic gradient descent sample xed batch size chosen randomly make optimization faster additional bene get stuck less often local minimum disadvantage absolute minimum never found bottou set batch size online learn becomes possible since one iteration make every add training sample stochastic gradient descent online learn nd application thesis see section advanced optimization technique time sophisticated optimization technique invent implement speed training popular one momentum qian rmsprop hinton adam kingma ba momentum minimize process stochastic gradient descent tends oscillate lot momentum algorithm us step gradient previous step attempt cancel oscillation increase optimization speed update process look like momentum term typically set rmsprop itdoesso divide learn rate exponentially decay average square gradient update step recommend set well adam combine idea momentum rmsprop manipulate gradient learn rate first decay average past gradient past square gradient compute basically estimate rst two moment respectively however result accord author bias towards zero hence value need correct accordingly update step adam optimizer widely recognize best perform optimizer therefore use thesis model speci problem description portfolio value calculation give relative price vector time portfolio value without transaction cost calculate follow way portfolio weight vector value asset tell proportion capital invest particular asset note general formula work sum weight way leverage make possible example leverage equal sum portfolio weight vector also equal next logarithmic rate return calculate log log nal portfolio value calculate like initial amount capital transaction cost time period portfolio need rebalanced even maintain weight cost fee need adjust price uctuations give relative price vector weight vector end period calculate like account transaction cost transaction remainder factor introduce basi cally account shrink portfolio value time period portfolio value begin end formula formula log return nal portfolio value respectively log log transaction remainder factor determine set equation include fee paid sell asset reduce weight buying increase weight commission rate purchasing commission rate sell respectively experiment equal note found left hand right hand side equation solve directly impossible therefore iterative algorithm use ormos urb first calculate initial estimate use formula moody iterate converges model asset pre selection dataset consists various cryptocurrency price bitcoin use cryptocur rencies major advantage intraday algorithmic trading cryptocurrencies trade hour day day week algorithm account pause counter trading data source poloniex exchange besides bitcoin available cryptocurrencies select high volume time period validation set necessary ensure high enough liquidity ful two condition require make trading algorithm work properly allow meaningful backtesting zero slippage request trade happen instantly displayed time order place zero market impact market volume high enough trade algorithm signi cant impact market price time period coin use experiment discuss section price tensor price tensor data inputted neural network every time period shape number feature number time period number asset select previous step three hi lo feature close price high price low price however normalize since relative price absolute price interest portfolio management vhi vhi vhi vhi vhi vlo vlo vlo vlo vlo hadamard division operator illustration input price tensor note asset may available whole training set period example etherium start trading august data july case random uctuating value insert way jiang random price movement decay rate reinforcement learn state main advantage reinforcement learn strategy output buying sell recommendation would insert model instead whole trading process do automatically formal term rein forcement learn model look like agent model neural network make trading decision thesis convolutional neural network action model output neural network portfolio weight vector environment cryptocurrency market provide agent information high low close price every minute form price tensor state consists two part external state state environment price tensor internal state state agent previously determine weight vector information use agent determine action take earn reward possible reward construct combination reward punishment reward equal achieve total return log magdon ismailandatiya among investor max another important part punishment target drawdown course target wise aim complete elimination risk investment strategy instead like markowitz portfolio optimization target risk metric determine investor algorithm try exceed threshold formula target punishment max target note expression square award mild punishment small excess make sure algorithm converges close target drawdown also make reward function di erentiable overall reward function de ned max target unlike many typical reinforcement learn task way get reward clearly de ned reward function addition action agent ect environment therefore reason exploration learn process algorithm learn optimal policy maximize reward without worry might function grant even case deterministic policy gradient apply learn deterministic policy gradient apolicy sincethereward function deterministically de ned simple gradient ascent algorithm enough nd optimal policy policy de ned parameter action obviously plusl regularization policy parameter initialize randomly update along gradient direction learn rate gradient performance metric ifmini edsequential time range use themini ciency described section set batch size allows additional learn additional sample insert data online stochastic batch learn asmentionedearlier though mini batch chosen training random give xed batch size time point generate result mini batch note count two completely di erent mini batch due structural change happen nancial market time time recent time period relevant prediction long past one therefore probability realize use geometrically distribute probability probability decay rate determine fast probability decrease step past current time point network structure network structure determine optimal policy determine consist naloutput weight vector input cnns price tensor de ned earlier section output rst cnn weight network unleveraged weight vector mean weight sum achieve use softmax function output layer look like goodfellow ezj kez structure hidden layer call jiang ensemble identical independent evaluator eiie basically mean information asset price pass hidden layer information one asset never interacts information asset rather interaction happen time interaction asset happens softmax layer ensure weight sum suppose improve overall performance since algorithm easy time distinguish individual asset advantage eiie faster learn lessweightstotrain linearscaling learn time linearly capability preserve useful information future update make online learn possible without need retrain network zero every time time period pass structure weight network convolution set di erent asset interact second cnn leverage network output single number leverage coe cient range get number output layer fully connect sigmoid function multiply activation function sigmoid function de ned ex ex range besides output layer network structure identical weight network structure leverage network structure besides output layer nal weight vector compute multiply output weight network output leverage network calculation nal portfolio weight vector portfolio vector memory one feature neural network use weight vector previous period get current weight vector information drawn call portfolio vector memory base experience replay memory introduce mnih essentially matrix contain portfolio weight vector time period portfolio vector memory need train rst initialize weight equal get update every time step whenever time step randomly picked enough training step portfolio vector memory converges use prediction portfolio vector memory also allows network train mini batch portfolio vector memory work weight vector previous time period import memory help training next weight vector nishing calculation result write memory use portfolio memory train even phase online learn every time additional data insert certain number time period randomly chosen retrain accounting new data end mean two training phase algorithm regular training usage training set build portfolio memory network parameter online learn usage new available data ne algorithm adapt new market situation result overview thesis two main experiment check behaviour algorithm two di erent economic situation experiment experiment training set validation set test set data time range use training set validation set test set thetestsetofthe andsome cryptocurrencies signi cantly outperform bitcoin time example etherium see also volatile time would interest see algorithm regulate second experiment test time period happen severe structural change cryptocurrency market market overall become small less volatile consider high trading fee make pro much high challenge hyperparameter value description batch size size mini batch training number trading period input price window size matrix number asset number preselected asset trading rstphaseoftrain total step ing training set value depends perfor mance validation set thel regularizationcoe cientappliedtoboth regularization coe cient cnns hidden layer learn rate step size adam optimization rate commission fee apply transac commission rate tion number online training step period roll step duringtheback test geometric distribution parameter select sample bias ing online training sample batch desire maximum drawdown algorithm target drawdown aim hyperparameter value use algorithm rl maincomputation basis code provide jiang data data consists coin select base trading volume validation set time frame select coin experiment trading value found experiment experiment coin volume btc coin volume btc eth etherium usdt tether ltc litecoin eth etherium xrp ripple xrp ripple usdt tether str stellar etc etherium classic xmr monero dash dash etc etherium classic xmr monero dash dash xem nem ltc litecoin fct factom bch bitcoin cash gnt golem zec zcash zec zcash dgb digibyte select coin two experiment ranked trading volume volume total volume poloniex bitcoin time period validation set plot etherium bitcoin exchange rate found plot select coin etherium bitcoin exchange rate rl coinfigures notice coin spike around june july test period experiment still algorithm react fast bene spike zcash turbulent start least poloniex immense spike begin drop much low level since present respective plot found coin minimum st quartile mean median rd quartile maximum bch dash dgb etc eth fct gnt ltc usdt str xem xmr xrp zec descriptive statistic monthly return available value percent rl note mean mostly positive median negative coin result extreme rare positive spike reference benchmark crix index cryptocurrencies trimborn ardle however since reference currency thesis bitcoin crix value also convert bitcoin easily do apply formula btc crix crix usd sample plot result modi ed crix crix value convert bitcoin begin uctuations comparably low mostly bitcoin even dominant see example coinmarketcap also worth mention structural break begin volatility noticably increase peak alternative coin expensive compare bitcoin around july february match dynamic etherium make sense since bitcoin etherium high weight crix experiment experiment experiment various target drawdowns try compare target return drawdowns ected check whether limitation work intend total capital equal compare crix target rl experiment performance crix target target target return daily result btc return nal capital experiment naturally thereturnsimproved thenextthingtocheck drawdown change time drawdown drawdown drawdown target target target rl drawdownfigures target mdd volatility drawdown annual volatility data see maximum drawdown decrease small however target small value maximum drawdown high target drawdown target mean algorithm fail instead reason algorithm train di erent dataset target drawdown achieve transfer one one test set interest thing analyse leverage change throughout trading period displayed follow plot leverage rl experiment leverage target total leverage seem change time quickly change maximum minimum value test period however interest note begin period leverage uctuate much hover around might due adaptation algorithm make learn note high leverage always close maximum target valueof atthesametime erentford target mean algorithm prefers manage risk invest less volatile asset broader diversi cation rather decrease leverage make sense since diversi cation may improve risk return ratio deleveraging decrease risk return time follow plot visualizes diversi cation various target drawdowns average investment percentage least invest coin equal target test period week rl weightdistribution expect low target drawdown result high invest even least popular coin general downwards trend result continuous learn backtest experiment test period experiment cryptocurrency environment change easily notice result total capital disabled leverage compare crix target rl experiment performance leverage crix target return daily result btc return nal capital experiment overall seem like algorithm struggle gain pro fact leverage thealgorithmwouldsu erlosses however algorithm decide trade exception therefore work intend applies even high value target leverage rl experiment leverage target loss leverage scenario due two reason first structure cryptocurrency market change dramatically early make hard adapt new situation market second market become less exploitative real life trading fee decrease time algorithm still work high fee discussion research discussion method data quality problem data unfortunately cryptocurrency market new amount data relatively low result coin select exist begin select period miss data need lled arti cial data even lled data good enough imitation second problem extreme volatility happens coin rst emerges although algorithm take long past event account small extent nal result may still ected data generalization cryptocurrency market special predictable unpredictable time one hand market new contains asset unknown future impact everyday life make volatile hand market far perfect inexperienced irrational participant theory easily exploit thesis primarily use cryptocurrencies data availability nice property continuous trading closing market stock market data use daily addition assumption must make closing price match opening price next trading day do paper liang apply reinforcement learn chinese stock market stock market unlike cryptocurrency market come drastic structural change although structural change certainly still exist might make adaptation new situation easy although overall return may weaker model architecture reinforcement learn technically predictor could use cnn lstm rnn would also good choice base predictor however cnn best perform network paper jiang likely best one thesis goal thesis though check whether algorithm include risk metric optimization process much nding best structure possible hand leverage network could surely improve upon example would interest try use leverage memory save leverage addition portfolio weight cient training discussion result rst glance algorithm work well able learn complex reward function change target drawdown ect result leverage also work intend increase favourable situation go zero loss expect experiment total maximum drawdown consider training however might reasonable consider recent maximum drawdown future stock market common take last month consideration example calculate calmar ratio young cryptocurrencies may make sense make time window even low course trading strategy backtests necessarily show algorithm always performs well future structural break may signi cantly change outcome generative adversarial network recently another deep learn algorithm start relevant portfolio management purpose gan good fellow use generate picture certain characteristic every gan consists generative network generates data discriminative network try distinguish generate real data network train simul taneously objective minimize error objective maximize zhou us generative network capture distribution nancial data generate arti cial stock price movement training discriminative net work therefore essentially predict stock price conclusion thesis new deep reinforcement learn model introduce test cryptocurrency data first theory cnns reinforcement learn explain work well policy gradient chosen learn model nancial application built combination two cnns output leveraged weight apply directly investment purpose model could evaluate train use reward function reward return punishes excessive drawdown finally two experiment make completely di erent test data one rise market stagnant fall market forrisingmarkets structure leverage depend target drawdown obviously result maxi mum drawdown still deviate target begin sometimes large sometimes small expect since training data always di erent test data especially cryp tocurrencies time performance solid successfully beat crix term return bearish market algorithm correctly identi ed incapability positive return mostly due high transaction cost stop invest even generous target drawdown thesis use model cryptocurrency market minor adjustment applicable market stock market fact could even work well since target drawdown reach reliably stable environment algorithm work well experimental time period however ideally maximum drawdown measure base certain time window since inception improvement could consider allow short sell add feature volume course experiment metric sharpe ratio etc reward function also make future bottou inonline learn andneuralnetworks ed byd saad cambridge uk revise oct farebrother machado bowling generalization reg ularization dqn arxiv preprint index goodfellow bengio courville deep learn mit press http www deeplearningbook org hinton overview mini batch gradient descent unpublished lecture hoerl kennard ridge regression bias estimation nonorthogonal problem technometrics holt forecasting seasonal trend exponentially weight move aver age international journal forecasting goodfellow pouget abadie generative adversarial net th annual conference neural information processing system nip jiang xu liang deep reinforcement learn framework financial portfolio management problem arxiv preprint index kingma ba adam method stochastic optimization international conference learn representation li johnson young deep reinforcement learn lecture note stanford liang chen zhu jiang andy li learn portfolio management arxiv preprint index luo zhang xu wang neural stochastic volatility model arxiv preprint index magdon ismail atiya maximum drawdown risk magazine markowitz portfolio selection journal finance mnih badia mirza graf lillicrap harley silver kavukcuoglu asynchronous method deep reinforcement learn international conference machine learn moody wu liao safell performance function rein forcement learn trading system portfolio journal forecasting necchi reinforcement learn automate trading ormos urba performance analysis log optimal portfolio strategy transaction cost quantitative finance prechelt early stop heidelberg springer heidelberg qian momentum term gradient descent learn algorithm neural network cial journal international neural network society silver huang master game go deep neural network tree search nature sirignano deep learn limit order book arxiv preprint index srivastava hinton krizhevsky sutskever salakhutdinov simple way prevent neural network tting journal machine learn research sutton barto reinforcement learn introduction cam bridge massachusetts mit press tikhonov stability inverse problem doklady akademii nauk sssr trimborn ha rdle crix index blockchain base curren cies sfb discussion paper economic risk watkins dayan learn machine learn young calmar ratio smoother tool future zhou pan hu tang zhao stock market prediction high frequency data use generative adversarial net mathematical problem engineering cryptocurrencies price  exchange rate coin include experiment time series frequency minute contain data end period range july october coin trade throughout whole period time series start begin trading end trading stop note initial trading day zcash zec extremely volatile exclude plot rl coinfigures cryptocurrencies monthly return  return value percent coin include experiment time series frequency minute contain data end period range july october coin trade throughout whole period time series start begin trading end trading stop note initial trading day zcash zec extremely volatile exclude plot rl coinreturnsfigures