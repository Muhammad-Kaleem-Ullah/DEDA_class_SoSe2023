personalize recipe recommender system use recurrent neural network master thesis submit prof dr stefan lessmann universit zu school business economics chair information system xun gong partial fulfillment requirement degree master science january content list iii list iv introduction related work work sequence aware recommender system work recipe recommendation methodology problem formulation model structure new recipe representation sequential model use rnn hybrid model experiment design dataset nlp task baseline model evaluation method experiment setting result comparison lstm recipe representation rq comparison static model rq case study conclusion ii list model architecture propose hybrid sequential model model architecture text classification task structure fold unfolded rnn basic structure lstm memory cell sne plot recipe meal type tag new recipe representation validation loss training process iii list summary statistic interaction dataset evaluation result sequential lstm model recipe representation vari ations evaluation result static model case study iv introduction record share cooking recipe human activity date back thousand year teng one old cookbook apicius usually thought compile st century ad family recipe pass generation cently rapid development internet mobile technology recipe share access easily emerge huge amount online recipe website recipe com epicurious com chefkoch de time large scale user recipe interaction data generate include implicit feedback like search click save well explicit feedback like rating comment rich data collection various food related computation task food visual perception analysis ofli restaurant specific food recognition bettadapura recipe recommen dation yang widely conduct health agriculture medicine biology gastronomy domain among task recipe recommendation system historically receive comparatively less attention even compare area relate leisure entertainment movie recommender system trattner elsweiler build successful recipe recommender system first step accurately understand user food preference harvey exist work recipe rec ommender system try model user general food preference static time agnostic algorithm however food preference static may change time wagner inspire sequence aware recommender system model user movie watch behavior zhao current project explore evaluate method capture user food preference two perspective short term behavior user cook next user short term intent could capture last action interaction log could signal short term interest drift contextual change example user browse turkey recipe likely browse stuff recipe soon long term behavior user like cook general long term behavior describes user long term dietary habit example vegetarian gluten free recently recurrent neural network rnn gain lot attention session base recommendation problem due recurrent structure variant rnn long short term memory lstm gate recurrent unit gru mean sophisticated hidden dynamic model even longer complex temporal depen dencies make suitable deal longer input user item interaction additionally number study show rnn base recommendation model could improve incorporate content base information user item hidasi cui song business value dataset use evaluate propose model generate one fast grow online recipe website kitchen story base germany company monetizes product placement partner toward large engage user base recommendation system brings business value case netflix year development personalization recommendation netflix able reduce user churn several percentage point gomez uribe similarly kitchen story give allowingtheir partner reach large audience base bring content base information also fight cold start problem help user discover item long tail return huge income business effective discovery critical anderson specifically also case meal kit meal planning service rather take popularity base recommendation approach provider could utilize improve recommender system capture dynamic preference provide diverse relevant offering http www kitchenstories com de food diversity health food special place importance almost every home culture food variety also positive effect happiness dietary diversity food variety also consider key predictable component dietary adequacy murphy additionally individual family prepare food home high diet quality tobey kitchen story app encourages people cook home provide engage relevant convenient experience sometimes however helpful change home cooking routine health also happiness reason recent poll question kitchen story mobile apps sample user state favor towards diverse routine recipe contribution summarize main contribution current project propose implement recipe recommender system use lstm best knowledge first sequential recommendation model domain food user level additionally seem first paper use cooking interaction action among recipe recommender system result demon strates propose sequential model performs well static benchmark model learn pre train new recipe representation content base recipe text description tag information use nlp model incorporate sequential model cope cold start problem model new recipe trainedembedding model however require less time train perform extensive experiment real world dataset evaluate model performance versatile metric include accuracy long term short term long sequence short sequence also diversity practical consideration run time convergence remainder paper first discus exist research technology recommender system section follow methodology elaboration section section address research question design empirical experiment lastly section final result represent answer research question brought section related work section first review related work sequence aware recommender system base technique use discus major work related recipe recommender system work sequence aware recommender system aim capture longer term user preference profile however real world many application scenario consider short term user interest longer term sequential pattern central success recommender current research call model consider information sequentially order user item interaction log sequence aware recommender system quadrana compare area recommender system sequence model overall less explore research due long last focus time agnostic model donkers dive research sequence aware want differentiate another type also brings time information model time aware recommender system clarify vinagre time aware considers time related information context feature set timestamps serve additional source information model enrich however sequence aware system focus often less exact point time order event next section first introduce conventional algorithm sequence model use recommendation task markov chain base model variant base step cut edge technology deep learn base model markov chain approach markov chain mc process make prediction sequentially capture represent process present state cumulative previous state base markov chain approach model sequential order assume next user action depends limited number recent precede action transition matrix estimate give transition probability shani suggest take recommendation process sequential decision problem propose markov decision process mdp base model several heuristic technique skip similarity cluster recommend book online user rendle first introduce hybrid technique factorize personalize markov chain fpmc brings matrix factorization markov chain together prob lem next basket recommendation present model base underlie user itemmatrix factorization approach sahoo propose hidden markov model hmm correctly interpret user product selection behavior make personalize recommendation domain online reading movie music consumption model user preference hidden markov sequence experiment mention project lot others show superi ority sequential model non sequential one however mc base model variation face sparsity problem computational complexity handle long range dependency lipton limitation augment increase demand model handle large datasets deep learn approach meantime deep learn technique recurrent neural network rnn also gain lot attention sequence model recurrent structure rnn well extract dependency among different sequence first usage rnn recommendation system introduce hidasi propose gru model solves session base recommendation problem quite common problem small medium size commerce news medium site user id across session tracked session length vary quite lot thus factor model like matrix factorization hard apply due absence user profile hidasi later author improve gru model implement novel rank loss function tailor rnn hidasi karatzoglou although session base model show great result model short term user interest within session situation knowledge user action last session past behavior quadrana propose hierarchical rnn base architecture model user activity across session evolution interest time add one additional gru level zhao propose joint model combine rnn mf approach gan framework movie recommendation joint model adaptively adjust contribution long term mf short term rnn information user movie mixed together however study session base session aware recommender system short termperformance hitratio isevaluated last interaction test sequence even though long term user preference model devooght bersini various evaluation metric assess short term long term performance apply result show rnns adapt session base collaborative filter also perfectly suit collaborative filter dense datasets outperforms traditional item recommendation algorithm base study villatel tweaked evaluation metric add parameter control number target interaction test set propose evaluation method shed light short term long term prediction ability rnn work recipe recommendation sequence aware recommender system frequently mention domain commerce medium recommendation much food early example food related recommender system chef hammond julia hinrichs use case base planning generate meal plan satisfy multiple interact constraint user berkovsky freyne first brought modern recommendation algorithm food domain simple user near neighbor knn content base method implement found significant improvement accuracy could achieve simply break recipe individual ingredient subsequently many study recipe recommendation put focus content base approach teng propose novel recipe recommender method construct two type network capture relationship ingredient complement network derive co occurrence substitute network derive user generate suggestion modification another breakthrough make ge elahi propose matrix factoriza tion approach food recommender system fuse rating information user supply tag achieve significantly well prediction accuracy content base standard trix factorization baseline health topic work elsweiler author try find trade user recommend user want nutritionally appropriate integrate nutritional information recipe diverse persona height weight gender age nutritional goal lose gain maintain weight activity level employedapost nutritional aspect yet another important factor food decision visual represen tation dish schur recently capability deep learn image recognition large body work area food analysis consider recipe image yang present food image analysis tool fooddist use deep convolutional neural network base multitask learn adaptive visual interface use personalize recommendation process help elicit user fine grain food preference achieve significantly well result baseline approach however real life datasets get interactive user preference explicitly gao propose visually aware recipe recommender system user recipe interaction history food image food ingredient jointly leveraged hierarchical attention network user food decision influence personal preference also food trend population especially lately extensive usage social medium west provide insight population wide dietary preference use log online recipe access proxy actual food consumption study reveals weekly annual periodicity nutrition intake well specific shift diet around major holiday use similar approach wagner enrich analysis show specific type ingredient popular specific day week kusmierczyk online recipe consumption rating also recipe production data example identify low interest fatty food german speak country sunday whereas people avoid fatty food friday study indicate temporal pattern exist behavior cooking ever work exploit sequential pattern explicitly user level recipe recommen dations methodology section first formulate sequential recommendation problem give overview propose hybrid model architecture introduce novel approach building new recipe representation model sequential pattern user item interaction lstm recurrent neural network finally present hybrid model combine sequential model user interaction content base recipe representation detail problem formulation sequential model user represent sequence item interact goal predict next item set item consume high probability briefly goal sequential recommendation model complete user sequence rather whole user item matrix conventional collaborative filter model current project task model predict possible recipe user cook base sequence recipe user cooked represent user long short term preference term food adopt formulisms smirnova vasile let denote list order item user interact item rn represent one hot encode vector dimension number item whole dataset additionally set additional attribute available item dataset prediction task find function determine candidate sequence user would maximize joint probability give first item sequence current work propose model use lstm recurrent neural network decomposition formulate eq detail definition cook user interaction data model structure model architecture propose hybrid sequential model overview propose hybrid model architecture illustrate model consists three module input module recurrent module output module input module first embed sparse recipe input one hot representation recipe id low dimensional dense vector incorporate content base information recipe step text recipe tag recipe input informative dense vector call new recipe representation recurrent module recurrent module applies sequential model method variant recurrent neural network long short term memory lstm capture sequential pattern user cooking behavior order list recipe user interact finally output module return probability distribution recipe base update hidden state value recurrent module last task pick top ranked recipe final recommendation follow section explain methodology generate new recipe representation build sequential model rnn new recipe representation sequential prediction often encounter cold start problem feedback sparse learn fine representation user item cui overcome situation explore way improve sequential recipe recommendation model new content base recipe representation learn recipe textual description tag recipe illustrate first formulate supervise classification task predict recipe tag give recipe text use lstm model propose two method generate recipe representation directly use output last hidden state lstm model take word vector train recipe text weigh term frequency inverse document frequency tf idf recipe model architecture text classification task benefit add additional content base information quality representation item could beneficial building efficient recom mender system however many rnn base sequence aware recommender system input contains item id many real life case get auxiliary information item like image video text description integrate information might help rnn model well understand user long short term preference hand cold start problem encounter problem especially col laborative filter approach could also alleviate extent incorporate know content information forbes zhu auxiliary information item could also help improve recommendation novelty recommend new item get little engagement user thus low exposure rate popularity bias last least recommender system able recommend item see training process new item come whole training process need update immediately incorporate content base information would solve problem building stand alone item representation could use input even receive interaction data use recipe textual feature build recipe representation use recipe textual feature build recipe representation context cooking decide cook general complex multi faceted process influence many factor available ingredient user cooking skill kitchen condition various context like day week seasonality festivity gao time recipe textual description give whole picture recipe ingredient use also heterogeneous information include cooking technique difficulty preparation time well utensil tool appliance require large body previous study wang mori maeta recipe structural analysis explore method represent recipe text flow graph whose vertex important word sequence cooking edge denote relation ship among however require large amount domain specific knowledge high cost recipe name entity rne annotation recently rnns proven effective general purpose approach capture dense hidden representation sequence model application text processing strobelt learn hidden representation could leveraged application machine translation sutskever text classification dai le however application hidden representation use item representation recommender system many standard language resource available recipe domain mori show use word vector learn general domain text newspaper article dictionary example sentence cause problem address problem mulate supervise learn task generate domain specific word vector representation recipe text next need find label supervise classification task achieve well search result online recipe usually associate number tag cuisine chinese french mexican course main dessert diet type vegetarian gluten free tag assign base mostly cooking method ingredient use lever recipe tag could train model objective recipe tag assignment give recipe textual description supervise learn task frame text classification problem use single layer lstm recurrent network trainable embed layer depict word embeddings get hidden representation recipe text propose two method transform recipe text use lstm model learn previous process recipe representation non linear transformation directly take output last hidden state lstm model linear transformation first compute tf idf vector recipe building bag word bow representation recipe multiply tf idf vector give recipe newly learn word vector represent weight matrix embed layer lstm model analogoustousingpre could integrate embed layer rnn model sequential model use rnn rnns due recurrent feedback mechanism well suit model complex dynamic user action sequence quadrana section first introduce plain vanilla rnns limitation explain structure important variantofrnns longandshort termmemory lstm network sequential pattern user item interaction propose hybrid recipe recommender system plain vanilla rnn first formulation recurrent like neural network hopfield network make johnhopfield rnns feedforward neural network rnns use internal state memory process sequence input enables network temporal processing learn dynamic sequence speech recognition natural language processing equation explain forward pas vanilla rnn represent hidden state current time step usually described memory network calculate base current input hidden state previous time step output current state weight matrix non linear activation function tanh ux vh wh structure fold unfolded rnn mention section standard rnn due recurrent chain like nature well suit deal sequential data however apply real world datasets versatile prediction task limitation become apparent problem long term dependency explore depth bengio describes difficulty learn connection item input sequence long problem cause exponentially update weight matrix loop nature rnn potentially result two extreme scenario value weight become large overflow call explode gradient problem initial weight matrix value small weight value result nan value vanish gradient problem termpredictions good longer term sequence overcome limitation rnn new variant rnn model long short termmemory lstm gru modelswereinvented next introduce lstm far one effective powerful rnns long short term memory network lstm lstm modify version rnns make easy remember past data memory lstm model solve vanish explode gradient problem us gate control memorize process thus able keep information long ago memory neural network basic structure lstm memory cell illustrate first part lstm memory cell forget gate decide information get thrown away cell state input sigmoid function output value range sigmoid function update weight bias hidden state input vector compute value input gate candidate value state memory cell time denote eq tanh sigmoid function update weight bias hidden state input vector give value input gate activation forget gate activation candidate state value compute memory cell new state time denote eq new state memory cell compute value output gate employ decide part cell state output subsequent output current cell tanh sigmoid function update weight bias similar feed forward neural network increase depth recurrent neural network add layer stack lstm output low level recurrent module use input high level recurrent module prediction output high level recurrent module use hybrid model represent hybrid model combine sequential model method content base recipe representation illustrate hybrid model consists three module input module recurrent module output module section first specify input output three module single user case well understand loss function optimization method define input module input module transforms recipe order user interaction log new recipe representation vector input list recipe user interact recipe encode one hot vector dimension index difference zero corresponds index particular recipe total number recipe output input module map recipe new recipe representation xrrep rrep represent new recipe representation generation method pro rrep pose xrrep represent new recipe representation vector dimension feature dimension define new recipe representation recurrent module recurrent module could single layer lstm stack lstm layer input vectorwithdimension illustrate input time step output lstm layer take sequence input update current hidden state use current input xrrep previous state output hidden unit process output gate come dimension dimension hidden unit last layer lstm model xrrep rec represent sequence model method lstm rec output module output module consists dense layer final output input dense layer mapped output recurrent module dimension total number item linear transformation current project output softmax function apply output dense layer obtain proba bility distribution item softmax exp exp exp oi exp oi loss function interaction current project kind implicit feedback item optimization turn binary cross entropy minimization problem ground truth actual next item add item time step calculate loss eq predict probability item next item time step loss log log optimization method define loss function task turn perform optimization field deep learn gradient descent popular optimization algorithm stochastic gradient descent sgd batch gradient descent bgd two common variation gradient descent algorithm sgd iteratively update model go use random sample training dataset result faster training certain type problem mini batch gradient descent mbgd approach use current project brings efficiency bgd still able take advantage iterative training approach sgd expedite optimization procedure downside sgd sometimes require many iteration manually tune learn rate achieve convergence could time consume therefore adagrad use learn appropriate input parameter mbgd adagrad algorithm learn rate adapt component wise parameter incorporate knowledge past observation algorithm give frequently occur feature low learn rate infrequent feature high learn rate intuition time infrequent feature see learner take notice thus adaptation facilitates find identify predictive comparatively rare feature duchi experiment design section design implement empirical experiment propose model real life recipe dataset design experiment try answer follow research question lstm model propose sequential lstm model improve upon state art static rec ommendation method respect long short term recommendation accuracy artstatic recommendation method respect recommendation diversity research question context remainder section first give summary recipe dataset use result exploratory analysis experimental setting follow answer research question dataset dataset use training recommender system obtain authoriza tion kitchen story tech business intelligence team app brings monthly active user mio mostly germany united state china company us analytics tool call amplitude track event within app web client user interaction app include recipe either view like save favorite dataset use derive interaction data http amplitude com user interaction data user interaction data consist one year user interaction timeframe october september io platform whole project user interaction refers action cooking impute user metric directly measure single user interface event understandably since never know sure actually cooked recipe kitchen story internal analysis cooking define intention cook estimate give interaction fulfil action add ingredient shopping list adaption serve portion usage cooking mode length stay minute recipe detail page user cooked less recipe give timeframe filter would like measure relevant user actually use app make wealsofiltered suspicious super long sequence longer data preprocessing process additionally remove duplicate item interaction sequence kept record late timestamp give summary statistic interaction data user item interaction sparcity average sequence length summary statistic interaction dataset enter full screen step big text simulation printing digital format recipe data recipe core content kitchen story platform two type recipe one type create internal editor company go entire recipe content creation process sometimes lead professional chef iteration test cooking ensure recipe quality type user generate recipe submit community member approve internal content team kitchen story platform extract even though recipe recommendation recipe create internal editor include avoid data bias quality user generate recipe varies since less scrutiny put process creation base criterion total recipe dataset recipe tag recipe manually tag content team various tag category preprocessing process tag query database difficulty easy medium difficult meal type main dessert drink salad soup diet type vegetarian non vegetarian alcohol free alcoholic cuisine cuisine chinese vietnamese italian etc though generally accurate due manual nature tag process tag category tag strict adherence sometimes tag granularity reason cuisine exclude input model nlp task process building new recipe representation could take nature lauguage processing nlp task follow section detail text preprocessing process lstm language model set text classification task recipe text input standardize data cleaning process use include tokeniza tion lemmatization remove stop word punctuation final matrix pad completeness unique word derive recipe long description length word recipe represent pad list id represent word three recipe tag chosen output target difficulty meal type diet type discuss previously cuisine tag use many miss value three target variable filter record miss value target multi class label dimension respectively three output use categorical crossentropy loss function custom weight calculate total loss difficulty meal type diet type respectively weight arrange consider classification difficulty tag quality example diet type sometimes inaccurate clear rule define vegetarian tag dessert recipe thus medium weight assign correspond output adam optimizer use set original paper kingma ba output dense layer softmax activation function apply model hyperparameter setting embed dimension dropout rate shallow lstm layer hidden cell batch size set epoch epoch already get quite good result validation accuracy three output save model weighs input recipe text get output lstm hidden layer visualization new recipe representation another beneficial aspect recipe representation use visualize recipe similarity usually dimension reduction technique pca sne use get presentable dimension sne plot show see two recipe representation recipe similar meal type cluster together indicates recipe representation could capture recipe meal type feature pretty well ready input recipe representation sequential lstm model well capture temporal dynamic user interaction sne plot recipe meal type tag new recipe representation left one generate lstm hidden state method right one genrated tf idf method baseline model compare propose sequential lstm model flow non seqeuntial state art baseline model pop always recommends popular item pop strong baseline depend nature data villatel dataset top recipe account interaction cover user bpr mf bpr mf rendle state art matrix factorization method top recommendation base bpr rank loss user pairwise preference proven perform especially good implicit user feedback mf model de compose observe user item interaction matrix user item latent factor learn rate fix best performance al mf al mf hu factor model specially tailor implicit feedback recommenders treat data indication positive negative prefer ences associate vastly vary confidence level current project treat interaction equal confidence level implement mf model use pre built package implicit developed benfred source code available github important parameter mf number latent factor set number hidden unit lstm keep default value implicit parameter http github com benfred implicit evaluation method section introduces evaluation method use compare result model definition offline evaluation methodology apply current project due practical con straints adopt give next item evaluation protocol summarize quadrana give user sequence length first item interaction sequence reveal model input remain item held target take list item ranked first top prediction item recommendation set worth mention since model design predict next item top recommendation list actually consists probable item appear next sequence evaluation accuracy use follow measurement model accuracy metric set rank position recall recall define number correct recommendation divide number target item specify parameter control length target item measure model performance give short term long term recommen dations respectively target next item usual approach sequential recommendation next basket prediction commerce equal exact next item present recommendation set otherwise metric evaluates ability model capture user short term interest drift length held item target next item depend sequence length split input target metric capable evaluate ability model predict user long term preference trend ndcg normalize discount cumulative gain recall take top item equivalent item without rank order ndcg us monotonically increase discount emphasize rank order assigns high score true positive rank high position target list important order recommendation matter sometimes mobile app user may scroll see low ranked item evaluation diversity addition accuracy evaluate recommendation model secondary dimension factor diversity measure item coverage number distinct item correctly recommend blockbuster return percentage correctly recommend item top popular item intra list similarity intra list similarity metric first introduce ziegler define eq similarity two item calculate cosine similarity respect co occurrence user item matrix eq pairwise similarity item recommendation set sum get intra list similarity intra list similarity cossim user like cossim prefs prefs denotes user denote item denotes set user denotes top item recommend item user experiment setting experiment current project implement use kera library ten sorflow framework full code available series jupyter notebook py file github preprocessing sub sequence purpose training model diverse sequence length original interaction sub length original sequence sub sequence last item use target item computation loss function first item fed rnn logic illustrate maximum input length state devooght bersini maximum input length rnn usually fix difficult parameter choose influence training efficiency quality current project set max length input sequence consider sequence length distribution analyze training time input sequence training set validation set test set longer last item kept train test split split user interaction dataset train validation test set respectively keep recipe appear train set user test set first half recipe cooked give input model second half use target evaluation length even number target set get one item matrix factorization base model give recommendation user exist training process first half interaction test user include training process model unfair unfortunately unavoidable advantage method described devooght bersini sequence training set split sub sequence described validation set split half logic test set use model tune training process summary total sub sequence training sequence validation test hyperparameter setting lstm hyperparameter experiment set use stand shoulder giant base dataset statistic quite similar dataset movielen test devooght bersini parameter set reference original paper impact hyperparameters already analyze hidden unit size bydefault vasile hidasi devooght bersini also show small hidden dimension model performance severely limited stack lstm layer implement simplest version add second layer half hidden unit size first layer hidden unit size stack lstm layer loss function categorical cross entropy cce mention devooght bersini categorical cross entropy cce common loss language model suitable large item catalog since item cce use throughout training process optimizer analogous standard set deep learn experiment mini batch gradient descent use adaptive learn training process adagrad initial learn rate use training procedure set epoch training process batch size epoch validation set use evaluate model generalization also implement early stop method training process stop validation loss improve consecutive epoch best model low validation loss save result section answer research question rqs section base result empirical experiment comparison lstm recipe representation rq metric model recall recall ndcg itemcoverage blockbuster intralist rnntype reciperep allseq shortseq longseq similartiy lstm id lstm lstm lstm tf idf lstm lstm init stackedlstm id stackedlstm lstm init reciperep recipe representation method seq sequence represent breakdown short term accuracy sequence length cutoff length evaluation result sequential lstm model recipe representation variation present evaluation result sequential lstm model recipe repre sentation variation list key finding general model trainable embed layer high short term pre diction success rate pre train embeddings reasonable whole lstm model train optimize short term prediction thus embeddings layer also optimize short term prediction accuracy model use content base recipe representation id include fix trainable embeddings high accuracy longer sequence term short term prediction set single layer lstm could explain two reason first longer sequence high probability user cook popular dish model pre train embed layer tend recommend hit often trainable embed model second sequence longer might capture multiple interest drift seenlist trainableembeddings whichisthemostshort term optimize model content base info include fix trainable embeddings low item coverage rate high intra list similartiy mean recommendation give less diverse usually dish type example pancake pasta could explain two reason first add recipe textual information embed layer model recognizes recipe easily also easy find similar recipe second pre train embed layer model tends overfit earlier without thus result shorter training time term training efficiency see model lstm hidden state recipe representation green line td idf recipe representation orange line start low validation loss compare model recipe id representation red line however two model start overfit earlier model recipe representation initialization grey line performs rather stable low validation loss begin surpass model recipe id representation red line end result see add second lstm layer short term prediction accuracy increase improvement mostly due successful prediction longer sequence indicates high complex ity nonlinearity could beneficial training recurrent layer learn representation data summary answer rq pre train content base recipe representation add limitation sequential lstm model training process could helpful use initialization matrix embed layer use directly training time limited validation loss training process model lstm hidden state recipe representation green line model tf idf representation orange line model recipe representation initialization grey line start low validation loss compare model recipe id representation red line start overfit sooner comparison static model rq metric model recall recall ndcg itemcoverage blockbuster intralist allseq shortseq longseq similartiy pop bpr mf al mf stackedlstm id evaluation result static model present evaluation result static benchmark model propose sequential lstm model best result part list primary finding rq model base item popularity achieves comparable result due popularity bias nature data especially long term metric al mf strong baseline model term accuracy bpr mf outperforms al recommend diverse item sequential lstm model outperforms two mf base model eval uation metric regard accuracy especially short term prediction seq accuracy high uplift al mf however term diversity mf base model show superiority sequential lstm model see mf base model much low intra list similarity score blockbuster rate answer last two research question propose sequential lstm model show consistent improvement short term long term accuracy metric static model underperform term recommendation diversity case study next give quintessential example recommendation give follow three kind recommendation model present current project single layer sequential lstm model lstm hidden state recipe representation single layer sequential lstm model recipe id representation al mf model example illustrates sequence aware recommendation model predict short term interest drift frequently occur ingredient food replace scanningofresults user initially interested main course recipe later preference shift cooking dessert compare actual target three set recommendation two primary finding two sequential lstm model give high weight short term sweet dessert preference benchmark model al mf still recommends many savory main course could explain sequence agnostic nature static recommendation model additionally lstm content base recipe representation picked latent feature chocolate preference recently view item caramel nutella lava cake zebra bundt recipe step able recommend chocolatey recipe versus lstm model without content base info picked dessert preference know chocolate preference one possible way explain result incorporate recipe text recipe related feature like ingredient might leveraged calculate recipe similarity thus model content base info able capture chocolate hidden dynamic user preference case study give user cooked list recipe top left top recommend recipe three model true target list bottom conclusion goal current research build sequence aware personalize recipe recom mender system could capture user long short term cooking preference motivation propose sequential lstm model new content base recipe representation follow step do research state art sequential recommender system preprocess data real word track data design hybrid model combine sequential model method content base recipe representation implement propose model kera deep learn framework fine tune deep learn related parameter implement state art recommendation algorithm benchmarking evaluate model performance concern versatile metric compare propose sequential model different configuration non sequential model quantitatively qualitatively future work follow aspect would consider improvement integrate user general preference context vector improve long term predic tion accuracy apply attention mechanism user level item might different impact different user likerecipephotos whichmightaffect user decision begin implement custom loss function find well balance accuracy di versity training process anderson long tail future business sell less ed internat ed hyperion bengio simard frasconi learninglong descent difficult ieee transaction neural network bottou large scale machine learn stochastic gradient descent cui wu liu wang visual textual recurrent neural network sequential prediction dai le semi supervise sequence learn arxiv devooght bersini arxiv devooght bersini long short term recommendation recurrent neural network proceeding th conference user model adaptation personalization umap donkers loepp ziegler sequential user base recurrent neural net work recommendation proceeding eleventh acm conference recommender system recsys duchi hazan singer adaptive subgradient method online learn stochastic optimization forbes zhu content boost matrix factorization recommender system experiment recipe recommendation freyne berkovsky intelligent foodplanning personalizedrecipe recommen dation proceeding th international conference intelligent user interface iui freyne berkovsky smith recipe recommendation accuracy reason konstan conejo marzo oliver ed user model adaption personalization vol pp springer heidelberg gao feng huang guan feng ming chua hierarchical attention network visually aware food recommendation arxiv ge elahi ferna ndez tob ricci massimo use tag latent factor food recommender system proceeding th international conference digital health dh par harvey ludwig elsweiler eat learn user taste rating prediction kurland lewenstein porat ed string processing information retrieval vol pp springer international publishing liao zhang nie hu chua neural collaborative filter proceeding th international conference world wide web www hidasi karatzoglou recurrent neural network top gain session base recommendation proceeding th acm international conference information knowledge management cikm hidasi karatzoglou baltrunas tikk session base recommenda tions recurrent neural network arxiv hidasi quadrana karatzoglou tikk parallel recurrent neural network architecture feature rich session base recommendation proceeding th acm conference recommender system recsys hidasi quadrana karatzoglou tikk parallel recurrent neural network architecture feature rich session base recommendation proceeding th acm conference recommender system recsys hu cao wang xu cao gu diversify personalize rec ommendation user session context proceeding twenty sixth international joint conference artificial intelligence hu koren volinsky collaborative filter implicit feedback datasets eighth ieee international conference data mining khan rushe smyth coyle personalize health aware recipe recommendation ensemble topic model base approach arxiv kingma ba adam method stochastic optimization arxiv kusmierczyk trattner rv temporality online food recipe proceeding th international conference world wide web www companion le ngiam coates lahiri prochnow ng opti mization method deep learn liang altosaar charlin blei factorization meet item embed regularize matrix factorization item co occurrence proceeding th acm conference recommender system recsys lipton berkowitz elkan critical review recurrent neural network sequence learn arxiv ab liu lee achananuparp lim cheng lin character izing predict repeat food consumption behavior time intervention arxiv ab org ab maeta mori sasada framework recipe text interpretation proceedingsofthe compute adjunct publication ubicomp adjunct marin biswas ofli hynes salvador aytar weber torralba recipe dataset learn cross modal embeddings cooking arxiv rg ab org ab min jiang jain food recommendation framework exist solu tions challenge arxiv ab org ab min jiang liu rui jain survey food compute arxiv mori sasada yamakata yoshino machine learn approach recipe text processing quadrana cremonesi jannach sequence aware recommender sys tems arxiv quadrana karatzoglou hidasi cremonesi personalize session base recommendation hierarchical recurrent neural network proceeding eleventh acm conference recommender system recsys rendle freudenthaler gantner schmidt thieme bpr bayesian personalize rank implicit feedback arxiv stat rendle freudenthaler gantner schmidt thieme bpr bayesian personalize rank implicit feedback arxiv stat ab sahoo singh mukhopadhyay hidden markov model collaborative filter ing mi quarterly salvador hynes aytar marin ofli weber torralba learn cross modal embeddings cooking recipe food image ieee conference computer vision pattern recognition cvpr sanjo katsurai towards recommend diverse seasonal cooking recipe preliminary study base monthly view data ieee international symposium signal processing information technology isspit schur kleinhans goldberg buchwald schwartz maravilla activation brain energy regulation reward center food cue varies choice visual stimulus international journal obesity shani brafman heckerman mdp base recommender system arxiv smirnova vasile contextual sequence model recommendation recurrent neural network arxiv ab song yang cao xu neural collaborative rank arxiv strobelt gehrmann pfister rush lstmvis tool visual analysis hidden state dynamic recurrent neural network arxiv sutskever vinyals le sequence sequence learn neural network arxiv ab org ab teng lin adamic recipe recommendation use ingredient network proceeding rd annual acm web science conference websci trattner elsweiler challenge future research direction arxiv ab trattner elsweiler evaluation recommendation algorithm online recipe portal trattner elsweiler online data say eat habit nature sustainability trattner moesslang elsweiler predictability popularity online recipe epj data science villatel smirnova mary preux recurrent neural network long short term sequential recommendation arxiv stat ab vinagre jorge gama overview exploitation time collaborative filter exploit time collaborative filter wiley interdisciplinary review data mining knowledge discovery wagner singer strohmaier nature evolution online food preference epj data science wagner singer strohmaier nature evolution online food preference epj data science wagner singer strohmaier spatial temporal pattern online food preference proceeding rd international conference world wide web www companion wang li li dong yang substructure similarity measurement chinese recipe proceed th international conference world wide web www west white horvitz cooky cook insight dietary pattern via analysis web usage log proceeding nd international conference world wide web www wu ahmed beutel smola jing recurrent recommender network proceeding tenth acm international conference web search data mining wsdm xiao liang meng dynamic collaborative recurrent learn pro ceedings th acm international conference information knowledge man agement cikm yang hsieh yang pollak dell belongie cole estrin yum personalize nutrient base meal recommender system acm transaction information system ying zhuang zhang liu xu xie xiong wu se quential recommender system base hierarchical attention network proceeding twenty seventh international joint conference artificial intelligence yu liu wu wang tan dynamic recurrent model next basket recommendation proceeding th international acm sigir conference research development information retrieval sigir zhang yuan lian xie collaborative knowledge base embed recommender system proceeding nd acm sigkdd international conference knowledge discovery data mining kdd zhang dai xu feng wang bian wang liu sequential click prediction sponsor search recurrent neural network arxiv zhao wang yang ye zhao chen shen lever age long short term information content aware movie recommendation via adversarial training ieee transaction cybernetics declaration authorship xun gong hereby confirm author master thesis independently matter take publication source marked jan