neural network quantitative finance master thesis submit prof dr wolfgang ardle institute statistic econometrics case center apply statistic economics universit zu enzo giacomini partial fulfillment requirement degree master art december declaration authorship hereby confirm author master thesis independently without use others indicate source passage literally general matter take publication source marked rd december enzo giacomini content introduction neural network perceptron perceptron linear discriminant simple example error function delta rule learn classification task non linear separable set xor step function neuron propagation function activation function neural network neural network graph multi layer perceptron network mlp radial basis function network rbf representation power neural network statistical learn neural network learn problem loss function risk function set loss function empirical risk minimization neural network learn error surface descend gradient descend gradient neural network example xplore backpropagation application neural network time series forecasting autoregressive structure financial time series exogenous indicator example xplore neural network volatility estimation estimation conditional volatility example xplore estimation imply volatility example xplore experiment model time series transformation time dependency network performance measure result comment summary conclusion descgrad xpl example tsnn xpl example condvolrbf xpl example volsurfrbf xpl example volsurfmlp xpl example list perceptron weight threshold hyperplane separate set boolean function yield black white separate hyperplane separate hyperplane boolean function xor yield black white xor reproduce perceptron act output perceptrons neuron sigmoid function red cx dot red black step function limit case feedforward neural network nn mlp descend gradient forecast red exchange rate japanese yen dollar blue rbf network hidden unit lag forecast red exchange rate german mark dollar blue mlp network hidden unit lag log return conditional volatility exchange rate british pound dollar estimate rbf network hidden unit lag log return conditional volatility commerzbank stock estimate rbf network hidden unit lag imply volatility surface estimate use rbf network hiddenunits parameter data eurex imply volatility surface estimate use mlp pa rameters strike price maturity data german swiss future exchange eurex list function output perceptron output function output perceptron output function error function xor function output hidden layer linear separable time series sample size performance network jpyusd performance network demusd performance network bpusd introduction neural network powerful tool modern quantitative finance present field related semi non parametric regres sion pattern classification like time series prediction risk estimation credit score however mathematical concept involve neu ral network considerably complex theoretical aspect yet completely developed consequence implement net work choice parameter architecture may make without enough theoretical support thus much knowledge neural net work come heuristical approach practitioner developed apply real problem aim essay overview neural network quantitative finance application wide approach facilitate comprehension basic concept structure related implementation financial environment work divide three part first section introduces basic element constitutes neural network neuron charac teristics second part present application neural network finance related semi non parametric regression time series fore cast estimation conditional volatility estimation imply volatilitysurfaces network different architecture use forecast three exchange rate time series result prediction evaluate compare ac cord network architecture use section contain graphic practical example neural network use real data result illustration generate use library code xplore gather bibliography contains classical neural network deeper computational mathematical treatment found neural network perceptron model simple learn machine stay basis work neural network developed rosenblatt solve pattern recognition problem separate data two differ ent category attempt model mathematically neurophysiologic element perceptron biological neuron emit electrical pulse concentration chemical substance environment reach certain level sit uation happens neuron say excite state synapse emission electrical sign take place behaviour biological neuron mathematically simulated perceptron perceptron element weight sum input mathe matical analogy capture feature chemical environment compare result predefined threshold value concentration level chemical substance biological neuron excite weight sum input great threshold value percep tron release output otherwise analogous emit synapsis concentration level reach formulate precisely perceptron function transform dimensional input binary output rn component input rn weight rn sum sum compare threshold value great perceptron excite emit pulse represent step function writting indicator function get thus perceptron compose function whose output express alternatively one incorporate threshold value notation define new vector rn rn write output perceptron perceptron input weight graphically represent perceptron hhj perceptron weight threshold perceptron linear discriminant beyond reproduce behavior neurological counterpart percep tron may work linear discriminant see bishop mean perceptrons separate classify element belong different set give set linear separable two set dimensional space linearly separable exist real number element element set separate hyperplane give function rn linearly separable set element whichf hyperplane separate set becomes vector notation separate hyperplane hyperplane separate set give two linear separable set rn hyperplane described equation determine perceptron weight bias thus perceptron work classifier divide function input space two sub space classify input variable belonging either one another sub space set follow see note vector weight bias control position separate hyperplane simple example boolean function mapping take value thus function consider classify input see note function linearly separable function function boolean function yield black white outputy output function perceptron mimic function correctly set one possible sep arating hyperplane classifies input correspond output mention depends parameter weight bias use give determine position separate hyperplane parameter perceptron set separate hyperplane yield output output function see separate hyperplane separate hyperplane separate hyperplane ub separate hyperplane output perceptron output function error function order evaluate good classification obtain perceptron correct classification output perceptron function must compare purpose error function counting number input misclassified perceptron define set contains element call pattern error function pattern correspond input respective output set call training set thus error function sum misclassifica tions produce perceptron parameter pattern moreover error function depends parameter perceptron natural result one reminds position separate hyperplane determine parameter change weight yield another separate hyper plane show one misclassification mean show delta rule learn classification task best classification one mean per ceptron classifies input correctly obtain best classification weight setq found aprocedure call delta rule correct weight iteratively obtain say delta rule perceptron perfectly learns classification task delta rule consists follow step start randomly chosen weight value training set learn rate define iteration counter repeat element correctly classify change weight element misclassified change weight accord xor function element training set correct classify conver gence delta rule prove haykin non linear separable set set linearly separable single perceptron able perfectly classify element obtain best possible classification calledhidden layer xor basic example set linear separable set form linearly separable function xor mapping see xor function boolean function xor yield black white one realize separate hyperplane divide space set contain element output xor thus single perceptron correctly classify input well classification attain use perceptrons introduce hidden layer new perceptrons transform input space linear separable one release perceptron subsequent layerlinearly separable input one architecture reproducethe xor function hidden layer hn hj xor xor reproduce perceptron act output per ceptrons value parameter hidden layer perceptrons value parameter output perceptron input output yield parameter output hidden layer linear separable step function step function continuous consequently differentiable whole domain fact represent disadvantage practical implemen tations calculation require derivative output related input perceptron need overcome hurdle another function use specially sigmoid function sigmoid function continuous approximates step function well differentiable whole domain generally function use place step function situation one talk perceptrons anymore neuron neuron basic element neural network neuron building block neuron represent mapping rk transform dimen sional input real number neuron function compose propagation function rk activation function output argument thus neuron general form output bound consider stan dardization bound output polynomial degree call order neuron coefficient consider parameter neuron neuron ppq neuron propagation function common neuron propagation function weight sum input equation activation function activation function neuron may assume many form example linear function ax identity function binary function rn neuron call binary neuron two example activation function binary neuron binary step function bipolar step function generally non linear form rn specially inter esting sigmoidal function step function described limit case standard sigmoid function cx sigmoid function converges binary step function hyperbolic tangent function cx tanh cx cx hyperbolic tangent function converges bipolar step function function opposite step function continuous important characteristic property useful practical calculation neural network see subsequent section neuron basic unit organize certain way give origin neural network neural network representation power much high single neuron subject next section sigmoid function sigmoid function red dot cx red black step function limit case neural network artificial neural network mathematical representation biological neu ral structure produce mapping rn rm nn write nn neural network graph neural network compose interconnect neuron dispose accord certain architecture suitable represent graph neuron represent node associate arc connect node value represent weight parameter polynomial coefficient propagation function neuron besides node associate two value dimension input dimension output set node call input layer set non empty exists least one node set node call output layer set non empty exists least node level node long length input node node depth neural network length long path input node output node set node level belong input output layer call hidden layer graph acyclic direct graph output node serve input node low level case network call feedforward network network threshold value incorporate input nn vector rn output vector rm represent neural network ppq hj hj au ap ap pq hp hpq hj au ppq input layer hidden layer hidden layer output layer feedforward neural network nn multi layer perceptron network mlp output neuron sigmoidal identity function call multi layer perceptrons mlp network rn rm mlp compose input layer output layer without hidden layer call single layer perceptrons unit hidden layer denominate mlp component release neuron output layer function input parameter write compact weight input vector show graph neural network mlp mlp radial basis function network rbf radial basis function rbf neuron neuron propagation function form rn input weight activation function form radial symmetric func tion commonly gaussian function also generalize inverse multi quadric function thin plate spline function generalize multi quadric function network multi layer perceptron xxx hj jh hj hj jjj hj rhj jjj hj hj hj hj rj lj il mlp one hidden layer contain rbf neuron pr output neuron propagation function iden tity activation function call rbf network rn rm rbf neuron rbf hidden layer component output give neuron propagation function calculates close use case euclidian distance input vector vector gaussian activation function release high value input vector close vector small value input far away thus weight form cluster input space representation power neural network mlp rbf network huge representation power high number neuron single hidden layer high complexity represent function prove neural network one hidden layer enough number neuron replicate function practi cal purpose however network neuron distribute one hidden layer prefer function replicate unknown realization available neural network use extract relation form realization next section dedicate find best network replicates least approximate unknown function base realization statistical learn neural network main purpose statistical learn build model represent unknown process base data set contain empirical realization process model extract much information possible data good generalization property make good prediction future realization process section follow idea developed vapnik vidyasagar learn problem give set independent variable follow identical unknown proba bility distribution function unknown process input data produce output distribute model also call machine approximate process use parameter loss function order find best approximation measure quality necessary function express god performance model consequently function argument true output process output approximation function measure distant value function call loss function risk function get measure performance loss function sum possible value unknown distribution order express expect loss also call risk function df get good approximation risk function must minimize thus problem statistical learn risk minimization problem determine distribution unknown set loss function choice appropriate loss function determines kind problem represent loss function indicator function indicates whether model correctly classifies input output model interpret classifier achieve note error function use evaluate performance single perceptron data set loss function take form minimization risk function becomes least square regression estimation loss function density function ln minimization risk function becomes density estimation empirical risk minimization practical application distribution unknown risk function evaluate risk function substitute empirical function base give data set set input output call see training set empirical risk define emp learn process thus base empirical risk minimization erm set function min use loss function risk function becomes emp erm equivalent least square method use loss function ln risk function come ln emp erm equivalent maximum likelihood method general framework neural network represent machine able learn process base empirical data empirical risk minimiza tion many possible neural network structure structure represent function use learn process addition define loss function determines purpose neural network neural network learn neural network represent mapping rn rm transform nn input rn output rm furthermore set parameter determine architecture network influence mapping thus general neural network write nn one main purpose neural network reproduce approximate unknown functional relation rn rm variable rn rm training set empirical realization relation know thus target suppose form neural network input parameter yield nn nn output network differs target output nn framework precede section statistical learn ing machine neural network suppose extract much information possible training set generalize well future realization unknown process extraction information make empirical risk minimization parameter network chosen order minimize sum loss pattern training set minr emp loss function define minimize parameter found equivalent find global minimum space error surface use loss function square difference target output sum loss function pattern becomes sum square error characteristic remains multiply constant xx sse empirical risk function becomes mean square error mse function parameter network mse sse function view surface space call error surface descend gradient minimization problem min consists find weight neural network error surface reach global minimum among numerical method use solve minimization problem method descendent gradient show suitable property start initial correspond parameter correct iteration discrete step opposite direction gradient gradient zero parameter modify anymore minimum reach gradient risk function must determine iteration change parameter proportional negative gradient learn rate factor give magnitude change parameter influence convergence method thus step parameter modify accord iteration stop gradient zero cost function small value desire precision enough iteration parameter may reach value gradient change anymore minimum function attain however minimum local one global minimum inthiscase thedescending gradient method solve learn problem descend gradient neural network evaluate gradient neuron one remind structure base propagation activation function use loss function sse element gradient single neuron calculate partial derivative cost function respect output neuron output layer give derivative respect propagation function give finally derivative respect weight give derivative together yield call write note derivation verify delta rule standard percep trons see necessary take derivative function implement descend gradient method therefore propagation function must differentiable verify step sign function case perceptrons substitute function sigmoid hyperbolic tangent function allows implementation learn process implement error surface learn weight descend gradient example xplore example error surface descend gradient method example generate xplore sfmdescgrad xpl list single neuron sigmoid activation function train set value batch iscalculated opposite direction gradient vector method stop fix number iteration backpropagation version descendent gradient method apply neural network call backpropagation base recursive derivation net work derivation generalize weight hidden layer input layer whole network prove error neuron compose sum error neuron input output neuron jk therefore change weight output neuron hidden neuron jk backpropagation network feed pattern training set neuron derivative propagation function output neuron save error neuron calcu lated use equation change parameter compute parameter correct accord sum error pat tern batch backpropagation step accord error pattern online backpropagation method convergence strongly dependent choice initial weight case however parameter guarantee convergence know choice random see learn process descend gradient backpropagation refer duda hart stork haykin application neural network time series forecasting many time series contain unknown autoregressive structure future realization depend past realization neural network use extract estimation autoregressive structure hidden realization time series procedure network learns charac teristics dependency optimization parameter weight described section afterwards network may use fore cast future value series autoregressive structure time series vector represent past window size vector represent future window size displace step ahead non linear relationship suppose exist window time series form rp rf express function estimate neural network input nn neuron output neuron network must train use past window series size input vector future window size target vector training set define training neural network yield nn example model represent non linear autoregressive process order nn financial time series forecasting financial time series high frequency data daily hourly realization time consider non linear function past realization process call non linear autoregressive process order nlar see franke ardle hafner non linear relationship rp suppose exist set value form autoregressive structure approximate neural net work rp parameter nn nn sse use cost function network parameter argmin nn best approximate series least square sense prediction realization nn exogenous indicator past realization also realization another time series represent economic financial indicator process call non linear autoregressive process exogenous component order nlarx process realization time exogenous time series represent xh define xh non linear relationship suppose exist form rp express xh autoregressive structure approximate neural net work rp parameter nn xh nn training set construct use input vector output realization sse use cost function network parameter argmin xh nn best approximate series least square sense prediction realization xh nn example xplore use tsnn xpl see time series contain exchange rate japanese yen dollar german mark dollar use log difference transformation train rbf network hidden unit mlp network hidden unit respectively input unit lag value series output unit value series thus representation described section nlar process training network apply test set forecasting result well original series plot japanese yen dollar german mark dollar rbf forecast jpyusd test set forecast red exchange rate japanese yen dollar blue rbf network hidden unit lag mlp forecast demusd test set forecast red exchange rate german mark dollar blue mlp network hidden unit lag neural network volatility estimation estimation conditional volatility neural network use estimate conditional volatility finan cial time series consider time series stochastic volatility follow ar arch process form xh xh define xh rp rp write ar arch process follow var use neural network approximate obtain nn nn argmin nn use neural network approximate obtain nn nn argmin nn estimator obtain non negativity guaranteed special condition architecture network avoid constraint nn nn approach residual substitute sample residual see franke ardle hafner residual write follow approximate residual sample residual withparameters nn argmin nn estimation conditional volatility write nn example xplore use condvolrbf xpl see time series contain ex change rate british pound dollar use log difference trans formation train rbf network hidden unit input unit laggedvalues ries value thus series suppose follow ar arch process step described section conditional volatility estimate use rbf network hidden unit log return original series estimate conditional volatility plot procedure apply series commerzbank stock val ues rbf hidden unit use time dependency lag log return original series estimate conditional volatility plot estimation imply volatility black scholes model price call option time give formula ke ln st spot price underlie asset volatility underlie asset price process risk free interest rate time maturity strike price option cumulative distribution function normal distribution black scholes model assumes constant price process give underlie asset real situation option price underlie price time observable strike price time maturity settle contract volatility however observable possible obtain volatility imply option price value solves black scholes equation another parameter one obtain imply volatility invert black scholes formula xplore do two different numerical method bisection newton raphson quantlet implvola xpl opposite theoretical formulation imply volatility constant form smile plot strike price time see ardle kleinow stahl change also accord time maturity log return time conditional volatility time log return conditional volatility exchange rate british pound dollar estimate rbf network hidden unit lag log return time conditional volatility time log return conditional volatility commerzbank stock estimate rbf network hidden unit lag possible estimate dependency imply volatility another parameter like strike price moneyness time maturity special practical interest estimation relation may non linear form estimate neu ral network give imply volatility strike price moneyness different maturity available constitute training set network nn nn xn argmin nn use estimate volatility grid maturity strike price moneyness produce imply volatility surface example xplore xplore volsurfrbf xpl volsurfmlp xpl list estimate imply volatility surface data set volsurfdata dat use neural network data set contains settlement price dax underlie asset strike price interest rate time maturity price put call trade german swiss future exchange imply volatility surface estimate rbf network hidden unit show imply volatility surface estimate mlp network hidden unit show picture also show imply volatility curve red use estimation surface volatility surface rbf network imply volatility surface estimate use rbf network hidden unit parameter moneyness maturity data german swiss future exchange eurex volatility surface mlp network imply volatility surface estimate use mlp pa rameters strike price maturity data german swiss future exchange eurex experiment model forecast time series neural network involves sometimes heuristical choice parameter related architecture network experiment described section aim compare one step ahead forecast time series produce mlp rbf network different architecture three different time series eight different architecture use section non linear time dependency size lag consider series experiment us network one hidden layer nn contain neuron forecast realization time series nn afterwards performance forecast evaluate time series time series use contain daily observation exchange rate japanese yen dollar jpyusd exchange rate german mark dollar demusd exchange rate british pound dollar bpusd transformation eliminate trend seasonality time series transform first difference logarithm operation time series element represent logarithm financial return hold unit cur rency stock one period log log log time series split two set training set test set time series jpyusd demusd bpusd time series sample size training set contains roughly observation mod test set contains roughly observation show information time series size subset use time dependency process model lag realization dependent realization last trading day network neural network application many parameter choose number unit number hidden layer type neuron learn rate supervise unsupervised training initial weight rbf mlp network built one hidden layer neuron form architecture number unit hidden layer increase step unit architecture network train training set mse less reach another parameter default rbf mlp training quantlets xplore neural network library performance measure forecast make test set lag forecast compare true realization moreover define lag performance measure use normalize mean square error nmse xn nmse variance training set sample unconditional volatility mean absolute error mae mae function sign sign otherwise consider financial return useful check result network consider trading strategy purpose sign predict return sign real return compare ideal case fraction prediction sign true realization calculate function sign described result comment result show different architecture hidden unit step jpyusd rbf network perform well mlp architecture concern nmse mae best network rbf hidden unit demusd number rbf mlp network well perfor mance concern nmse mae best network rbf hidden unit second best mlp hidden unit bpusd number rbf well performance concern nmse mae best network rbf hidden unit see result rbf network considerably well mlp network extract information necessary perform good generalization training set consequence un supervise learn period take place rbf training algorithm cluster center deviance learnt mlp training period therefore training rbf network may faster efficient another side bad generalization mlp cause overfitting training data case mlp may learn information specific training set use general ization see anders besides one consider possibility mlps one hidden layer may generalize well maybe well rbfs number hidden unit use seem straight relation forecast performance network hidden unit perform well network many hidden unit way around verify mlps rbfs amount data training set original series split form may also influence result use example first half data training may exclude period special economic instability specially useful generalize purpose may improve performance test set initial weight learn rate default quantlets xplore library neural network another initial weight learn ing rate might effect generalization characteristic respective network rbfs perform well specific data set specific configuration experiment consider number parameter available adjust number variable simultaneously influence sult possible conclude general sense network type architecture forecast well network lag hidden neuron nmse mae sign rbf mlp performance network jpyusd network lag hidden neuron nmse mae sign rbf mlp performance network demusd network lag hidden neuron nmse mae sign rbf mlp performance network bpusd summary conclusion work present overview neural network basic building block neuron besides cover general approach network statistical learn process follow technical exposition descend gradient backpropagation method application described concentrate time series prediction estimation conditional volatility historical time series estimation surface underlie asset volatility imply option price small experiment compare forecast performance rbf mlp network different exchange rate series see summary neural network provide quantitative fi nance strong support problem related non parametric regression also remarkable heuristic consideration involve set neural network sometimes parameter architecture chosen trial error deeper comprehension mechanism technique use development neural network necessary decisive successful implementation thus realization work di rection example comparison performance network different architecture initial parameter evaluation overfitting opti mization stop time training period would useful probably result effective use neural network concern network application new study may direct toward practical financial problem estimation conditional value risk development credit score default prediction tool finally beyond neural network still framework statistical learn support vector machine remain interest challenge field anders statistische neuronale netze verlag vahlen mu nchen bishop neural network pattern recognition oxford press oxford duda hart stork pattern classification wiley new york franke ardle hafner einfu hrung die statistik der finanzm arkte springer verlag heidelberg haykin neural network prentice hall upper saddle river ardle kleinow stahl apply quantitative finance springer verlag heildelberg vapnik nature statistical learn theory springer verlag new york vidyasagar theory learn generalization springer verlag london descgrad xpl proc sfmdescgrad bias epoch ebook sfm see also sfmerrsurf macro sfmdescgrad description plot minimization use sigmoid activation exp follow method descend gradient usage sfmdescgrad bias epoch keywords neural network author giacomini re errsurf bias creates plot grid initial weight initializes epoch aa epoch sumerro sumgrad row calculates weight inp sum bias error function activ exp inp activation function deriv activ activ derivative activation function erro activ sqerr erro square error sumerro sumerro sqerr grad erro deriv gradient sumgrad sumgrad grad sum gradient endo sumerro sumerro give weight bb sumerro sumgrad corrects weight aa aa bb dd aa row aa cc setmask aa row aa line red thin setmaskp dd re setmask re point black size tiny plot path plot re dd cc plot calculate weight plot without path setmaskp bb plot calculate weight plot re bb next training period endo endp proc aa errsurf bias activ matrix row row bias activ exp endif activ ab replace nan endif endo aa endp example input target grid bias parameter sigmoid function period training library plot boolean boolean grid bias epoch sfmdescgrad bias epoch setgopt plot disp title error surface learn weight border tsnn xpl proc tsnn headline please select neural network item mlp rbf network selectitem headline item valuenames please select data use training please select lag default value readvalue valuenames default tr value lag value log difference diff log tr data training set floor tr row xtrain hh tr test set xtes hh row period test set xtes hh hh network valuenames number neuron hidden layer number training period default parmlp readvalue valuenames default par parmlp prepares input matrix output target standtrain xtrain min xtrain max xtrain min xtrain standtes xtes min xtes max xtes min xtes train lag standtrain lag te lag standtes lag train mlp network net nnrnet train yt train tt matrix row train yt parmlp par predicts training set nntra nnrpredict train yt net predicts test set nntes nnrpredict te yt net rescale data nntra min xtrain nntra max xtrain min xtrain nntes min xtes nntes max xtes min xtes train tt min xtrain train tt max xtrain min xtrain te tt min xtes te tt max xtes min xtes vardata var train tt evaluate prediction mlp mae sum ab nntes te tt row te tt mse sum nntes te tt row te tt nmse mse vardata sign sum sign sign te tt nntes row te tt mse mae nmse sign summarize te tt endif network valuenames number neuron hidden layer period unsupervised learn period supervise learn minimum mse default parrbf readvalue valuenames default prepares input matrix output target train lag xtrain lag te lag xtes lag vardata var train tt cluster parrbf learn epoch parrbf parrbf mmse parrbf activ train mlp network net rbftrain train yt train tt cluster learn epoch mmse activ predicts training set nntra rbfpredict train yt net min train tt max train tt predicts test set nntes rbfpredict te yt net min te tt max te tt prediction evaluate prediction rbf testtest rbftest te yt te tt net mae sum testtest aed row te tt mse testtest mse nmse mse vardata sign sum sign sign te tt nntes row te tt mse mae nmse sign summarize te tt endif training plot real data blue versus result network red row train tt tr setmask nntra line red thin ytr setmask train tt line blue thin test plot real data blue versus result network red row te tt tttt setmask nntes line red thin yyyy setmask te tt line blue thin evaluation plot fmt text mse nmse mae sign str string fmt mse nmse mae sign outtext text str disp createdisplay show graphic show disp tr ytr show disp tttt yyyy show disp outtext setgopt disp title neural network forecast xlabel test set border endp proc tt yt lag xt lag rearranges data vector give lag form input yt matrix output tt vector row xt tt xt lag yt xt lag yt reshape yt yt xt xt yt endo yt xt lag endp example axeson library stats library nn library plot xfgthb read xfgthbbasket dat row xfgthb jpyusd xfgthb demusd xfgthb sfm read sfm dat bpusd sfm gold sfm commerzbank sfm tsnn jpyusd condvolrbf xpl proc condvolrbf lag cluster learn epoch mmse activ take log diff diff log tt row prepares input matrix output target train lag lag row train tt train rbf neural network rbfnet rbftrain train yt train tt cluster learn epoch mmse activ rbftra rbfpredict train yt rbfnet min train tt max train tt square sample residual eps train tt rbftra rbfvol rbftrain train yt eps cluster learn epoch mmse activ prediction vol rbfpredict train yt rbfvol min eps max eps plot result disp createdisplay row eps tt row vol setmask vol line green thin series setmask tt line black thin show disp series show disp vol setgopt disp title log return xlabel time border setgopt disp title conditional volatility xlabel time border endp proc tt yt lag xt lag rearranges data vector give lag form input yt matrix output tt vector row xt tt xt lag yt xt lag yt reshape yt yt xt xt yt endo yt xt lag endp example axeson library nn library plot xfgthb read xfgthbbasket dat row xfgthb jpyusd xfgthb demusd xfgthb sfm read sfm dat bpusd sfm gold sfm commerzbank sfm cluster learn epoch mmse activ lag set lag condvolrbf bpusd lag cluster learn epoch mmse activ volsurfrbf xpl proc volsurfrbf metric cluster learn epoch mmse activ ivmethod check error message error sum sum volsurfrbf watch data entry negative error col volsurfrbf data matrix must contain column error sum volsurfrbf type need either zero error metric metric volsurfrbf metric need either numerical method calculation imply volatility exist ivmethod implvola else ivmethod bisect implvola bisect else error volsurfrbf choice method unclear endif endif sum follow line violate arbitrage condition wrong data set first colums row number orginal data set paf row error volsurfrbf watch data violate arbitrage condition see output window endif metric moneyness strike matrix row metric exp else endif prepares training set network rbfx rbfy training rbfnet rbftrain rbfx rbfy cluster learn epoch mmse activ prediction grid value origin min rbfx step max rbfx min rbfx newx grid origin step step result rbfpredict newx rbfnet min rbfy max rbfy surface smile graphical object plot rbfsurf newx result surface smile smile smileplotdot setmask smile fillcircle small red rbfplotdot setmask rbfsurf fillcircle tiny black plot rbfplotdot smileplotdot endp example library plot library nn library finance read volsurfdata dat read data cluster learn epoch mmse activ volsurfrbf cluster learn epoch mmse activ setgopt plot disp title volatility surface rbf network volsurfmlp xpl proc volsurfmlp metric neuron ivmethod check error message error sum sum volsurfrbf watch data entry negative error col volsurfrbf data matrix must contain column error sum volsurfrbf type need either zero error metric metric volsurfrbf metric need either numerical method calculation imply volatility exist ivmethod implvola else ivmethod bisect implvola bisect else error volsurfmlp choice method unclear endif endif sum follow line violate arbitrage condition wrong data set first colums row number orginal data set paf row error volsurfmlp watch data violate arbitrage condition see output window endif metric moneyness strike matrix row metric exp else endif prepares training set network xx yy uniformize data training mlpx xx min xx max xx min xx mlpy yy min yy max yy min yy training mlpnet nnrnet mlpx mlpy matrix row mlpx neuron prediction grid value origin min mlpx step max mlpx min mlpx newx grid origin step step result nnrpredict newx mlpnet scale prediction back create grid plot result result max yy min yy min yy origin min xx step max xx min xx newxx grid origin step step surface smile graphical object plot mlpsurf newxx result surface smile smile smileplotdot setmask smile fillcircle small red mlpplotdot setmask mlpsurf fillcircle tiny black plot mlpplotdot smileplotdot endp example library plot library nn library finance read volsurfdata dat read data neuron volsurfmlp neuron setgopt plot disp title volatility surface mlp network