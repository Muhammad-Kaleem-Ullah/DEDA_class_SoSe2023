hierarchical multiclass topic model prior knowledge master thesis submit first supervisor prof dr wolfgang rdle second supervisor prof dr cathy yi hsuan chen ladislaus von bortkiewicz chair statistic center apply statistic economics universit zu ken schr der partial fulfillment requirement degree master science statistic january th anewmulti inthisthesis cascadelda extends baseline generative model incorporate two type prior information firstly knowledge label training dataset use direct generative model secondly feature closely related label segregate classification problem ensemble small problem sample result achieve time speed baseline model thesis cascadelda perform datasets tag newly publish article keywords bayesian gibbs sample latent dirichlet allocation machine learn natural language processing topic model variational inference content content introduction latent dirichlet allocation conjugate prior collapse gibbs sample variational inference background variational inference latent dirichlet allocation full conditionals lda variational factor lda incorporate available information extension label lda extension hierarchically supervise lda generative model gibbs sample extension cascadelda data preprocessing evaluation method experiment setup challenge classification metric classification quality experiment setup result lda cascadelda number iteration speed assessment hslda conclusion application future research variational factor exponential family list list graphical model lda graphical model label lda label structure jel code tree graphical model hierarchically supervise lda graphical model cascadelda list definition variable ten likely word five jel code summary statistic corpus evaluation metric introduction automate text classification active field research decade use numerous application despite age field gain prominence past decade due increase need organise vast amount digital textual information development machine learn technique computational power number equally valid interpretation automate text classification prevail se bastiani interpretation range solely identify category body document tag document accord pre specify category thesis focus latter interpretation extensive variety method exist tag document use dataset label text model train predict category category text belongs training may involve neural network nam support vector machine joachim simpler method like naive bayes mccallum nigam name research latent dirichlet allocation lda blei use baseline method lda generative probabilistic model collection discrete data text corpus blei unsupervised machine learn algorithm common application lda identify latent semantic topic contain document however many author propose extension serve purpose document classification blei mcauliffe ramage rubin target topic identification jagarlamudi wang incorporate additional feature baseline lda framework representative corpus academic economic paper abstract utilised paper document dataset assign one label correspond academic discipline paper american economic association aea introduce tag system academic article standard economic literature jel journal economic literature code presence nature jel code constitute main source prior information form basis analysis aim research develop lda base document classification model assist author accurately tag academic publication section general framework latent dirichlet allocation thoroughly dealt include elaborate background theory conjugate prior collapse gibbs sample exponential family variational inference section delve dataset hand prior information integrate lda section introduce discus two exist lda flavour call label lda lda hierarchically supervisedlda hslda follow introduction critical assessment quality characteristic datasets use section use point challenge classification problem introduce evaluation metric discus variable setting use final model outcome model different datasets present section section also provide detailed discussion emphasise fundamental difference three lda extension finally section give overview finding use case suggestion future research latent dirichlet allocation order stay within scope paper section discus latent dirichlet allocation context text model application discrete data structure image audio classification therefore ignore section provide outcome ready use expression also deal mechanic property underlie distribution detail insight require development analysis lda extension additionally much attention give collapse gibbs sample variational inference lda view document corpus bag word sentence structure ignore completely every document assume mixture topic topic assume categorical distribution word corpus lda fully generative model assumes document word result mixture topic word drawn observable fundamental model document therefore word aim lda identify distribution represent latent semantic topic corpus represent document latent mixture topic two distribution refer topic word distribution document topic distribution respectively topic word distribution topic follow categorical distribution document document topic distribution follow categorical distribution variable value meaning number topic number document number unique word vocabulary iterators indicate topic document word key number word document total number word document pd uninformative prior weight prior weight topic document rk vector prior weight topic document rk matrix prior uninformative prior weight prior weight term topic dictionary key identifier word document topic assignment word document prob probability word occurs topic rv vector multinomial probability pv rk matrix probability prob probability document belongs topic rk vector multinomial probability pk rd matrix probability nk number word document carry label nk pn number word assign topic definition nv numberoftimeswordv isassignedtotopick howoften equal one definition variable graphical model lda node follow dirichlet dirichlet categorical categorical overview notational definition refer mention lda generative probabilistic model mean observe word assume result underlie latent distribution exactly distribution interact see generative model topic draw distribution word dir document draw topic mixture dir word document draw word topic assignment cat draw word cat note lda setup thus far assumes scalar value hyperpriors set hyperpriors uninformative alternatively may document topic specific respectively make informative extend informative hy perpriors however postpone section joint probability latent variable observe variable lda model yk yd yn interest lda however indirectly involves joint probability main goal find posterior latent distribution denominator require integration full space even analytical solution exist execution would prohibitively expensive therefore exact inference two main type approximate inference use context lda collapse gibbs sample section variational inference section key success lda choice conditional distribution neighbour node graphical model always conjugate pair section word parent node serve prior child node result convenient posterior dis tributions significantly eas derivation conditional posterior distribution insection approximate inference introduce section start introduce concept variational inference general term include necessary lemma section apply variational inference lda set conjugate prior generative model serf prior distribution section demonstrate use dirichlet distribute prior categorical distribution lead dirichlet distribute posterior dirichlet distribution conjugate prior categorical distribution order formulate explicitly distribution combine derive distribution posterior show conjugacy pk dirichlet prior qk extend distribution yield yd yn yk categorical likelihood use dirichlet prior parameter categorical likelihood result dirichlet posterior yd yn yk pk yk qk yd yn yk pd pnd xd xn dir dir dirichlet posterior posterior stem distributional family prior conjugacy show final expression classically show posterior distribution determine prior well data analogously posterior prior likelihood pair yk yk pv yv qv yd yn yk yv result follow dirichlet posterior xd xn xk dir dir nv collapse gibbs sample one way deal unknown complicate joint density like markov chain monte carlo henceforth mcmc specifically collapse gibbs sample geman geman mcmc markov chain construct equilibrium distribution property target distribution repetitively resampling asymptotically exact draw target distribution robert casella multiple rule developed repetitive resampling perform one rule call gibbs sample geman geman next state markov chain attain sample variable conditional variable data word instance every variable sequentially drawn respective full conditional distribution gibbs sample particularly useful target distribution overly complicate building block full conditionals know simpler method first introduce context lda griffith steyvers played big part development lda extension include extension present thesis value dirichlet distribute random variable lemma expect value dirichlet let dir pj outline griffith steyvers topic word assignment sample hence full conditional interest subscript indicates value except th value consider give data word topic assignment know notational convenience subscript refer document well explicitly conditioning hyperpriors drop difference notation emphasise introduce subscript temporarily replace subscript end derivation usual notation return result transform reflect document membership term rely available data term part conditioning set remove course derivation allows focus relevant dimension distribution affect characteristic distribution multiplicative constant cgs term cgs term cgs term full conditional split two distribution first may interpret probability word drawn give word assign topic second probability broadly interpretable relative frequency topic entire corpus cgs term investigation first term lead see wi second part integrand reformulate conjugate likelihood prior structure result posterior derive section dir nv expression therefore equal expect value dirichlet distribute random variable use lemma first term cgs term equal nwi nwj nwi cgs term cgs term analogous reason apply second term see second part integrand reformulate conjugate likelihood prior pair posterior know section dir nk hence expectation dirichlet distribute random variable final expression second term cgs term nk cgs term insert cgs term cgs term cgs term yield full condi tional first formulate griffith steyvers nwi nk intuitively may interpret product two empirical probability probability word occurs topic multiply probability hat topic occurs document expression play major role dynamic lda extension paper point rule sample repetitively drawn monte carlo procedure establish every draw perform full conditional full conditional every dependent rule must constantly adapt current state markov chain assign value markov chain initialise iterative sample procedure accord ever change full conditional distribution start sample posterior distribution generate every iteration markov chain latent distribution estimate every one iteration nv nk burn period iteration every th sample markov state save take average save markov state result predictive distribution new word topic condition griffith steyvers far collapse aspect gibbs sample procedure address ex plicitly thedocument topicdistribution andtopic worddistributions serveas dirichlet prior categorical likelihood see equation prior integrate collapse collapse result unconditional distri bution independent prior sample take case conjugate pair result distribution particularly simple form comparison initial conditional distribution thus make gibbs sample process less complex also de pendencies categorical variable rely dirichlet prior create easily confirm observe distribution partially determine via term variational inference background posterior distribution latent distribution compute intractable variational inference use deal model infeasible evaluate posterior distribution infeasibility may stem high dimensionality latent space complex form disable analytical tractability bishop use tool variational inference posterior distribution approximate handleable distribution turn optimise general idea minimise kullback leibler divergence kullback leibler true posterior approximate distribution whilst restrict family distribution select approximate distribution family must rich enough resemble relevant feature true distribution simultaneously restrict family member tractable feasible optimisation assume model observe variable latent variable posterior distribution intractable due high dimensionality latent space resides aim variational inference approximate tractable density kullback leibler divergence minimise arg min kl kl log logq logp log logq logp family distribution select still involves however nothing additive constant optimisation function may ignore since kl divergence non negative kullback leibler easy verify logp low bound logp logq logp also refer evidence bayesian statistic follow call evidence low bound elbo elbo logp logq definition kl divergence elbo see minimise kl divergence equivalent maximise elbo focus henceforth restriction place family distribution select optimal choice would set equal kl divergence would obviously zero ensure simplicity structure variational distribution limited distribution factorise marginal distribution word joint variational distribution equal product marginal variational distribution blei jordan general joint distribution one factorise like logp log logp logp incorporate mean field variational family assumption elbo yield elbo logp logp logq qm recall aim maximise elbo respect variational distribution hence logp may regard constant since elbo maximise eventually derivative set zero convenient formulation make dependency explicit use law iterate expectation take partial derivative solve yield elbo logp dy logq dy elbo logp logq exp logp expression solve derive do section itisinteresting pay close attention similarity gibbs sample coordinate ascent recall gibbs sample us full conditional distribution sample coordinate ascent variational inference method discuss us exponentiated expect log value full conditional see set variational factor blei inadditionto member family different natural parameter see proof difference multiplicative constant true full conditionals variational factor lie natural parameter reason set also refer variational parameter natural parameter complete conditional full derivation refer order arrive variational factor lda model useful aware follow lemma lemma markovblankets model see bayesian network posse number property help identify conditional independence node network awareness property allows one nimbly ass whether conditioning set variable reduce without affect conditional distribution markov blanket node denote mb bayesian network consists parent child parent child call co parent node outside mb conditionally independent condition mb implies node outside markov blanket redundant conditioning set long mb contain conditioning set bishop lemma inabayesian network separate block node set node direct towards neither descendant part set geiger lemma expect value log dirichlet let dir expect value logarithm equal logx log digamma function dy lemma natural parameter dirichlet distribution let rm dir natural parameter lemma natural parameter categorical distribution let cat natural parameter logp logp logp variational inference latent dirichlet allocation order use obtain optimal variational parameter full conditionals latent distribution require three group latent distribution exist lda different full conditional distribution section start obtain true full conditionals section afterwards proceed identification optimal variational distribution section final result end coordinate ascent variational inference cavi algorithm lda full conditionals lda get full conditionals build conjugacy result section lemma present end section full conditional document topic distribution markov blanket mb consists parent child co parent know qn cat dir lda model specification conjugacy result section full conditional topic proportion dir xd xd nk full conditional topic word distribution markov blanket mb con sists child co parent parent last term final equation follow lemma collide node part conditioning set therefore conditionally independent lda model specification know qd qn cat dir consider latter serf conjugate prior former distribution know see section result must follow dir distribution xd xn xd xn full conditional word assignment markov blanket mb consists parent child co parent hence conditional distribution know directly use chain rule equation write like cancel part mb cancel separate node lemma variational factor lda ational factor parameter distribution still unknown let unknown parameter document topic topic word word assignment distribution respectively topicdistribution dir thenatural parameter full conditional nk variational distribution topic word distribution dir natural parameter full conditional pd pn variational distribution word assignment distribution cat lemma natural parameter full conditional equal logarithm parameter therefore log log natural parameter full conditionals crucial section since link explicitly optimal natural parameter variational factor see expression variational distribution document topic distribution find appropri ate parameter variational distribution rely thereason andz use full conditional expectation take variational distribution except since term inside expectation operator natural parameter full conditional contains expect value equivalent qzd xd qzd xd final line follow fact follow categorical distribution unknown parameter document topic variational distribution dir variational distribution topic word distribution analogous deriva tion variational distribution document topic distribution know qzd xd xn qzd xd xn therefore dir variational distribution word assignment expression natural parameter variational distribution qzd since categorical distribution natural parameter logarithm parameter see lemma actual parameter denote exp exp qzd exp log log exp log log exp last line follow lemma incorporate available information input require perform baseline lda merely corpus document number topic value hyperpriors corpus use paper additional source information exploit namely jel code document document label structure label also know example label member label family likely common label incorporate prior knowledge unsupervised baseline lda transform semi supervise method section present three extension baseline lda model label lda lda hierarchically supervise lda hslda cascadelda extension utilises available information way quality result depends ultimate aim user time specific feature dataset extension label lda unseen academic abstract full academic paper implies interest identify latent topic rather explicit topic one one correspondence jel code multiple way manipulate topic identify lda one could example provide set seed word use bias topic word distribution towards certain term jagarlamudi alternatively create must link link word topic andrzejewski structure topic unlabeled corpus researcher domain knowledge topic wish identify external knowledge source wikipedia article regard graphical model label lda desire topic also use form prior knowledge wood case research manipulate topic exactly correspond label set explicit topic label data investigate ramage rubin crucial piece information topic label cover every document know priori importantly know topic part document put lda specific term know element positive document topic mixture element equal zero hence space topic document distribution resides training every document shrunk tremendously since limited jel code attach document cause shift way conditional distribution model interact three step process firstly parameter distribution recall cat mostly equal zero positive value position correspond label document lead second step process assign positively load label hence word document associate label result document affect topic word distribution correspond topic label document dubbed label lda lda ramage claim lda outperforms support vector machine identify label specific docu document despite contextual difference lda practice equivalent rubin flat lda rubin illustrates label information incorporate extend baseline lda model full generative model lda nearly identical lda except document topic distribution dir dimensional vector indicates whether label part document ten likely word five jel code root econom cycl agent skill hous mortal cours model inform capit price health signific busi princip human boom increase student shock optim educ transact percent research recess mechan labor home age author aggreg privat school area birth regress volatil effici worker buyer suggest expeditur rate incent occup percent rate call fluctuat alloc wage seller life yield product select college rent effect root generic label assign document improve classification result real label macroeconomics monetary economics price business fluctuation cycle business fluctuation cycle microeconomics information knowledge uncertainty asymmetric private information mechanism design labour demographic economics demand supply labour human capital skill occupational choice labour productivity urban rural regional real estate transportation economics real estate market spatial production analysis firm location housing supply market health education welfare health health behaviour label set dynamic force topic loading part document label set zero keep rest positive result highly accurate topic word distribution label see important characteristic root label assign every single document academic actual jel label shrink document topic document distribution set stage noteworthy side effect due few possible value iterative process gibbs sample cavi training time converge much faster stable way extension hierarchically supervise lda label document implicit structure illustrate label consist one letter two digit like letter stand general category level microeconomics international finance first digit represent first refinement within general category level second digit provide final level granularity level leaf label therefore expect jel code topic word distribution closely related jel code generally neighbour leaf label tend highly similar also human reader hard distinguish idea related topic within lda model investigate number paper blei lafferty generally allow pre specify additionally thesemi supervise hierarchical topic model sshlda mao introduces promising model learns new topic automatically since aim thesis classify accord pre specify label identify new topic sshlda would unnecessarily complicate computation derivation thesis without guarantee regard correctness hierarchy dependency lda introduce rubin however dependency lda would require create positive loading topic related document label turn decrease discriminative power model additionally specifically design perform well corpus large number label whose frequency follow power law distribution since dependency lda outperforms method datasets label follow power law distribution method also pursue hslda byperotte perform sample prediction primary aim base line lda model augment entirely new mechanism exploit hierarchy structure label via show graphical model hslda top part model baseline lda model actual latent topic search information document label integrate bottom part graphical model microeconomics market structure pricing monopoly microeconomics market structure pricing perfect competition label structure jel code tree generative model dive technical part intuitive explanation model mechanic provide intuitive take model start node root node serf run variable probit model follow normal distribution withmeanz root root root wherez document topic mixture test give label assign document correspond run variable exceeds pre specify threshold case label dummy label set one run variable label tree dependent label dummy test truncate threshold parent value threshold therefore impossible label label tree turn positive ancestor negative label know training time run variable always force either threshold regression coefficient train reflect interaction observe label latent topic identify top part graphical model note top part identifies latent topic actual aim model crucial aspect run variable label dummy latent topic merely tool help achieve goal graphical model hierarchically supervise lda except specification generative model corresponds original hslda topic draw distribution word dir label draw label application coefficient contains identical value diag document draw topic proportion dir draw word assignment cat draw word cat set root node root label start child root hdp dt pa draw pa dt pa apply label document accord else wherez dt pn may regard intermediate estimate hierarchical couple regression coefficient creates posteriori dependence cause label predictor deeper hierarchy focus distinguish feature label level gibbs sample collapse gibbs sample use approximate inference hslda whereas baseline lda model collapse latent variable except hslda involves two additional latent conditional distribution namely conditional posterior distribution word topic assignment use paper differs important way original hslda specification instead conditioning run variable condition reason discuss bit lead conditioning set term could reduce final line node outside relevant markov blanket lemma separate lemma consider variable take value know exp da exp da derivation differs two way original hslda formulation firstly original hslda take product present label account thus effectively ignore second term justified assumption absent label may actually part document true label set oppose observe label set absent label restrict threshold implies second set integral evaluate since integral proportional probability density function ignore corpus thesis however implicitly assume label assign document indicate paper true present positive label therefore also absent nega tive label secondly paper focus conditional probability large small threshold instead reason former quantifies distance threshold well latter thus also capture magnitude presence accurately second term equivalent result section eq nwi nk conditional posterior distribution regression coefficient corresponds least square regression result prior conditional available data statistic literature refer ridge regression ta tz conditional posterior distribution run variable closely related model specification difference probability pa graphical model cascadelda force zero truncate hierarchy violate result exp dy pa exp dy pa extension cascadelda cascadelda design take advantage hierarchy structure label different approach hslda mechanism foreseeable issue previously discuss extension seem unlikely topic word distribution dis criminative sibling label focus entire corpus global scope topic word distribution leaf label bound pollute dominant word dominate sibling parent topic word distribution one hand represent leaf label topic word distribution hand help discriminate sibling label cascadelda aim find discriminate feature sibling node zoom single non leaf label time local scope serf magnify glass difference sibling node proceed useful clear terminology use explain ca cadelda firstly term sibling label neighbour label may use interchangeably secondly leaf label necessarily level label name suggests refers detailed label label tree model ignores level label level automatically become leaf label thirdly label descendant non leaf label serve parent label similarly label except root label serve child label fourthly non leaf label serf basis local scope local scope may also refer neighbourhood family scope non leaf label serf start point local scope training time non leaf label call parent label scope document parent label label set retain local scope whereas document fall outside scope retain document label set obfuscate contain theparent schildrenlabels remain document label refer subcorpus create subcorpus lda perform final crucial manipulation subcorpus generic label add document label set addition child label local scope analog root label childlabels allnon generic label training fact feature share document subcorpus assign generic label include generic parent label feature part document likely attribute child label one set child label apart sibling lda model local scope converge result topic word distribution child label retain cascadelda model typically cascadelda cascade label tree choose one former child label next parent label process continued non leaf label serve parent label cascade branch cascadelda topic word distribution distinguish sibling label classic lda approach leaf label consider label topic word distribution identify non leaf label cascadelda however non leaf label also topic word distribution may seem like small detail becausethetopic worddistributions valid usable respective scope discriminate feature local scope toseewhy considerahuman family child recognise base hair colour even though may creates challenge test time scope unseen document test process test start root attempt assign document one level label way lda apply classification test procedure cascade promising level label document content fit parent label local scope discriminate child label procedure continued promising branch pursue discussion label consider promising postpone section beware certain dynamic handle caution consider situation word test document convincingly assign label result model zoom label make parent follow local scope may occur test document contain feature allow recognition child even though document really member family result word test document assign generic label path consider branch prediction strand non leaf label datasets use favourable characteristic leaf label assign document data preprocessing label discuss apply first corpus consists abstract contain average standard deviation unique token order obtain wide range topic abstract gather different journal include american economic journal speciali sation journal apply economics economic policy macroeconomics microeconomics creator jel code journal economic literature also source nu thus relevant academic economic literature past year full approximatelyhalfof article correspond document first corpus half stem collaborative research center economic risk part interdisciplinary center apply statistic economics allfull document extract text document image non text data remove document contain average unique word standard python source code http github com kenhbs pdf text abstract abstract full text full text level level level level nr document nr word per doc leaf label nr label nr label per doc distinct label set incl non leaf label nr label nr label per doc summary statistic corpus deviation training classification model next step test evaluate effec tiveness standard classification problem model apply unseen document also know test set even though label available test set make theoretically suitable use training document test set isolated model training fail strictly split training data test data likely lead overfitting external validity classification would immediately compromise document label human author subjective interpretation paper semantic topic interpretation may may align well way author label paper lead inconsistent label document also know inter index inconsistency hamill zamora useful keep potential flaw data quality mind interpret result model label use less document remove datasets due lack data create reliable prediction theory jel label exist level level leaf label leaf label see portion label retain cleaning label please refer text transform bag word stem porter stemmer use standard setting python module gensim eh ek sojka evaluation method experiment setup section discus challenge face task classify document particular corpus hand furthermore metric use evaluate predictive quality model introduce discuss challenge classification point challenge face predict document jel label elaborate overview difference basic classification method lda additionally itisimportant stress influence classification method parameter threshold may quality classification first difference common use case classification model thesis thatis falseortrue forasubject oursetting one multi class classification every jel leaf label consider class instead predict zero one model train choose value number class roughly two technique exist approach multi class problem one one one multiple classifier need either case class need separate either way one strategy creates classifier every class separate subject order predict class unseen instance every separate function apply instance thus assign value discriminate function produce high score chosen supposedly separate best rest number class closely related one distant class one approach tend wrongfully predict distant class two related class hard separate therefore tend rarely distinguish rest one one approach suffer set creates single classifier every combination two class result classifier class win one one comparison final prediction one one secondly single instance may assign one class know multi label classification problem method deal case either transform problem single label problem transform single label algorithm match multi label requirement read tsoumakas katakis profound relevance label label train predict independent thus ignore dependence structure label select threshold include labelcase thisthreshold either define term label probability regardless label label probability ranked probable label predict surely hybrid two also possible put perspective lda extension paper note hslda model design classification produce proper probability whereas lda cascadelda produce document topic mixture test document let elaborate bit fundamental aim lda learn topic word distribution document topic mixture establish topic word distribution could consider aim training time whereas document topic distribution important result test time theoutcomesofl foragivendocument pk mind also consider multiple label eight maximum may apply single document becomes clear even though label may account extreme posterior inference unseen document may result percent share give label remainder allocate another label case single label prediction seem appropriate hence label share document topic mixture may interpret probability label present document therefore would inappropriate drive decision include label prediction solely share document topic mixture suitable approach third fourth significant label approach use paper exact feature hybrid prediction rule vary among datasets model mention significant weakness one approach see paragraph multi classification favour class farther away class number approach cascadelda come rather focus document class take subset aim discriminate neighbour class previously refer sibling still constitutes one approach albeit within give local neighbourhood distant neighbour unlikely cascadelda may view ensemble one approach create collection multi class subproblems also refer hierarchical classification silva palacios metric classification quality noneofthem capture notion classification quality perfectly therefore metric tends focus accord one metric inferior term another metric multi label nature add complexity evaluation metric numerous approach propose tsoumakas katakis thorough comparison lda extension therefore require array metric corpus label depth model present actual metric appropriate put choice metric context clear feature capture illustrate let take step back focus lda mechanism may cause inflate deflate quality metric recall gibbs update section consists two part one concern probability word part topic via topic word distribution whereas term related document current document topic mixture cause update rule favour currently present topic update one consequence small document topic mixture tend vanish course iteration benefit already present topic root two potential bias classification metric first number label large topic word distribution similar risk get wrong foot long topic word distribution nearly identical reason gibbs sampler move away current label favour almost identical neighbour result lda may often predict neighbour document label set document classification misclassifying label neighbour regard less misclassification predict label side spectrum due generative nature lda loss asymmetry misclassification implementable straightforward fashion conventional machine learn method asymmetric loss function implement training already thus affect model actual classification ability abe zhou liu lda extension paper however asymmetric cost misclassification could use get accurate metric classification quality rather actually affect prediction second issue originates generative mechanic lda concerned majority topic zero document topic mixture discuss numerous chain event cause real label absent prediction small share topic shrunk zero uncommon percent label document topic mixture share zero rank prediction label descend order presence trail percent label randomly sort rank base metric rely distance low ranked true label high ranked false label therefore useful research test document label ranked least likely base rank follow rank base evaluation metric calculate auc area curve auc receiver operating characteristic roc roc everypointon roc curve fpr tpr false positive rate fpr true positive rate tpr threshold move zero one fpr fp fp tn tpr tp fp false positive tn true negative tp true positive tp fn fn false negative give threshold label give value true false fp tn tp fn value cell contingency threshold auc roc therefore capture accuracy rank may interpret probability randomly select subject label predict hanleyandmcneil randomly auc whereas perfect prediction result hence roc value auc always compare obtain auc roc roc every test document average get macro auc single metric roc model predictive capability xtest auc auc roc macro roc dtest one hit percentage document high ranked label correct relevant lda cascadelda case hslda use classify document whereas lda cascadelda document topic mixture use basis classification two hit percentage document least one two high ranked label correct score measure harmonic mean tpr fpr since fpr may consider probability false alarm either term consider favourable regard classification quality score also measure regard threshold paper threshold produce high score give test document chosen threshold refer subscript opt tpr fpr opt opt doc tpr fpr opt opt score test document average obtain model score xtest macro dtest metric roc implementation lda cascadelda obtain prediction give document document ignore calculate evaluation metric understand do inspect case model produce prediction case lda occurs word topic assignment assign root label root label valuable prediction actively remove prediction inflate predictive quality every single document cascadelda model may get stuck way see end section evaluation metric throw error document topic mixture zero document remove cascadelda tends end solution often lda especially academic corpus full text corpus however rarely happens experiment setup inthissubsection choice regard hyperparameters model setting present discuss model estimate mean collapse gibbs sample cgs even though method available theory realistic competitor cgs cavi point asuncion claimed difference performance different mean model mainly attributable hyper parameter configuration choice cgs mainly driven relative simplicity availability rudimentary code example gibbs sampler programmed run iteration lda model reason discuss section cascadelda sampler produce best result less iteration per subcorpus use importance hyperpriors point many paper asuncion wallach conclusive setting regard magnitude prior exist test multiple magnitude hyperpriors best result achieve equal equal without go much detail notable large say actually produce well result case number label small despite lack highly detailed grid search considerable improvement observe relatively small change hyperpriors speed iteration document save token id frequency tuples instead record market hundred time bag word record assign weight hundred upon resampling instance word document reassign time save significant computational cost necessary step handle full text corpus inaddition file process extract text pdfs base test run however prune dictionary produce best result corpus abstract corpuswithfulltexts percent document prune dictionary way reduces size vocabulary token id frequency tuples size full text corpus reduce nearly million word less million without affect predictive quality recall every single word assign topic every single iteration hence effort save computation time actually result cost reduction percent number topic determine number unique label corpus lda cascadelda hslda however number topic refer number label instead indicates number latent topic use run probit regression label presence upon personal inspection jel code suitable number semantic topic likely distinguishable establish incorporate hierarchical dirichlet process hdp hyperprior make flexible estimate base available data teh quality stability latent topic suppose improve extension wallach unfortunately result hslda barely affected consider trade complexity hierarchical dirichlet process add value cover paper result section present discus classification quality model present metric base different lda extension two datasets multiple level detail label compare model much attention paid different root difference despite promising feature mechanic hslda prove fatal classify academic article accord jel code classification metric neither presentable interest therefore dismiss paper nevertheless detailed discussion reason outcome provide section certainly insightful lda cascadelda present evaluation metric multiple setting general consensus exists however valuesbetween roc poor value exceed regard good regardless interpretation safe say classifier perform well random prediction evaluation metric improve label depth decrease particularly hit hit metric show large increase expect result briefly touch upon section discuss asymmetric cost misclassifications notion classify sibling label penalise much classify far away label thus implicitly incorporate consider label level label tree downside however slightly misclassified document score perfect prediction textcorpus evaluation metric model dataset auc hit hit score roc lda ab lda ab lda ab cascadelda ab cascadelda ab cascadelda ab lda full lda full lda full cascadelda full cascadelda full cascadelda full python code available http github com kenhbs lda thesis label level depth consider analysis parameter setting number iteration gibbs sampler number iteration consecutive save markov chain help discriminate label model apparently handle concentrate information abstract well enough achieve high quality separation label number iteration feature require thorough discussion fact cascadelda performs best iteration gibbs sampler explain phenomenon need answer two question firstly produce good result small number iteration secondly performance increase fact decrease iteration answer first question lie fact cascadelda break large multi class problem multiple small multi class problem number label subcorpus reduce drastically compare entire corpus stable state gibbs sampler expect occur faster local scope expectation jump mind number topic label choose assign value relatively small result sampler subcorpus stable settle steady state faster second question tentatively answer mechanism refer gravitational pull generic label consider document contains four label belong different branch label tree investigate document subcorpus three label obfuscate percent document word relevant local scope percent belong obfuscate label majority percent expect assign generic label recall gibbs update word topic assignment favour topic high document topic load already mechanism also discuss section albeit different set course iteration generic label continuously favour child label suppose share remain percent result word topic assignment gravitate towards generic label gradually move away child label actually belong two fold answer describes two separate mechanism influence state markov chain simultaneously therefore argumentation markov chain settle faster stable state hold absence gravitational pull generic label fact gravitational pull drag markov chain away optimal state force cascadelda terminate small number iteration result predictive quality cascadelda fluctuates heavily run speed assessment understand less iteration require cascadelda next step ass mean term computational speed force increase cascadelda computational cost compare lda present first afterwards cost save mechanic discuss finally advantage cascadelda term speed illustrate timing experiment even though number iteration small single subcorpus approxi mately subcorpora analyse additionally single document occurs multiple local scope even document single label occurs three subcorpora first head root second third local scope head level level parent respectively course single full iteration cascadelda single metric present correspond final run model necessarily best value obtain document analyse least three different local scope few number iteration require tremendous push term speed fur thermore inl ldaeverytopic dimensionalvector probability use draw value cascadelda number possible label much small therefore computation speed effect large local consider level label note however even though calculation dimensional vector probability vectorised numpy van der walt still affect computational speed asimpletimingcompar ison perform setting result high predictive quality abstract dataset use assessment computation time reflect time expire load train test evaluate model abstract result serve mere indication speed interpret formal statement timing comparison vary depend machine use compute also dataset optimal setting likely influence computation time model unequally say cascadelda require minute finish task whereas lda ran minute attain comparable predictive quality conservatively cascadelda faster lda factor final note cascadelda produce result three label depth single run whereas lda require training entirely new model label depth change reason cascadelda train perform every local scope thus also recordstopic leaflabels lda ontheotherhand onlyidentifies topic word distribution leaf label whatever reason classification every label level demand lda train three separate model hslda hslda take account hierarchical structure label one main feature peaked interest model lda particular fails acknowledge important piece prior information result suboptimal separation sibling label obtain quality hslda overcome remains theoretically convincing reason fatal hslda result lie structure jel code recall section run variable assign every document every label follow truncate normal distribution mean variance mean every run variable thus linear combination document latent topic mixture regression coefficient simultaneously force threshold base presence parent label pa document label set upon integrate hslda thesis hope label hierarchy would separate run variable mean surely within topicmixturesz andl separation sibling label would thus occur regression coefficient differs regression coefficient label note latent topic whatever may cover sibling label must nearly identical jel code label since cover almost identical topic result mechanism hslda unable train discriminate regression coefficient point may seem hslda offer advertises however consider catalog product description assign main category subcat egories online catalog amazon com single product may reside multiple subcategories different top level product category let consider typical product like sneaker sneaker appear top level category sport outdoors clothing shoe jewelry within sport outdoors sneaker may also occur athletic clothing well sport collectible forth contrast jel hierarchy sibling label athletic clothing similar sneaker therefore discriminate well take account prior information product within cer tain branch lead hslda produce regression coefficient latent topic separate sibling label unfortunately proven case jel code classification conclusion conclusion lda cascade produce comparable high quality prediction datasets speed cascadelda attains result faster factor roughly highly dependent label structure general feature dataset increase speed come cost less stability hslda could unfortunately include model comparison due misalignment label structure datasets datasets use original paper systematic difference predictive quality found corpus aca demic abstract full text apparently discriminative feature contain full text also present abstract far cascadelda lda concerned application high quality prediction indicate lda cascadelda use provide suggestion jel code labelings unseen paper practical use case either model would embed upload process new academic article analyse new article author quickly provide top label include description author would able select fitting jel code mean simple point click implementation would two benefit firstly relevant label suggest base actual content paper lead less subjective labelings secondly author would go list jel code search suitable code paper case cascadelda suggestion would take less half second whereas lda would require approximately second perform task cascadelda could use general setting well within scope text mining could use automatically archive new arrival digital library however application outside text mining also possible topic model technique use extensively field bioinformatics example predict protein function function cerrietal thesefunctions may see label may use hierarchically predict function sub function unseen protein sequence future research final part thesis dedicate idea may improve model worth consider future research interest concept pursue prune dictionary less crudely idea behind reduce dictionary remove word thus shrink uninformative word share currently word occur nearly barely document remove corpus refine way run exploratory lda identify top word generic label remove corpus equivalent cut frequent word least frequent uninformative word could also identify cut away topic word distribution matrix exploratory lda analyse recall topic represent row instead focus row one could analyse column search column column sum close zero column correspond word low value term identify topic reduce corpus way noise remove corpus succeed analysis focus less computationally expensive idea specialise prune particularly interest cascadelda parent node jel label tree subcorpus contains document belong partially say labeld sibling specialise prune would remove generic term term irrelevant lead much small document topic share rest label would thus overcome risk gravitational pull therefore may stabilise cascadelda performance computer science point view would interest thoroughly analyse computational complexity different model could formally validate increase speed potentially confirm scalability cascadelda abe zadrozny langford iterative method multi class cost sensitive learn proceeding tenth acm sigkdd international conference knowledge discovery data mining kdd page new york ny usa acm andrzejewski zhu craven incorporate domain knowledge topic model via dirichlet forest prior proceeding th annual international conference machine learn icml montreal quebec canada june page asuncion well smyth teh smooth inference fortopicmodels intelligence uai page arlington virginia united state auai press bishop pattern recognition machine learn springer verlag new york inc blei kucukelbir mcauliffe variational inference review statistician journal american statistical association blei lafferty correlate topic model proceeding rd international conference machine learn page mit press blei mcauliffe supervise topic model neural information processing system blei ng andjordan mach learn re cerri barros decarvalho andjin reductionstrategies hierarchical multi label classification protein function prediction bmc bioinformat ic geiger verma pearl identify independence bayesian network network geman andgeman gibbsdistributions andthebayesian restoration image ieee transaction pattern analysis machine intelligence griffith andsteyvers proceeding national academy science suppl hamill andzamora journal american society information science hanley mcneil meaning use area receiver operating characteristic roc curve radiology jagarlamudi daum iii udupa incorporate lexical prior topic model proceeding th conference european chapter asso ciation computational linguistics eacl page stroudsburg pa usa association computational linguistics joachim learn classify text use support vector machine method theory algorithm kluwer springer jordan ghahramani jaakkola saul introduction variational method graphical model machine learn kullback leibler information sufficiency annals mathematical statistic mao ming chua li yan li sshlda semi supervise hierarchical topic model proceeding joint conference empirical method natural language processing computational natural lan guage learn emnlp conll page stroudsburg pa usa association computational linguistics mccallum nigam comparison event model naive bayes text classification ininaaai tion page aaai press nam kim menc gurevych rnkranz large scale multi label text classification revisit neural network joint european conference machine learn knowledge discovery database page springer perotte wood elhadad bartlett hierarchically supervise latent dirichlet allocation shawe taylor zemel bartlett pereira weinberger editor advance neural information processing system page curran associate inc ramage hall nallapati man label lda supervise topic model credit attribution multi label corpus proceeding conference empirical method natural language processing volume volume page association computational linguistics read pfahringer holmes frank classifier chain multi label classification machine learn eh ek sojka software framework topic model large cor pora inproceedings lrec workshop new challenge nlp framework page valletta malta elra http muni cz publication en robert casella monte carlo statistical method springer text statistic springer verlag new york inc secaucus nj usa rubin chamber smyth steyvers statistical topic model multi label document classification mach learn sebastiani machine learn automate text categorization acm comput surv silva palacios ferri ram rez quintana improve perfor procedia computer science international conference computational science icc june zurich switzerland teh jordan beal blei hierarchical dirichlet process journal american statistical association tsoumakas katakis multi label classification overview int data warehouse mining van der walt colbert varoquaux numpy array structure efficient numerical computation compute science engineering wallach mimno mccallum rethink lda prior matter inbengio schuurmans lafferty williams andculotta editor advance neural information processing system page curran associate inc wang chen fei liu emery target topic model focus analysis proceeding nd acm sigkdd international conference knowledge discovery data mining kdd page new york ny usa acm wood tan da wang arnold source lda enhance probabilistic topic model use prior knowledge source arxiv print zhou liu multi class cost sensitive learn proceeding st national conference artificial intelligence volume aaai page aaai press variational factor exponential family let full conditional member exponential family exp base measure natural parameter always depends know parameter conditioning set sufficient statistic log normaliser use general expression exponential family member solution exp logp exp log exp expe log exp exp log normaliser normalise factor ensures exponential family member density integrates one therefore may ignore act scale constant optimisation problem thebasemeasureh andsufficientstatis tic variational factor full conditional fact implies variational factor must member exponential fam ily distribution full conditional difference variational factor full conditional lie value natural parameter variational factor natural parameter equal instead