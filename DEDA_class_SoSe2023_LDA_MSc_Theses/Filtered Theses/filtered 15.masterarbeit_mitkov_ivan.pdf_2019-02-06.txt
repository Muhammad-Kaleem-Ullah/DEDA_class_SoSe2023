linear nonlinear model forecasting realize volatility cryptocurrencies master thesis submit prof dr wolfgang rdle prof dr stefan lessmann universit zu school business economics ladislaus von bortkiewicz chair statistic ivan mitkov partial fulfillment requirement degree master science economics january would like thank junjie hu help three month research bruno spilak share high frequency data also grateful people privatissimum seminar idea constructive criticism last least important gratitude family girlfriend constant support study germany cryptocurrencies know high fluctuate price order minimize risk investor many empirical researcher propose realize volatility calculate intra daily log return approximation true volatility market empirical paper aim make possibly best forecast daily realize volatility next forachievingofthese goal linear nonlinear model consider make use popular linear heterogenous autoregressive model har rv various recurrent neural network srn lstm gru additionally hybrid model simplest feedforward neural network heterogenous autoregressive model propose result demonstrate superiority nonlinear model whereas obvious recurrent neural network low volatility time reduce training data set forecast value use calculate value risk portfolio cryptocurrencies unfortunately unconditional coverage test conditional coverage test reject null hypothesis forecast future daily realize volatility suitable approximation calculate value risk noise jump data point possible reason result therefore start point research application complex model like har rv har rv cj deep recurrent neural network suggest ii content list abbreviation list vi list vii introduction realize volatility linear model heterogeneous autoregressive model extension heterogeneous autoregressive model nonlinear model deep feedforward neural network recurrent neural network simple recurrent neural network long short term memory gate recurrent unit hybrid neural network har estimate training predict model estimate har rv training tune neural network training hybrid fnn har data empirical application accuracy predict model high frequency time low frequency time financial application value risk backtesting iii applicaiton value risk backtesting conclusion iv list abbreviation ai artificial intelligence dl deep learn fnn feedforward neural network gru gate recurrent unit har heterogeneous autoregressive model lstm long short term memory ml machine learn nn neural network rnn recurrent neural network rv realize volatility srn simple recurrent neural network list single layer perception deep neural network two hidden layer simple recurrent neural network simple jordan recurrent neural network architecture lstm architecture gru structure hybrid fnn har model prince coin price evolution price weight portfolio six coin log return portfolio predict realize volatility high volatility time predict realize volatility low volatility time true log return high volatility time var true log return high volatility time var true log return low volatility time var true log return low volatility time var vi list hyperparameters recurrent neural network descriptive statistic price six cryptocurrencies descriptive statistic log return prediction error high volatility time long prediction error high volatility time short test statistic diebold mariano test sample prediction high volatility time prediction error low volatility time long prediction error low volatility time short test statistic diebold mariano test sample prediction low volatility time backtesting high volatility time backtesting low volatility time descriptive statistic price six cryptocurrencies vii introduction digital asset like cryptocurrencies know high fluctuate price ever exactly fact provoke many people invest digital currency huge amount money invest volume trade cryptocurrencies challenged also lot researcher focus empirical work estimate predict true volatility type asset unfortunately find good approximation measure volatility thisstepisimportant sincevolatility model mostly closely link risk uncertainty thus milestone asset pricing portfolio management option pricing risk management various approach estimate true volatility however traditionally popular way calculate variance log return later researcher make use dynamic volatility estimation model garch model stochastic volatility model increase role data availability data intraday return lead development new kind model base high frequency data first proposal direction make merton prove true variance calculate sum square intra daily return data sufficiently high frequency key concept work namely realize volatility propose first time andersen bollerslev found prediction daily log return corresponds realize variance accord barndorff nielsen shephard integrate volatility true unknown parameter could consistently estimate realize variance however unrealistic since price continuously observe practice thus end bias estimator noise data show imperfection realize volatility approximation true volatility attractive empirical researcher demonstrate daily weekly monthly measure characterize long range dependency significant autocorrelation high lag moreover first researcher found negative correlation volatility asset return leverage effect second also show measure include successfully jump previous period forecasting future volatility fact many researcher concentrate predict correctly realize volatility discuss forecast good financial application real practice usually realize volatility predict autoregressive fractionally integrate move average arfima model suggest andersen thomakos wang well heterogenous autoregressive model realize volatility har rv propose corsi latter become broadly use due simple form linear regression however still able reproduce stylize fact cite recent empirical work andersen corsi also suggest incorporation model jump time series result follow extension har rv har rv cj however financial time series usually associate nonlinearity usually ofthedata however neural network mostly suggest deal nonlinearity barun ehl propose hybrid model simple feedforward neural network fnn har rv call fnn har whereby author use explanatory variable har rv input variable artificial neural network ann demonstrate improvement forecast next day realize volatility however barun ehl criticize due high number parameter include suggests overfitting therefore arneri reduce amount hyperparameters fnn har fnn har demonstrate compete hybrid model classic har rv har rv actually pretty similar accuracy prediction main objective paper provide possibly best forecast realize volatility portfolio cryptocurrencies find good financial application forecast achieve goal make use classic linear heterogeneous autoregres har rv moreover also propose nonlinear machine learn technique improve accuracy best know paper try involve recurrent neural network predict realize volatility therefore claim empirical contribution completes specific area research something innovative simple recurrent neural network srn modify architecture long short term memory lstm gate recurrent unit gru apply solve call occasion regression problem selection algorithm relies fact capture nonlinear element data time also memorize significant event back past use information predict next outcome additionally also reproduce propose barun ehl arneri hybrid fnn har model able memorize information thanks input variable equal har rv also model nonlinear activation function feed forward neural network check ability model forecast correctly differentiate scenario high volatility low volatility time word compare good model learn different volatility time furthermore ability model learn reduce amount training data consider well forecasting model compare diebold mariano test second part empirical work find financial application forecast calculate value risk multiple period ahead predict value validate backtesting technique empirical result work prove superiority nonlinear model term forecasting accuracy becomes even obvious reduce training data recurrent neural network outperform model however judging value diebold mariano test result seem significant high volatility time consider financial application forecast null hypothesis valid value risk forecast multiple period ahead reject unconditional conditional coverage test however tendency well perform recurrent neural network confirm high volatility low volatility time remainder paper organize follow section discus detail concept realize volatility section introduces linear har rv model section give information theory behind nonlinear algorithm exact training model introduce section section provide descriptive statistic concern data section cover actual empirical result work section concludes realize volatility cial asset reason volatility use many purpose approximation measure risk asset allocation risk management option pricing since true volatility observable struggle find good proxy always trend conditional heteroscedasticity stochastic volatility model cover common way calculate forecast volatility unfortunately two model heavily depend detection correspond underlie process volatility result strong restriction parameter estimate measure volatility base type data first proposal direction make merton notice conditional variance obtain sum square intra daily return data sufficiently high frequency one hand specific assumption andersen andersen demonstrate work use high frequency data successfully approximate conditional variance hand barndorff nielsen shephard prove realize volatility rv could serve estimate latent variability log return financial data ex post volatility estimation error jump exclude becomes observable could forecast mean predict algorithm let briefly review realize volatility order prove good approach predict conditional volatility start notation introduce example represent univariate process logarithmic price daily return entire available information time period suppose dp dw deviation strictly positive square integrable standard brownian motion bucci let return period dw continuous time interval quadratic variation qv way variability stochastic integration theory measure bucci equation prof drift innovation contribute volatility return case quadratic variation qv coincides integrate variance iv however true data jump noise assume available data obtain follow equation iv qv accord andersen quadratic variation could also approximate plim consider fact daily return define intraday return look like could derive realize variance sum intra daily square return converges probability qv correspondingly iv long world realize variance rv lead rv iv take notional volatility account equates quadratic variation return series time interval one could easily prove realize volatility consistent estimator notional volatility rv recalculation obtain rv ji ji follow specific assumption process square integrable realize volatility unbiased estimator conditional volatility rv ji qv ji var ji equation couple conditional variance autoregressive conditional heteroskedasticity arch model realize volatility word one could develop model predict conditional volatility use realize volatility empirical evidence establish follow stylize fact realize volatility firstly suggests existence long memory data kruse exactly portfolio cryptocurrencies also would result well performance svalueatrisk third another promising fact good predict model realize volatility would find financial application risk management unfortunately use realize volatility proxy also disadvantage example clear data frequency suitable one want make accurate prediction issue consider however since case minute interval data available would concentrate case additionally use realize volatility measure unobservable fluctuation become especially popular publishing work andersen show intra daily observation go zero realize volatility consistent estimator since go beyond scope particular work remainder would assume criterion met would move next section reveal technique use predict realize volatility linear model section discus popular linear model forecasting realize volatility namely heterogenous autoregressive model aim give good argu mentation model include benchmark work second half section extension har rv also introduce heterogeneous autoregressive model additive cascade model go low high frequency component propose corsi fair tailsandself similarity easy economic intuition heterogeneous autoregressive model different market player take action base different temporal horizon volatility refers lleretal market react differently diffident temporal component volatility another find corsi include participant actually increase volatility market hand heterogenous market cause improvement convergence third fact found paper different geographical location market actor contribute also heterogeneity market consider three finding one could conclude take heterogeneous market hypothesis account especially context cryptocurrency trading would step right direction since daily realize volatility determines high frequency return process wtihr able make prediction daily realize volatility next period suggest model linear function considers partial daily weekly monthly volatility since author concentrate typical financial market trading day week day month correspondent formula look like rv rv rv equation rv rv rv represent monthly weekly daily realize volatility correspond iid error term intercept finally coefficient explanatory variable look closer one could notice equation consist ar part second part expectation next large level cascade straightforward recursive substitution partial volatility consider fact cascade equation could rewrite rv rv rv equation three factor model base realize volatility different frequency represent exact model propose corsi easily notice left hand side actually rv latent realize volatility estimation error mean equation could reformulate rv rv rv rv equation simple autoregressive form include factor realize volatil ities different time therefore label har rv bunch empirical simulation study show propose har rv model successfully cover many aspect financial time series demonstrate har rv fat tail self similarity importantly study capture stylize fact long memory dependency typical kind data additionally model propose corsi show positive result cover multifractality proven despite feature model remains simple economic interpretation calculation behind hide nothing ols estimator due fact assume heterogeneous autoregressive model good model predict realize volatility realize purpose particular work make small modification model state equation since cryptocurrencies trade hour day day year need rewrite regression equation assume trading day consists hour week day month day extension heterogeneous autoregressive model various empirical work propose extension heterogeneous autoregressive model subsection popular model briefly present classic har rv model assumes price process constant however reality turn process rather mixture continuous discontinuous part part volatility cause jump dp dt dw dq dq responsible jump quadratic variation process take form qv quadratic variation consists integrate variance part iv rt know jump variation wesimplyaddthejumps tothestandardhar rvmodel obtain extension call har rv propose andersen rv rv rv rv andersen introduce also another model call har rv cj base explicit separation realize variance continuous jump component word classic explanatory variable har rv replace daily weekly monthly continuous jump part final formula take form equation rv har rv cj advantage allows observation exact contribution single element regardless continuous jump leverage heterogeneous autoregressive model lhar introduce corsi reno extends heterogeneous structure leverage effect large increase volatility negative shock positive one dependency model asymmetric response realize volatility another extension heterogeneous autoregressive continuous volatility jump lhar cj model introduce corsi reno author identify three main component cause dynamic financial market namely heterogeneity leverage jump ing accuracy prediction power negative past return especially point consider list element model estimate ols newey west covariance correction serial correlation logv log log log logc logc logc represent jump component stay continuous element mark negative return atree tree har andcorsi cover two stylize fact realize correlation namely strong temporal dependence structural break audrino corsi demonstrate three har also outperforms classical predict algorithm like ar arma arima tree ar andthestandardhar rvmodel thedailytick har model take follow form fu sequence innovation expect value zero variance conditional expectation time conditional dynamic give rgc jw rgc jm rgc tpred rj tpred rj parameterizes local har dynamic last extension heterogenous autoregressive gamma leverage hargl option pricing model us gain information realize volatility theempirical work prof two factor especially important performance model namely use realize volatility provide good fast adapt proxy unobserved volatility hand high persistence smooth cause hargl model due feature algorithm hargl well reproduce dynamic hence achieve well result typical garch option pricing model one take consideration hargl model differs classic har rv way explanatory variable calculate namely rv rv rv rv prevents overlap volatility various period finally one could conclude various extension har rv model successful application real world data prove predict power algorithm nonlinear model explain methodology hiding behind hybrid model firstly need intro duce popular algorithm machine learn ml namely artificial neural network ann concretely feedforward neural network fnn since fnn recurrent neural network rnns share huge amount similarity next subsection intro duce important definition parameter valid nonlinear algorithm particular work recent year order capture nonlinearity forecasting model attention give technique machine learn specifically neural network nns section discus different architecture artificial neural network explain since look model combine nonlinearity memory feature natural suggestion implementation recurrent neural network due fact develop also hybrid fnn har rv reveal theory behind simple ann well section divide follow start introduce classical feedforward neural network afterwards discus detail recurrent neural network precisely three suitable architecture proven efficient nns empirical work financial time series deep feedforward neural network deep feedforward neural network also call multilayer perceptrons mlp gain attention recent year could explain fact volume data use increase dramatically performance modern computer improves constantly importantly nonlinear technique perform pretty good context predictive analytics fnn emerge popular machine learn algorithm call perception developed rosenblatt inspire mcculloch pitt idea behind model approximation function like instance function map input variable target value exactly fnn define mapping find value result best approximation function know feedforward information flow one way function though function achieves target value word feedback connection back propagation algorithm type neural network discuss later neural network call like typically represent combine level connection output resembles structure biological neural network instance one could three various function let say function couple chain translate language ml neural network three layer whereby first function represent input layer contain input value second layer case hidden layer last one output layer length chain show depth particular nn finally dimension hidden layer defines exact width model unit layer could associate role neuron neuroscience therefore unit also call neuron one way understand feedforward network begin linear model consider problem nonlinearity could solve nns could efficiently use transform linear model nonlinear function one could make use nonlinear model directly stay nonlinear conversion however step probably one would ask exactly formulate intheneuralnetworks use learn map target value could interpret hidden layer essentially parametrize transform linear model apply optimization algorithm discuss detail section estimate associate good representation wediscusssome main concept detail single layer perceptron multilayer perceptron single layer perceptron slp show feedforward network base threshold transfer function slp simplest ann could classify linearly formula ix output else stay weight th input slp however know initial weight therefore randomly assign begin algorithm sum weight input result large certain threshold slp neural network activate output equal otherwise see equiation introduce slp work properly case linearly separable output output input output layer layer input input output input single layer perception common example inability slp solve problem linearly non separable data xor function exclusive problem could solve mean multilayer perception mlp use back propagation algorithm however let firstly introduce traditional example xor function problem use neural network exactly two dummy variable input xor function return exactly least one input variable equal otherwise express differently want learn target function model proposes make close possible next step minimize loss function simplicity goodfellow suggest mse although best loss function come dummy variable suppose choose linear model obtain follow function thelinearmodelgives asaresulteverywhere tosolvethisproblem weapplythesim computedbyf hidden unit use input output layer end still linear model output produce hidden unit raw input value whole model obtains follow form mean linear whole model remains linear need activation function transform feature lead weight linear transformation bias one take consideration activation function normally chosen applicable element yield laterinsection disadvantage detail particular case make use popular relu activation whole neural network look like max take equation account easily could estimate consider input matrix establish thanks dummy variable replace parameter equation value equation obtain exact target value however ofcourse example could neural network one hidden layer would change whole model depth depth depth depth depth ann two hidden layer call deep neural network dnn see example problem neural network dimension width depth focus many empirical work practice question simple depends data aim issue discuss detail section however lippmann show mlp two hidden layer sufficient create classification region desire shape instructive although note indication many node use layer learn weight give hand alvarez salzmann prove number neuron could reduce accuracy prediction still roughly corresponds aim purpose simple architecture neuronal network move next type nns actual learn network illustrate another important concept namely back propagation back propagation algorithm groundbreaking article rumelhart introduce back propagation train ing algorithm first time training instance algorithm feed network estimate output every single neuron every single layer af ter calculate network output find error forecast value desire value possible thanks specific problem loss func tion introduce various type loss function section next step back propagation algorithm find much neuron last layer contribute output neuron error proceeds like layer layer algorithm reach input layer namely backward step algorithm define back propagation phase estimate error gradient across connection weight input hidden hidden output layer layer layer layer input input input output input deep neural network two hidden layer propagate error reverse backward direction end back propagation algorithm gradient descent step realize calculation use error gra dients already measure gradient descent word engine hiding behind back propagation algorithm make learn process possible term explain detail section recurrent neural network recurrent neural network another family anns form take feed forward network add additional connection previous layer already introduce back propagation algorithm also successfully train network however pattern must always sequential form unlike feedforward neural network rnns extra neuron connect directly hidden layer like input neuron additional element hold information one layer training nn also previous time step extra neuron call context unit contributes long term memory feature recurrent neural network due property model increasingly important predict financial time series one hand language picture recognition hand order make whole picture clear consider different architecture rnns discus suitable predict value dynamic process like input hidden output layer layer layer input output simple recurrent neural network daily realize volatility cryptocurrencies simple recurrent neural network term simple recurrent network srn often refers network architecture propose jordan elman elman network illustrate three layer new element context unit unit connect hidden layer constant weight one idea fix weight recurrent edge fundamendtial long short term memory network however would introduce detail correspond section learn step input value fed forward make use learn rule propose former section point back connection firstly make copy previously see value hidden unit information use next step learn algorithm thus network maintain information make useful problem sequencial data emlan network equivalent simple rnn hidden node single self connect recurrent edge network introduce jordan similar difference hidden layer extend call state unit value output node fed special unit afterwards feed value node hidden layer next time step additionally state unit self connect contribute send information across multiple time step without perturb output intermediate time step lipton recurrent connection state unit output state unit make possible output period use input period input hidden output layer layer layer input input output input state unit simple jordan recurrent neural network represent simple jordan network one hidden layer neuron output phase equation correspond parameter follow input vector hidden layer output vector parameter matrix activation function note recurrent neural network become especially attractive field finance due ability capture nonlinearity solve long memory problem typical financial time series example lawrence compare prediction simple recurrent neural network statistical regression tech niques show rnn able predict price movement correctly benchmark model perform rate wang leu additionally research prediction power simple recurrent neural network put factor arima propose nn demonstrate model able deliver acceptable accuracy prediction period week ahead however ing intraday realize volatility something research intensively liu demonstrate rnn similar result compare har rv nonlinear model need much shorter input time frame signal histor ical data scarce could rely model field modern ai furthermore error rnn uniformly low linear model reduce case large historical data set author also try find financial application prediction prove model contributes attractive sharpe ratio trading volatile derivative however training recurrent neural network long sequence like associate financial time series model run many time series cause deep neural network especially relevant case problem propagation input particular layer apply activation function regard popular function later like sigmoid small value derivate get constantly small result vanish gradient move start layer thus becomes difficult train start layer algorithm never converges good solution problem call vanish gradient also another case rather encoun tered recurrent neural network gradient grow big big many layer get large weight corresponds state call exploiding gradient problem trick overcome two problem like good parameter initialization faster op timizers non saturate activation function see section however rnn need handle even moderately longs sequence input training still slow ron solve issue various type architecture cell long term memory suggest popular among empirical researcher get long short term memory cell gradient recurrent unit demonstrate far well result simple recurrent neural network recent year basic cell found application anymore fact aim make use newly propose cell order output ht cell cell ct ct tanh hidden tanh hidden ht ht input xt architecture lstm predict best way realize volatility therefore regard architecture detail next two section long short term memory note order solve problem vanish expoiding gradient researcher developed long short term memory model lstm gain importance recent year algorithm propose hochreiter schmidhuber en hanced year several researcher zaremba graf schmidhuber basically lstm could consider like normal cell differ ence performance much well time estimation convergence avoid long term dependency problem rnns know form chain repeat module neural network standard chain represent lstm structure well however instead single neural network layer four layer interact specifically typical lstm network see compose different memory block call cell letwesay cell cell state know cell state hidden state manipulation memory important component lstm discuss cell state memory cell key lstm run whole neural network chain whereas small linear correction undertaken contributes flow constant error memory block point infor mation give cell state could characterize three different dependency could generalize like firstly information previous step previous cell state case memorize realize volatility former period sec ondly output previous cell know also previous hidden state finally input current time period however discuss model able remove add information cell state decides pas block information base give weight gate cell define forget gate responsible delete information cell state infor mation much importance require delete via multiplication filter due feature lstm performance model improve forget gate require two input respectively hid den state previous period input value current period multiply weight matrix bias add follow sigmoid function value apply product basically function contributes decide value keep pas whereas mean forget gate want remove value corresponds memorize give information vector multiply cell state input gate play main role addition new information cell state stepprocess afterwards avectorcontaining possible information create do mean tanh function give value range form last step value sigmoid function multiply vector value tanh function useful information put cell state task select information current cell state select show output undertaken via output gate could consider three step process first phase help tanh function value cell state scale value afterwards filter use value sigmoid function apply order choose exactly give output finally vector multiply information transform output also sent next cell go whole architecture step step deal follow equation tanh tanh sigmoid function weight matrix hidden state current input error term tanh tanh activation function cell state respectively forget input output gate termmemorymodel forecasting realize volatility next period algorithm however find broad model power forecasting correspond value increase popularity machine learn technique increase also number attempt predict stock price volatility mean algorithm example mcnally show lstm achieves well accuracy predict direction bitcoin price compete autoregressive integrate move average model arima additionally yu li also demonstrate superiority lstm model predict volatility shanghai compos stock price index compare value four type loss function illustrate lstm model well predict effect result fact knowledge attempt predict realize volatility lstm convince include algorithm among model introduce basic lstm algorithm fact almost every empirical work related model suggests slightly different variation define author im provement lstm opinion gate recurrent unit worth discuss detail purpose work therefore regard separately section gate recurrent unit another efficient way solve gradient vanish problem gate recurrent unit gru algorithm propose cho order make recurrent unit able capture long term dependency like lstm network consists gate unit control flow information however without special memory cell architecture contrast lstm us forget input output gate gru operates use reset update gate reset gate situate previous activation ontheotherhand theupdate gate selects much candidate activation use update cell state gru visualize stay reset gate update gate intuitively reset gate regulates couple new information previous one update gate determines information previous time period kept gru could regard also specific vanilla case rnn reset set update gate let go whole architecture step step one start calculate update gate mean follow equation wzx uzh wherex multiply sigmoid activation function lead product afterwards gru selects information maintain possible thanks reset gate make use formula bellow wrx urh resembles equation difference come weight main object gate let see exactly gate reflect output first new memory content detect store next important information past procedure complete reset gate us follow equation tanh ut wh multiply input weight add element wise product reset gate wh determines remove previous period sum result multiply nonlinear tanh activation function finally calculate information vector pass network point update gate include decides kept current memory content former period process follow equation efficiency gru focus chung evaluate model task polyphonic music model speech signal model author conclude lstm gru outperform significantly classical rnn however modification lstm gru differ significantly di persio honchar compare model outcome rnn lstm order make forecast google asset author demonstrate gru perform slightly well algorithm small training data set property model make optimistic try forecast realize volatility high asset cryptocurrencies hidden ht hidden ht rt zt tanh input xt architecture gru hybrid neural network har artificial intelligent technique especially valuable come make prediction extremely volatile time series data unfortunately application forecasting therefore inourcurrentwork aim contribute new result area research goal concentrate mainly empirical work arneri author build hybrid model classical har rv feedforward network fnn estimation arneri etal fnn author model prove one hand come sample prediction accuracy fnn har model slightly outperforms benchmarked har rv har rv cj although base diebold mariano test difference sig nificant hand classical har rv model behave well sample prediction additionally say arneri use full potential nns intentionally include large number neuron hidden layer since lead overfitting distortion accuracy measurement similar model find usage barun ehl different time lag oil price enter fnn input variable contribute prediction price compare prediction propose fnn har forecast har rv arfima turn hybrid fnn har model yield low error uniformly test period also show well performance lead economic benefit especially calculate future value risk accord author work median realize volatility propose andersen another factor contributes well result equation take follow form icv hedrv nn med ytj ytj ytj stay price jump size number intraday yt observation etal andbarun kandk ehl end new algorithm able reproduce memory feature financial series also nonlinear character thanks activation function nns allcompetingmodels however exact selection hyperparamers training algorithm explain detail follow section therefore let move forward estimate training predict model particular section would regard model train additionally discus briefly parameter fine tune happen also reveal theory accompany hyperparameters especially neural network correspond empirical result summarize section whereby say calculation within work do python mostly make use module scikit learn kera respectively har rv neural network estimate har rv heterogeneous autoregressive model realize volatility take form simple linearregression theauthorsworkwith asset trade standard stock market assume trading day consists hour week day month day however exchange market cryptocurrency restriction since trade hour day day year due fact need modify equation new linear regression obtains follow form rv rv rv rv rvolharfnn whererv rv istherealized volatility monthly realize volatility original equation replace rv corresponds stay error term exact estimation model could found quantlet cite use training data set estimate coefficient linear regression ofequation ols test data make prediction training tune neural network since training neural network combine hyperparameters step common type nns use empirical work would regard detail procedure particular section step step give reason choose specific parameter training neural network belongs algorithm associate supervise learn ing supervise learn described procedure input data set output thanks give information input data type learn call supervise resembles teacher school corrects student false answer case algorithm correct forecast large deviation target value true realize volatility supervise learn split two main group classification case output variable category example blue red green stock price raise stock price fall regression situation result real value weight object thus naturally lead class loss function order correct error prediction make algorithm specify loss function word need function goodfellow measure quality prediction loss function could also divide two group classification regression problem however since work predict exact value associate case loss function consider one common occasion function available also extension tensorflow kera summarize mean square error commonly use function regression case simply sum square distance target forecast value mse take value mse mean absolute error equal sum absolute difference target value prediction word calculates absolute deviation target consider direction deviation value also range mae jy mean absolute percentage error usually express accuracy percentage error whereby us follow formula mape mean square logarithmic error rmsle measure ratio actual predict value follow rmsle log log log cosh loss function log cosh log cosh realize goal empirical work choose set mse standard instrument error measure training neural network thus mse part training procedure simple rnn lstm gru also hybrid fnn har consider various loss function one select appropriate optimization algorithm help learn optimal parameter correspond model optimization function optimization function help minimize maximize loss function relevant work optimization function could present bellow gradient descent important technique intelligent system train typical algorithm small step take direction negative error gradient parameter oq learn rate oq gradient loss function respect vector basedonthis procedure complete gradient equal zero already achieve minimum concretely start passing random value step call random initialization next phase improve gradually whereby algorithm converges direction min imum small step decrease loss function time unfortunately procedure drawback get easily stuck local minimum another disadvantage associate traditional gradient descent calculates gradient whole training data set result slow process large data set stochastic gradient descent sgd ensures update small step training example need less time general sgd pick random instance estimate gradient fromthissmallsample however average solution problematic mini batch gradient descent principle gorithm estimate gradient small random sample instance call mini batch mini batch consists observation define batch size batch size responsible neural network weight update afterwards whole gradient calculate average value mini batch whereby iteration result follow update oq mini batch whole training data set training neural network stick sgd mini batch test optimal batch size choose mini batch size train neural network faster optimization algorithm far consider classic stochastic gradient descent mini batch solution improve time performance algorithm large data set use fact could achieve also faster optimizers would cover important one paragraph momentum optimization considers iteration contrast stochastic gradient descent previous gradient step algorithm subtracs local gradient momentum vector actualize weight add vector express differently point gradient serf acceleration speed equation momentum optimization take form show oq parameter prevent momentum grow large unfortunately add one parameter tune could consider disadvantage algorithm adagrad decay learn rate however faster steep dimension dimension gentler slope feature name adaptive learn rate contributes quick convergence result update toward global optimum another advantage algorithm require much tune learn rate correspond formula present oq oq accumulate square og gradient symbol element wise multipli cation represent element wise division however proven adagrad algorithm slows quickly actually never converges optimum due negative side algorithm rmsprop propose accumulates recent iteration use exponential decay begin rmsprop obtain mean equation oq oq oq combination rmsprop momentum optimization call adam optimization momentum optimization include exponentially decay average past gradient hand rmsprop involves exponentially decay average past square gradient oq oq oq similarly rmsprop adagrad adam optimization adaptive learn rate algorithm therefore need fine tune therefore technique mostly prefer training neural network empirical work learn correspond nns make use adam optimizer since combine best property two adaptive algorithm additionally easy configure default configuration parameter well problem test different learn rate choose apply propose model avoid overfitting regularization neural network typically dozen even thousand parameter make nns able fit best way complex data however occasion often refer problem call overfitting go hand hand opposite state underfitting concept introduce overfitting state model memorizes exactly pattern training data look like fail generalize patter test data set word large deviation error training test data set could expect underfitting opposite situation model fails learn patter data thus predict error get large introduce neural network often deal problem overfitting due fact researcher developed technique solve issue early stop simple strategy stop training performance test data set start decrease technique work well practice could even improve combine technique dropout popular regularization technique propose krizhevsky despite efficiency algorithm work pretty simply every training step every neuron probability exclude drop network completely ignore step probably include next one parameter call dropout rate usually define max norm regularization another relatively popular technique avoid overfitting set upper bound magnitude weight vector every neuron whereas applies project gradient descent enforce constraint main advantage network explode even large learn rate include however aim possibly best predict algorithm take account price cryptocurrencies change dynamically generalization outside train data set always challenge therefore avoid add new regulate layer neural network activation function see section activation function play probably main role training ournonlinearmodels essentially meaning responsible decide give information relevant refer equation activation function one fire fire correspond hidden unit arise question could train neural network without specific activation function answer yes however case model would nothing simple regression discus popular function contribute aim nonlinearity neural network sigmoid probably popular nonlinear activation function reason give solution already discuss gradient vanish problem however function also negative side output non zero center cause slow convergence additionally function saturates kill gradient form present function take value range tanh similar characteristic sigmoid function however gradient strong tanh derivative slightly steeper bound relu orrectified linear unit plus activation function importantly upper bound reduces issue connect gradient descent additionally sparsity activation could point another advantage large complex neural network around neuron fire due characteristic relu lead lighter version model however could detect also thesocalleddying relu die start output zero happen unlikely particular unit come back life max leaky relu solution mention problem textitrelu leaky relu additional parameter define much give information leak word hyperparameter ensures around neuron never die mathematical equation hiding behind leaky relu look like max typically set elu exponential linear unit propose clevert author demonstrate elu outperforms suggest nonlinear activation function test data set function obtains follow form exp allows take negative value form elu avoids dying theparameter signalizes value function approach relative large negative furthermore function smooth however elu also characterize disadvantage slow computation discuss main activation function natural ask function suit best solve regression problem since deal predict realize volatility need activation function give value range average around zero additionally work fine large amount data criterion fulfil elu apply activation function layer neural network number unit layer already introduce concept overfitting cause large number parameter neural network property nns able predict sufficiently good accuracy output data set complex data pattern however since want compare outcome different linear nonlinear model word ensure fair comparison linear har rv rnns state reason apply neural network srn lstm gru also hybrid fnn har consist three layer input layer hidden layer output layer whereby number hidden unit two result neural network construct parameter number epoch epoch another important hyperparameter training neural network one epoch result entire data set pass forward backward whole neural network neural network weight one pas epoch would result underfitting large number epoch would lead likely overfitting right number passing data forward backward neural network unfortunately clear rule define suitable number epoch trail detect epoch fit well data cause neither underfitting overfitting summary summary training fine tune neural network complex process consist many small step however order ensure competitive linear nonlinear model environment predict realize volatility cryptocurrencies keep apply neural network simple possible due fact nns construct layer hidden unit whereby neuron fire elu activation function gradient descent whereby start learn rate set batch cover observation number forward backward pass call number epoch equal parameter summarize since use har rv benchmark model comparable condition neural network provide equation us explanatory factor variable one time period therefore neural network input choose daily realize volatility current time period attempt forecast value paramater value number hidden layer number hidden unit activation function elu optimizer adam learn rate batch size epoch total number parameter hyperparameters recurrent neural network srn lstm gru rvolrnn target value corresponds variable shift hour parameter state refer recurrent neural network propose study srn lstm gru exact training network could found quantlet cite caption additionally hybrid model fnn har rv however therefore regard separately next section training hybrid fnn har section introduce hybrid model aim combine memory feature har rv model nonlinearity classic feedforward neural network order achieve goal calculate daily realize volatility rv weekly one rv monthly one rv equation three variable use input variable feed forward neural network whereby consider value time period forecasting realize volatility move window meaning target value equal rv end end model take memory har rv also use nonlinearity simple fnn illustrates hybrid model fnn har assumes two neuron one hidden layer whereby input variable daily weekly monthly realize volatile calculate adjust formula section hyper parameter associate nns remain unchanged take value rnns input hidden output layer layer layer input input output input structure hybrid fnn har model rvolharfnn show correspond quantlet building hybrid model stay caption data realization goal particular master thesis use data grant bruno spilak dyos point want thank share high frequency data cryptocurrencies offer data cover time period include minute frequency data bitcoin btc ethereum eth litecoin ltc starcoin str monero xmr ripple xrp minute interval data set reveals information opening closing high low price usd six available cryptocurrencies plus correspond volume trade unit time series daily closing price present becomes obvious coin emerge market later period therefore choose concentrate last day data namely correspond descriptive statistic six coin represent purpose particular work create portfolio available cryptocur additionally wouldshowus broader perspective financial application forecast realize volatility therefore prince coin btc blue eth red ltc green str orange xmr violet xrp black rvolprep btc eth ltc str xmr xrp min max mean median std st quartile rd quartile descriptive statistic price six cryptocurrencies rvolprep price evolution price weight portfolio six coin rvolprep combine coin single portfolio unfortunately lack market capitalization data require follow way crix index establish trimborn rdle assume strategy price weight portfolio price evolution follow formula give portf btc btc eth eth ltc ltc str str xmr xmr xrp xrp respectively correspond coin weight price additionally equally weight portfolio single omega btc eth ltc str xmr xrp obtain value evolution daily closing price portfolio certain period time visualize fact empirical work relates daily realize volatility calculate log return portfolio accord equation ln whereby log return portfolio present descriptive standarddeviation skewness test could found log return portfolio rvolprep btc eth ltc str xmr xrp portfolio mean std skew kurt jb descriptive statistic log return rvolprep unfortunately data available six coin enough good training neural network therefore decide work roll window consider day hour observation minute interval hour keep add next minute time remove first observation rowssequence inotherwords weconsider value hour period order lose information relatively short sequence data value log return daily realize volatility weekly realize volatility monthly realize volatility firstly estimate whole data set do due fact concentrate specific period give time series would lose information first day order find monthly realize volatility calculate necessary variable could move forward exact empirical result research empirical application cryptocurrencies know price fluctuation make accurate prediction volatility extremely difficult one notice even short data set period time distinguish high volatility log return hand last month seem relatively quiet huge price deviation capture thesefacts also period relatively low volatility additionally vary amount data available training test data set do order check ability model learn shorter time interval also control predict power different time horizon predict power apply model judged mean classic coef ficients estimate accuracy statistic forecast call loss function section model also compare reason apply standard mariano test dm test dieboldandmariano essentially errorsrespectivelye ande jt loss function null hypothesis test signalizes equal predictive accuracy model time period idea behind test corresponds follow equation jt stay correspond loss function code use visualization forecast calculate diebold mariano test part cite quantlet empirical work finalize financial application model predict var multiple period ahead strength approach control backtests move exact empirical result let firstly introduce na approach frequently use benchmark work time series na approach fact us forecast true value previous period word predict realize volatility na approach would assume true value daily realize volatility period still accurate kind make prediction consider work quite well many economical financial time series therefore usually assume benchmark many scientific work however accord zakamulin na approach use make forecast long run also criticize persistent nevertheless model use benchmark empirical work accuracy predict model order look accurately model work various situation predict error demonstrate choose concentrate two separate time interval data set first one cover last three month characterize high price fluctuation second time window consists observation high frequency time firstly wetakethe wholeperiodfrom predict realize volatility high volatility time target value navy na yellow har rv grey fnn har brown srn green lstm orange gru red left panel long case right panel short case rvolaccur na har fnn har srn lstm gru rmse mae rmsle rmse mae rmsle prediction error high volatility time long upper panel training data set prediction error bottom panel sample prediction error note low number number multiply rvolaccur data science namely first belong training data set rest test data set maintain sequence time series time dependence afterwards training data set reduce extract middle day time interval predict model train simplicity end paper call case longer shorter training data set respectively long case short case forecast realize volatility predict model show one could easily notice output model similar course predict time series follow almost exactly evolution target value however difference forecast seem unable reproduce na har fnn har srn lstm gru rmse mae rmsle rmse mae rmsle prediction error high volatility time short upper panel training data set prediction error bottom panel sample prediction error note low number number multiply rvolaccur na har fnn har srn lstm har fnn har srn lstm gru har fnn har srn lstm gru test statistic diebold mariano test sample prediction high volatility time upper panel long case bottom panel short case rvolaccur extreme value error forecast summarize corre sponding result diebold mariano test sample prediction cover occurs interest long case high volatility time series prediction seem relatively similar result fact support also result dm test find model significantly different others exception make couple har fnn har rather signalizes hybrid model bad predict power scenario hand look short case one could notice recurrent neural network tend perform slightly well model however difference predict accuracy significant point research important stress deal financial time series cryptocurrencies suggests significant jump noise data natural patter data set capture linear model would rather require nonlinear model accurate prediction case hybrid model recurrent neural network significantly outperform linear har model assign fact research nonlinear algorithm kept verysimple unit thus become unable reveal potential predict situation high volatility low frequency time section regard ability model forecast realize volatility train low volatility time procedure section repeat length training data set varied end situation long case short case forecast algorithm plot whereby correspond prediction error long case short case summarize respectively one could notice graph model able capture fluctuation direction target value found previous section none model reproduces extreme value except na model follow strictly true value previous hour however one thing could distinguish predict realize volatility low volatility time target value navy na yellow har rv grey fnn har brown srn green lstm orange gru red left panel long case right panel short case rvolaccur na har fnn har srn lstm gru rmse mae rmsle rmse mae rmsle prediction error low volatility time long upper panel training data set prediction error bottom panel sample prediction error note low number number multiply rvolaccur well lstm model seem much small variation prediction especially case less amount training data correspond error prediction long case demonstrate nonlinear model tend small error compare model statement also confirm value upper panel null hypothesis equality predict model reject always one model couple na har rv model fact also result sign nonlinear model strong longer training period low volatility time whole picture change significantly short case judging result na har fnn har srn lstm gru rmse mae rmsle rmse mae rmsle prediction error low volatility time short upper panel training data set prediction error bottom panel sample prediction error note low number number multiply rvolaccur na har fnn har srn lstm har fnn har srn lstm gru har fnn har srn lstm gru test statistic diebold mariano test sample prediction low volatility time upper panel long case bottom panel short case rvolaccur one could say nonlinear model still well na approach har rv however new find recurrent neural network seem outperform algorithm look output bottom panel becomes obvious particularly simple recurrent neural network lstm model different forecasting result model summary one could say nonlinear model perform significantly well long case short case find high volatility time scenario recurrent neural network tend predict low error remains valid also low volatility time additionally show accuracy srn lstm significantly different model fact would suggest probably recurrent neural network concretely srn lstm well predict model low volatility time find also corresponds hypothesis low number hyperparametrs use nonlinear model reason inability perform significantly well na har model high volatility time however hyperparameteres nonlinear model well make forecast also require less information training phase financial application forecast realize volatility next high low volatility time useful could find financial application output degiannakis potamia checked ability intra day return predict cor rectly value risk var expect shortfall stock index commodity however provide good result step ahead step ahead multiple period ahead var confidence level giot laurent also contribute volatility calculate one day ahead move window however accord author approach seem improvement standard method hand kruse show model link realize volatility able build suitable var model come one period ahead forecast author also note performance model assume normality relatively good additionally bedowska sojka also prove var estimate obtain classic model daily return comparable result calculate realize volatility however long memory feature asymmetry realize volatility seem improve slightly prediction furthermore forexample corsietal present propose rv model outperforms compete stochastic volatility option pricing model index option stentoft also demonstrate model realize volatility use option pricing lead well result traditional model base inter daily data consider state fact chose apply forecast calculate multiple period ahead var validity control classic backtesting method described var method backtests use quantlet contain code financial application cite caption value risk value risk quantifies market risk portfolio market fluctuation next period usually number define amount money could lose within certain horizon whereby probability event equal var come log return var usually define follow var rest empirical work var refers left tail portfolio return distri bution long position word take follow form varl backtesting regard two method var backtesting apply research kupiec propose test call proportion failure pof coverage test ever mostly cite literature like kupiec test unconditional coverage test propose test work binomial distribution approach likelihood ratio order test probability exception corresponds probability expect give confidence level var null hypothesis model able predict var reject give data probability exception different test statistic take follow form xpx lr ln uc number failure prediction number observation next test use validate power var prediction introduce christoffersen measure probability observe exception par ticular day dependent whether exception occur previous period test considers dependency consecutive day therefore also know conditional coverage test test statistic give lr log cc number period failure follow period failure number period failure follow period failure number period failure follow period failure number period failure follow period failure probability failure give failure period probability failure give failure period probability failure applicaiton value risk backtesting show introduction section var could estimate different period ahead however training model small number step ahead move window verytime consumingprocess therefore decide look var forecast behave multiple period ahead step word consider forecast model long case high low volatility scenario calculate correspond value risk whereby assume normal distribution measure performance singlemodels since focus move window length single step minute mean first step also daily log return calculate result test high volatility time summarize correspond show since null hypothesis unconditional coverage test kupiec test conditional coverage test christoffersen test reject one could conclude none predict model give output able use forecasting var multiple period ahead however take account recurrent neural network tend closer number violation expect exceed model statement also corresponds forecasting result show section contains result backtesting low volatility time sulting visualization attach value also confirm state conclusion power model predict correctly value risk additionally tendency well forecast recurrent neural network accord number violation remains unchanged however small improvement value unconditional coverage test null hypothesis still reject finally could conclude predict model value realize volatility test result unconditional conditional coverage test could explain fluctuation price typical cryptocurrencies suggests accordingtocorsi thus var would probably estimate falsely since assume realize volatility converges integrate volatility case sufficiently high frequency could point another reason poor performance model demonstrate result however correspond empirical work research area since still show realize volatility clear improvement one try approximate true volatility financial time series conclusion paper consider accuracy various model forecasting realize volatility exp exceed violation uc cc na har fnn har srn lstm gru na har fnn har srn lstm gru backtesting high volatility time uc unconditional coverage test cc conditional coverage test upper panel bottom panel rvolbackt model realize volatility har rv distinguishes proven long memory feature however model take form simple linear regression order include nonlinearity forecast technique machine learn technique apply weinvolvethesimple recurrent neural network srn variation like long short term memory lstm gate recurrent unit gru train correspondence call regressionproblem feedforward neural network heterogenous autoregressive model combination call fnn har two model memory feature link har rv also nonlinear property due use activation function neural network empirical contribution work field research briefly summarize model predict realize volatility train model two different period time rencies define high volatility time period exp exceed violation uc cc na har fnn har srn lstm gru na har fnn har srn lstm gru backtesting low volatility time uc unconditional coverage test cc conditional coverage test upper panel bottom panel rvolbackt low volatility period time window addi tionally also regard two case call long case short case volatility time furthermore reduce amount training data set also allow con troll model deal less information result provide research one could summarize high volatility time model perform equally accord diebold mariano test however short case become clear recurrent neural network tend well forecast learn well shorter period time hand low volatility time nonlinear model fnn har srn lstm gru significantly well forecasting result additionally tendency strong recurrent neural network short case approve empirical result conclude na approach linear har rv model good solution one try predict realize volatility portfolio since cryptocurrencies know high fluctuation price one assume jump noise time series supposes nonlinearity data however use nonlinear model kept simple possible purpose order outnumber benchmark method amount hyperparameters however result disadvantage neural network simplicity chose kind model allow cover data pattern training data set thus probably provide poorer forecast wealsoappliedtheout samplepredictions model approximation true volatility multiple period ahead value risk calculate various level high volatility low volatility time however turn none algorithm able produce forecast realize volatility value could use correct calculation multiple step ahead value risk nonetheless show despite simplicity structure recurrent neural network give closer number violation expect exceed model possible reason failure var approach point possible jump noise time series also probably accurate frequency data particular work include extension heterogenous au toregressive model call har rv har rv cj able predict realize volatility time series jump since work data high fluctua tions consider model would good start point research area additionally reveal full potential nonlinear model fnn har however training deep neural network forecasting realize volatility cryptocurrencies another point could research detail work since ability realize volatility preciser calculation value risk controversial one could think change approach calculate var one period ahead move window consider another distribution another interest point could application forecast cryptocurrencies option pricing present work view contribution research area forecasting true volatility market cryptocurrencies turn however still enough point consider enhance future empirical work alvarez salzmann learn number neuron deep network advance neural information processing system andersen bollerslev answer skeptic yes standard volatility model provide accurate forecast international economic review andersen bollerslev andf diebold roughingitup include jump component measurement model forecasting return volatility review economics statistic andersen bollerslev diebold labys exchange rate return standardize realize volatility nearly gaussian tech rep national bureau economic research model forecasting realize volatility econometrica andersen bollerslev meddahi correct error volatility forecast evaluation use high frequency data realize volatility econo metrica andersen dobrev schaumburg jump robust volatility estimation use near neighbor truncation journal econometrics arneri poklepovi teai neural network approach forecasting realize variance use high frequency data business system research journal audrino corsi model tick tick realize correlation computa tional statistic data analysis barndorff nielsen shephard econometric analysis real ized volatility use estimate stochastic volatility model journal royal statistical society series statistical methodology barun ehl combine high frequency data non linear model forecasting energy market volatility expert system application bedowska sojka daily var forecast realize volatility garch model bucci forecasting realize volatility review cho van merri nboer bahdanau bengio property neural machine translation encoder decoder approach arxiv preprint arxiv christoffersen evaluate interval forecast international economic view chung gulcehre cho bengio recurrent neural network sequence model arxiv preprint arxiv clevert unterthiner hochreiter fast accurate deep network learn exponential linear unit elus arxiv preprint arxiv corsi simple approximate long memory model realize volatility journal financial econometrics corsi audrino ren har model realize volatility fore cast corsi fusari la vecchia realizingsmiles optionspricingwith realize volatility journal financial economics corsi reno har volatility model heterogeneous leverage jump available ssrn degiannakis potamia multiple day aheadvalue riskandexpected shortfall forecasting stock index commodity exchange rate inter day versus intra day data international review financial analysis di persio honchar recurrent neural network approach fi nancial forecast google asset international journal mathematics computer simulation diebold mariano compare predictive accuracy journal business economic statistic elman find structure time cognitive science ron hand machine learn scikit learn tensorflow concept tool technique build intelligent system reilly medium inc giot laurent model daily value risk use realize volatility arch type model journal empirical finance goodfellow bengio courville deep learn mit press http www deeplearningbook org graf schmidhuber framewise phoneme classification bidirec tional lstm neural network architecture neural network hochreiter schmidhuber long short term memory neural compu tation jordan serial order parallel distribute approach ic report san diego california institute cognitive science krizhevsky sutskever hinton imagenet classification deep convolutional neural network advance neural information processing system kruse realize volatility improve accuracy value risk forecast leibniz hannover work paper kupiec technique verify accuracy risk measurement model lawrence use neural network forecast stock market price manitoba lippmann introduction compute neural net ieee assp maga zine lipton berkowitz elkan network sequence learn arxiv preprint arxiv liu pantelous von mettenheim forecasting trading high frequency volatility large index quantitative finance wei huang chen well forecasting model comparison har rv multifractality volatility physica statistical chanics application mcculloch pitt logical calculus idea immanent nervous activity bulletin mathematical biophysics mcnally roche caton predict price bitcoin use machine learn parallel distribute network base processing pdp th euromicro international conference ieee merton estimate expect return market exploratory investigation journal financial economics ller dacorogna dav pictet olsen andj ward fractal intrinsic time challenge econometrician unpublished manuscript olsen associate rich rosenblatt perceptron probabilistic model information storage organization brain psychological review rumelhart hinton williams back propagate error nature stentoft option pricing use realize volatility thomakos wang realize volatility future market journal empirical finance trimborn rdle crix index blockchain base curren cies wang leu stock market trend prediction use arima base neural network ieee int conf neural network vol yu li forecasting stock price index volatility lstm deep neural network recent development data science business analytics springer zakamulin real life performance market timing move average time series momentum rule journal asset management zaremba sutskever vinyals recurrent neural network regu larization arxiv preprint arxiv true log return high volatility time predict var na yellow har rv grey fnn har brown srn green lstm orange gru red rvolbackt true log return high volatility time predict var na yellow har rv grey fnn har brown srn green lstm orange gru red rvolbackt true log return low volatility time predict var na yellow har rv grey fnn har brown srn green lstm orange gru red rvolbackt true log return low volatility time predict var na yellow har rv grey fnn har brown srn green lstm orange gru red rvolbackt btc eth ltc str xmr xrp min max mean median std st quartile rd quartile descriptive statistic price six cryptocurrencies rvolprep declaration authorship hereby confirm ivan mitkov author master thesis independently without use others indicate source consult publish work others form idea equation text always explicitly attribute january ivan mitkov hiermit erkl ich ivan mitkov das ich die vorliegende arbeit allein und nur unter ver wendung der aufgef hrten quellen und hilfsmittel angefertigt habe die pr fungsordnung ist mir bekannt ich habe meinem studienfach bisher keine masterarbeit eingereicht bzw diese nicht endg ltig nicht bestanden den januar ivan mitkov