face value company deep learn nonverbal communication master thesis submit prof dr wolfgang karl ardle prof dr cathy yi hsuan chen universit zu school business economics institute statistic econometrics ladislaus von bortkiewicz chair statistic sophie burgard partial fulfillment requirement degree master science november side effect digitalization massive amount unstructured data generate every day unstructured data comprises video speech text image data easy interpret human challenge computer financial research much engage recent history decision make base textual sentiment analysis textual analysis base verbal part communication human interaction face face level nonverbal communication play equally important role support message theinterpretation ofemotionsin major communication deep learn versatile technique use numerous application provide somewhat cognitive capability machine thesis describes build deep convolutional neural network ability detect emotion face different approach deep convolutional model design test evaluate result use evaluate video regular press conference european central bank january september processing step result emotional score facial expression press conference single picture investigate whether information nonverbal communication measure level emotional excitement link movement euro stoxx index face value compare value speech accompany research use image data press conference source unstructured data transfer nonverbal communication stock market topic best found knowledge yet focus upon research keywords convolutional neural network deep learn return financial empirical unstruc tured data multivariate analysis ii content list iv list abbreviation introduction deep learn artificial neural network single neuron activation function network architecture network training error function optimization method back propagation challenge network training image classification convolutional neural network convolutional layer pool layer network fine tune facial expression recognition deep neural network data preparation convolutional neural network fine tune neural network comparison microsoft cognitive service discussion result face value requirement data european central bank press conference estimation conclusion bibliography iii list fully connect three layer neural network information processing single neuron common form nonlinear activation function hidden layer pseudo code adam optimizer back propagation neural network dropout example fer dataset learn curve convolutional neural network heatmap cnn prediction quality learn curve fine tune convolutional neural network heatmap vgg fine tune model prediction quality heatmap microsoft emotion api prediction quality index movement press conference day average emotional score per press conference correlation matrix emotion score return circle correlation partial least square linear discriminant analysis iv list abbreviation ann artificial neural network api application program interface adam adaptive moment estimation cnn convolutional neural network ce cross entropy cet central european time dl deep learn da discriminant analysis ecb european central bank fc fully connect layer fer facial expression recognition gd gradient descent kl kullback leibler divergence mlp multilayer perceptron nn neural network ols ordinary least square pc press conference pca principal component analysis pls partial least square relu rectify linear unit rbg red green blue se square error sgd stochastic gradient descent vgg visual geometry group introduction side effect digitalization massive amount unstructured data generate every day unstructured data comprises image speech text increasingly video data human interpretation data source usually simple task machine challenge computer somewhat human like cognitive capability vision remote future development strongly link deep learn network tesla self drive car able observe surroundings make decision base deep learn helbing ibm watson almost human like capability processing speech feng name example versatile various application technology master apart field robotics deep learn technique make way many scien tificdomains orsentimentanalysis inrecentyears duetothe various possibility incorporate information unstructured data decision make kumar ravi textual analysis base verbal part communication face face set understand interpretation message also highly dependent nonverbal factor nonverbal communication combination body posture facial expression happy sad surprised etc gesture also audible component remainder thesis use recognition facial expression measure nonverbal communication straightforward categorization detection ambition thesis link nonverbal communication observe live broadcast press conference financial measure question research possi ble generate similar signal observe facial expression stock market see sentiment analysis effect consider face value company might beneficial observe nonverbal communication analysis live stream base follow hypothesis facial expression subtle movement harder fake spontaneous word therefore might contain honest level information porter ten brinke interpretation emotion displayed per son face make without knowledge person language speaker also without knowledge topic extent nonverbal communication consider universal ekman friesen facial expression might therefore versatile measure speech paper split empirical analysis two major part first necessary chose deep convolutional neural network design capacity interpret face do compare different design task facial expression recognition different option apply evaluate use suitable data set computer vision automate interpretation face become interest field research kumari appli cation facial expression recognition use marketing evaluate consumer reaction shergill measure joy player computer game zhan second part empirical analysis engage connect facial expression live press conference financial measure paper use regular press conference european central bank president offer good experimental set analysis extensive research focus textual analysis area example rosa investigate importance sentiment central bank communication global stock market volatility moniz de jong use speech sentiment predict interest rate expectation investigation also engage intraday level hussain also measure long term effect speech sentiment stock market schmeling wagner best found knowledge thesis new three aspect video data press confer exceedingthe textual analysis event treat facial expression recognition measure nonverbal communication use deep learn method new approach apply nonverbal communi cation context finance consumer confidence level marketing context yet focus upon research remainder thesis follow section two provide substantial overlook different element need build deep convolutional neural network describe different layer type modern optimization procedure architectural possibility special focus put task image classification section three us kaggle dataset facial expression recognition transfer theoretical model consideration practice three different approach emotion classification apply evaluate section four transfer potential facial expression recognition component nonverbal com munication measure effect stock market live event company institution discuss requirement data consider event single press conference european central bank review different estimation method use detect potential link nonverbal communication ex press facial expression stock market additional material thesis access www quantlet com indicate quantlet symbol deep learn artificial neural network mlp neuralnetworks nn areamathematical attempt recreate information processing process brain general purpose nn mapping input prespecified output feedforward describes one directional acyclic flow information network unlike recurrent cyclic nns feedforward network contain feedback loop net like structure nns arises chain many nonlinear function call layer input one layer output precede layer depth network describes number stack layer network input output nn depth three could write first input layer fourth case output layer inter mediate function hidden layer special property almost universal capability nns arise adaptive weight throughout network structure deep net work commonly network one hidden layer intuitive way visualize nns display form flow graph see fig network eq call three layer nn convention layer adjustable weight count weight input layer per definition fix description theory limited model classification problem since line research question paper nevertheless also regression problem solve use deep neural network single neuron neural network consists different layer layer consists many single unit neu ron one neuron small processing unit network circular structure fig receives input either raw data output another unit transforms nonlinear function pass nonlinear function call activation function final output result score allow make classification decision output input layer layer hiddenlayers fully connect three layer neural network single neuron performs follow step feed forward process see fig first collect input value multiplies respective weight add bias term second nonlinear activation function apply weight sum input factor result pass neuron subsequent layer element scalar vector matrix depend network design adjust weight bias neuron every layer task network training information processing single neuron activation function activation function nonlinear activation function crucial element design nn nonlinearity importance since network consists linear function linear transformation model power therefore restrict linear dependency impractical limitation problem necessary define specific form nonlinear dependency neural network perfectly fit problem neural network least one hidden layer nonlinear activation function universal approximation system universal approximation theorem cybenko hornik state feedforward network linear output least one hidden layer squash example logistic sigmoid activation function approximate borel measurable function one finite dimensional space another desire non zero amount error provide network give enough hidden unit goodfellow proven rectify linear unit leshno common form nonlinear activation function maxout goodfellow since activation function essentialpartsofnns shape functional form also allow efficient training common neural network rectify linear unit hidden unit output layer dependingonthetask sigmoidorsoftmax whichbothderivefrom statistical background follow paragraph discus use different variation common activation function sigmoid function sigmoid function logistic function exp showsa shapedform fig interval use favor early application neural network due analogy interpretation fire rate biological neuron zero signal pass one neuron fire full rate state art procedure optimization process nns use gradient descent method combination back propagation gradient calculation sigmoid function fall favor due follow undesirable property use gradient base optimizers hochreiter vanish gradient problem gradient tail sigmoid function converges quickly zero opti mization procedure update weight direction gradient gradient almost saturate tail optimization weight inefficient even impossible within finite training time non zero center output property induces undesired dynamic weight update process gradient constantly negative positive problem overcome careful choice initial weight start sigmoid function still necessary component nn target training binary classifier use activation function output layer statistically speak apply logisitic regression input come precede layer output sigmoid function directly interpret probability belonging one two possible group different form base sigmoid function hyperbolic tangent tanh stand sigmoid function map real value input range output zero center advantage compare sigmoid function still vanish gradient problem occur serious problem sigmoid rectify linear unit relu common form nonlinear activation function hidden layer softmax activation statistical context softmax activation estimate multiclass logit regression output precede layer multiclass analogy interpretation sigmoid function binary classifier interpretation come exponential distribution detail see duda probability belonging specific class one overall class give exp exp hold time rectify linear unit rectify linear unit commonly shorten relu max become default recommendation activation function modern neural network jarrett relus greatly speed computational time op timization process use stochastic gradient optimization method due strict cut zero fig relus expose danger kick train ingprocess simple way overcome kick neuron either restrict learn rate use slight modification leaky relu activation function small positive slope negative input value cx small positive constant network architecture recent publication neural network regularly use term deep learn techni cally deep learn use neural network describe method deep learn refers use multiple hidden layer within one network old application often use one hidden layer justified universal approximation theorem formal definition neural network deep network shallow structure state art application sometimes use layer network today deep structure old application neural network regularly use one hidden layer unarguably shallow since minimum setup neural network deep cybenko hornik neuralnet work one hidden layer nonlinear activation capable model kind nonlinear functional form theoretical capability represent specific form mean true function also learnable throughout optimization process neural network theory universally capable model function independently network architecture guaranteed sufficient fit found within finite learn process practice also occur realistic chance optimization pro ce decides incorrect functional form example consequence overtraining detailed section moreover theoretically sufficient use single hidden layer order achieve desirable low error rate layer may contain large number neuron practice may impossible optimize deeper model layer allow reduce number unit per layer increase representational power time goodfellow numerous data analytics predictive model competition especially field recognition show past decade practical relevance high performance deep structure schmidhuber self fulfil mechanism building deeper model increase performance show ba caruana certain circum stance even shallow one layer network perform competitively modern application field recognition task adversely recent publication urban state convolutional deep structure generally replicate shallow structure therefore matter depth deep learn network still discussion network training goal network training find set model weight minimizes desire error function give problem training problem consists two component find suitable error function apply suitable optimization algorithm perform minimization error function error function measure goodness network fit goal find set network weight minimizes function minimize equal minimize number misclassifications therefore maximize network prediction accuracy cross distribution estimate data follow silva estimate probability observation belonging class real probability come underlie true distribution sample consists different non overlap class know real underlie distribution probability realize complete sample assume stochastical indepence observation decompose pt pt ptk accordingly observe sample probability yt yt ytk number observation assign class interest best approximation sample density eq true density eq achieve minimize kullback leibler divergence kl kl divergence measure distance two distribution kl log pt pt ptc log yt yt ytc log log true model probability unknown independent model weight fore equivalent minimize log ce binary cross entropy special case multiclass cross entropy number class log log ce optimization method success deep learn past decade overcome two major bottleneck network training advance optimization algorithm vast supply inexpensive computational power follow section discus different gradient base optimization method gradient descent rumelhartetal already consider gradient descent gd training optimization procedure batch gd optimizer iterative method update network weight base gradient error function iteration model weight move direction gradient process repeat complete training data whole batch parameter call learn rate fix step size weight update choice crucial small convergence may take long large might convergence achieve algorithm settle local global minimum error function duda another shortfall gradient descent method large datasets iteration may computational intense many redundant calculation step stochastic gradient descent sgd line version gradient descent method call stochastic incremental gradient descent practical use large datasets bishop lecun overall error function set independent observa tions sum individual error data point sgd update weight consider one single observation time weight update repeat accord new observation next update step need chosen randomly dataset otherwise algorithm also learns arrange ment input data stochastic gradient descent usually unsteady jump lot convergence process since weight update high variance also advantage since random behavior allows overcome local minimum also possible use kind intermediate procedure update weight one observation whole training data procedure call mini batch sgd commonly use combine benefit extreme parameter update low variance sgd efficient gd still less likely get stuck local extreme dependent training set size specific problem common choice mini batch size lie practice ruder adam optimizer adam optimizer kingma ba state art gradient base optimizer ruder adam method choice due favorable behavior practice adam adaptive moment estimation computes adaptive learn rate parameter additionally decay average past square gradient average past gradi ents consider optimizer combine advantage advanced optimizers adadelta zeiler momentum method qian adam optimizer us parameter update information gather first also second moment certain hyperparameters require optimization process learn rate step size weight update exponential decay first moment exponential decay second moment small positive constant prevent division zero procedure adam optimizer described kingma ba pseudo code fig iteration algorithm considers different random mini batch dataset algorithm adam optimizer require hyperparameters require stochastic objective function parameter require initial parameter vector initialize first moment vector initialize second moment vector initialize time step converge calculate gradient update st moment bias update nd moment bias bias correction st moment bias correction nd moment parameter update end return pseudo code adam optimizer representation base kingma ba default value parameter show good behavior practice give kingma ba back propagation produce desire estimation output information pass strictly forward neural network understood complex chain single operation stream start input layer passing hidden unit end calcula tion network error function call forward propagation optimization procedure described section gradient base method meaning take information error function gradient update model weight process calculate gradi entiscalledback propagation since inanalogytoforward propagation function flow backward network error back propagation understood apply chain rule calculus several time every node network way gradient complex structure like deep neural network feasible calculate number small sub problem example back propagation arbitrary activation function result back propagation equal chain rule calculus representation base rojas accord goodfellow back propagation often misunterstood whole learn algorithm artificial neural network fact method compute gradient actual learn perfomed use optimization algorithm described section numerical evaluation gradient one main computational problem network training even might analytically rather simple method much old limited neural network efficient error back propagation context multilayer perceptrons gradient descent make popular rumelhart back graph computational graph break mathematical expression series subordinate problem treat independently back propagation process gradient every point network determine apply chain rule calculus proof see rojas neuron technically function two direction forward pas applies nonlinear activation function backward pas first derivative activation function use forward pas derivative input specific node also calculate store backward pas calculates gradient whole network do apply chain rule calculus assume simple case two layer network one node layer see fig illustration challenge network training overfitting state universal approximation theorem see chapter kind functional form model neural network sufficient size practice model training process often tackle problem limited availability training data deep network large layer lead problem network highly affected sample noise consequently model adapt dataset high capability describe give observation show poor result sample prediction new unseen data problem call overfitting overtraining see detail tetko intuitive way reduce overfitting increase number training data depend specific application network always feasible approach output layer input layer hiddenlayers network apply dropout cross node deactivate dropout one method brought hinton add dropout network layer time fig prevents learn specific structure property sample data underlie true process reduces number overall neuron network np neuron sample probability way several random network thinner structure estimate test process overall effect node calculate average weight yield full network node compare regularization method dropout show im prove performance come computational efficiency predicitive power many benchmark datasets machine learn problem srivastava regularization randomize element network overfitting problem reduce penalize size model big model general well ability model functional relation prone model spurious relation reg ularization method penalize big network size add additional parameter error function possible method regularization method latter commonly use practice nielsen weight decay regularization extends error func tion penalty factor equal sum square adjustable weight withoutthebiases scaledbyafactor sample size size determines strength penalization model size weight size parameter hyperparamter free choice depend specific dataset network target small closer regularization tends unre stricted error function small model weight desirable network classification decision consideration supposedly robust sample prediction less de pendent see training data case categorial cross entropy eq optimization problem change log ce regularization elastic net regulization add term error function despite similar intuition penalize network size size weight regularization show difference behavior regularization lead weight vector diffuse small weight regularization produce vector base main feature push weight zero consequently make weight vector sparse result nielsen image classification convolutional neural network ambition thesis building neural network classifies picture fix number class discuss method general brick building neural net work use task problem data fed network one dimensional vector number transform picture human eye see machine readable input color pixel convert binary representation picture arrange three dimension width height depth width height refer pure greyscale picture depth one convert use color black represent number white represent hold bit per pixel color depthtables thered green blue rgb spectrals picture treat separately give representational matrix depth three pixel rgb layer possible value range standard version network fig every neuron connect sub sequent layer call fully connect layer different purpose data structure different layer design chosen fully connect design raw data give neural network one dimensional vector nature picture two dimensional grid like pixel height width possible convert two dimensional picture one dimensional vector simply stack row column take significant information raw data convolutional network use among already mention fully connect layer two special kind layer convolutional layer pool layer idea convolution neural net work initially call neocognitron first mention fukushima inspire model hubel wiesel simple complex cell cat visual primary cortex simple cell pas signal detect simple pattern complex cell rather schmidhuber layer complex cell similar function like pool layer since training cnns use gpus standard allows faster training krizhevskyetal cnnsarecomponent almost winner international image classification competition imagenet large scale visual recognition challenge often refer benchmark state art image classification convolutional layer motivation use convolutional layer structure picture shall ex ploited information processing intuitively pixel close image high relevance recognition complete object spatially close specific structure built number pixel call feature determine local feature small excerpt picture merge high level layer finally whole picture identifiable thereby network go generic form pattern almost independent network scope feature later layer highly dependent use picture objective dataset convolutional layer characterize follow hyperparameters filter size receptive field usually size chosen stride filter move along feature map integer value zero pad optional allows control output volume size pad input volume zero filter depth confuse color depth number filter use convolutional layer fix size square input volume input layer determine size pixel picture output size first convolution dimension necessary chose parameter way result integer value filter depth chosen freely weight share besides spatial consideration input matrix convolutional layer impose special restriction number adjustable weight unlike fully connect layer filter us weight throughout complete picture lead flashlight like structure procedure base assumption one filter extract useful feature position picture filter also useful position prof also practice true since structure found filter almost universally generic look distinct edge line krizhevsky consider follow example illustrate effect number weight input volumev hasdimensions convolutional filter depth output volume convolution accord eq single neuron weight weight share apply first convolutional layer weight plus bias need optimize throughout training process rise every layer deep structure common modern convolutional network number become extremely high apply weight share weight layer reduces number parameter plus number bias parameter share also cause equivariance translation within convolutional layer equivariance hold shift input transformation rotation scale equivariant convolutional function goodfellow pool layer desire classification output usually much small dimension size input volume pool layer achieve stepwise reduction matrix dimension convolutionallayers amongother choice one common option max pool input volume scan depth layer wise spatial extent apply max function convolutional layer stride chosen freely constraint result dimension output volume integer calculation output size analogously filter depth remains unchanged throughout operation commonly apply lead overlap pool procedure reduces probability overfit krizhevsky large spatial extent receptive field pool layer act restrictively model capability pool layer make representation approximately invariant small input translation desirable property image processing since majority feature big interest whether feature present image precise position goodfellow network fine tune specific design convolutional neural network consist numerous free choice rameters optimal design reasonable often feasible due computational complexity different approach apply transfer learn transfer learn take knowl edge already gather one task apply another target therefore intuitively close human learn process largely base previous experience widely differ circumstance transfer learn help three way first model start high initial knowledge make well prediction right begin second learn time decrease third final level accuracy high learn torrey shavlik achieve training base network original target fix first layer training classifier top slight variation fine tune error weight fix back propagation also adjust new learn objective method use depends size kind data small dataset large base model significant risk fitting fine tune therefore freeze layer optimization process prefer yosinski look neural network feature need go general low level network highly specific learn task last layer network found generic low level filter convolutional layer almost independent training objective model specification yosinski krizhevsky therefore also use task facial expression recognition ambition thesis measure nonverbal communication evaluate effect finan cial data live event quantification nonverbal communication achieve several emotion section different model design discuss train evaluate suitable dataset deep neural network subsection deep neural network built train purpose recogniz ing facial expression deep neural network need large amount classify data adjust weight training process fit specific problem use dataset take kaggle competition challenge representation learn facial expression recogni tion challenge kaggle platform machine learn competition data competition provide company researcher fer aaron courville use google image search api respective keywords picture face rectangle set automatically software label face rectangle approve human supervisor reject misspecified picture picture cropped along face rectangle convert grayscale square cut shape pixel see goodfellow information dataset generation base small scale experiment goodfellow determine accuracy human solve classification task give dataset final fer dataset contains image follow emotion count appear ance dataset original encode within dataset anger image disgust image fear image happiness image sadness image surprise image neutral image dataset oversampled class happiness disgust underrepresented class roughly evenly size fer contains wide variability age look people fig show random sample example image picture seemingly show act emotion others appear real snapshot also face painting dataset picture show occlusion like glass hat hand cover part face watermark right owner picture head pose vary frontal portrait picture side face shot adjust along vertical axis moreover illumination single image varies heavily throughout dataset data preparation original fer dataset consists csv file contain vectorized bit grayscale version image correspond label pixel vector dimension one row represent one picture use kind neural network common standardize value input range since input matrix value interval achieve divide value within input matrix standardization context picture manipulation represent assimilation illumination deep learn input matrix often also normalize divide element standard deviation zero center subtract mean normalization do data preparation step repeatedly perform within several batch normalization layer throughout network gridstructure pixel therefore necessary arrange vector dimension matrix dimension last dimension equal color scale grayscale dataset throughout paper color last notation use standard notation theano library python emotion label encode integer zero six anger disgust fear happiness sadness surprise neutral use categorical cross entropy error metric training necessary define label one scheme one hot encode one hot encode represent label series single high rest random example fer dataset one emotion per row top bottom anger disgust fear happiness sadness surprise neutral class represent zero example label class fear one hot encode label class neutral one hot encode etc convolutional neural network subsection discus different model option two different approach first model follow general design image classification hyperparameters chosen base random initialization use common optimization method second fine tune approach base vgg face network apply fer data vgg face network deep neural network structure purpose general face recognition fine tune compare microsoft emotion api microsoft service task model architecture problem facial expression recognition focus upon research different method tian manglik lawrence follow analysis focus estimation use one single convolutional neural network pramerdorfer kampel compare different state art procedure facial expression recognition base research model design developed yu zhang implement combine simple structure good performance chosen net work hidden layer cnn displayed tab determine depth network layer adjustable weight count case convolutional layer fully connect dense layer count layer input layer max pool dropout python chollet others use theano rfou backend kera modular fast implement api especially deep neural network theano performs efficiently numerical computation size input layer determine shape input image fer data fix pixel size one color channel depth one since picture grayscale size convolutional filter receptive field move stride one large stride reduces size input matrix faster since pixel consider already small size input matrix fast reduction undesirable small stride also ensures representation invariant small input translation first two convolutional layer depth increase later convolutional layer network layer filter depth train detect certain feature within layer type output dimension filter stride parameter input convolution convolution max pool convolution convolution max pool flatten dropout dense dropout dense dense specification cnn fer data set fvcconvnet picture rectify linear unit chosen activation function final class prediction achieve use softmax activation train convolutional neural network consists trainable parameter summarizes weight bias model tab show number parameter per layer low number parameter low level network compare fully connect layer due weight share restriction convolutional layer pool layer use max function receptive field stride reduces input size accord eq approximately half pool layer due small format input format pool layer use every two convolutional layer pool layer follow batch normalization layer ioffe szegedy speed computation estimation efficiency since zero center desire property reduce risk overfitting convolution pool procedure neuron flatten structure vector afterwards two fully connect layer add every neuron two layer connect use dropout reduce risk learn structure sample specific optimization adam algorithm apply version stochastic gradient optimization parameter value learn rate chosen propose original paper kingma ba initial value model weight randomly chosen along convention neural network training dataset size batch size use optimization process repeat iteration categorical cross entropy determines accuracy loss prediction many different specification model architecture hyperparameter choice try either effect worsen model predictive power basic convnet training set acc test set acc ycarucca noitciderp iteration learn curve convolutional neural network tab fvcperformance estimation result fig show prediction accuracy repetition model rise accuracy already first epoch within iteration reach maximum prediction power test data remains level meaning model classifies never see image correctly one seven exist class hand accuracy within training set use optimize weight accuracy converges grow number iteration fig allows evaluate choice learn rate seem accurate range accuracy rise almost good indication sufficient model depth specific problem network high precision recognize emotion happy neutral disgust see fig number main diagonal fig understood positive predictive value ppv tp ppv tp fp ppv equal ratio true positive tp observation class predict class member true positive plus false positive fp observation class pre dicted belong class sum observation predict member class resembles model precision per class diagonal show consequently false recovery rate proportion misclassified observation prediction per class yu zhang use dropout layer start convolution left implement model since show bottleneck model prediction capacity without dropput first stage increase accuracy approximately achieve result may reasonable since first stage dropout harsh crop picture reduces size half original model also us stochastic pool layer max pool layer use since stochastic pool implement kera cnn anger disgust fear happiness sadness surprise neutrality anger disgust fear happiness sadness surprise neutrality prediction quality cnn diagonal positive predictive value diagonal false discovery rate fvcheatmap fine tune neural network model architecture next training neural network zero fine tune approach popular apply deep learn practice convolutional neural network show generic become specific depth network fine tune us deep precisely train network generic data adjusts last specific layer new dataset deep network include weight train long period multi gpu computer publish deep learn enthusiast researcher public use even though specific target base model major importance deep network use specifically train processing face model description weight publish non commercial use visual geometry group vgg oxford vgg face cnn parkhi base deep structure vgg weight train specifically face huang wolf background vgg face project label facial datasets need training deep network sparse free public use time big commercial company facebook google already possession large data variety restrict public due privacy restriction also serve competitive resource parkhi combine different datasets achieve large facial dataset unique picture layer cnn set face recognition problem consider deep structure main part vgg face cnn model built section convolutional filter filter size rectify linear unit nonlinear acitvation function small stride size guarantee high level feature invariance convolutional layer follow maxpooling layer model end set fine tune network specific question convolutional layer kept two fully connect layer hidden unit adjust fer dataset use rectify linear unit activation function softmax layer class prediction consider original data vgg face model dimension single observation fer dataset dimension first two dimension adapt zero pad third dimension color depth fer dataset grayscale color depth one vgg face dataset color therefore depth three red green blue color depth three simulated simply concatenate identical matrix three time third dimension imitates red green blue representation grayscale picture three channel identical estimation result despite state art procedure apply deep structure clas sification problem limited data different structure problematic fine tune approach show significant bad result predictive power neural network shallow structure random initialization parameter section themodel convergence overall slow seem get stuck plateau begin iteration final predictive power approximately reach fig remains last iteration almost less competitive model model analysis base confusion matrix fig exhibit model severe prediction problem prediction accuracy happiness surprise satisfactory generally fine tune vgg face model overpredicts neutrality dominant emotion layer type output dimension filter stride parameter input convolution convolution max pool convolution convolution max pool convolution convolution convolution max pool convolution convolution convolution max pool convolution convolution convolution max pool flatten dense dense dense specification vgg model fer dataset fvcfinetuning able detect emotion follow fine tune method could adjust well give task possible model would gain significant predictive power would train longer period also model training set accuracy remains around may indication model would need iteration improve performance another reason might suboptimal choice learn rate may cause plateau behavior begin finetuning vggface training set acc test set acc ycarucca noitciderp iteration learn curve convolutional neural network tab horizontal line reference performance model tab fvcperformance finetuning anger disgust fear happiness sadness surprise neutrality anger disgust fear happiness sadness surprise neutrality tunedvgg model diagonal diagonal false discovery rate fvcheatmap comparison microsoft cognitive service follow subsection compare performance microsoft emotion api previ dataset corporation us machine learn algorithm offer easily accessible integratable artificial intelligence service speech recognition sentiment analysis face recognition also facial expression recognition emotion api service emotion classification picture paper python use access api fvccall api microsoft service black box model detailed information offer behind lie chine learn technology emotion label emotion api fer dataset differ slightly emotion anger contempt disgust fear happiness neutrality sadness surprise set seven emotion fer extend additional emotion contempt label test data previous model use determine network predictive power evaluation make base test fraction fer first comparison label data original dataset versus predict emotion yield accuracy api anger disgust fear happiness sadness surprise neutrality anger disgust fear happiness sadness surprise neutrality diagonal diagonal false discovery rate fvcheatmap compare previous model significantly bad investigate reason bad accuracy show emotion api unable detect face rectangular request picture due occlusion like hat watermark glass invalid clip inaccurate picture since size picture pixel already small compare real world application picture become uninterpretable could also explain inferior result fine tune method use vggface base model section problem different number class neglect analysis picture face classify show contempt test dataset clean picture interpretable face emotion api process rerun fine tune model section show accuracy slightly small dataset improvement basic cnn model random parameter initialization section improves accuracy expect emotion api improves significantly picture withdetectablefaces random parameter initialization section competitive prediction accuracy discussion result human accuracy solve task accord goodfellow interpretation emotion static image also human complicate task interpretation facial expression often base background situation moreover number seven emotion class final selection also many emotion could interpret belonging one class assume impossible set ting also misclassifications dataset bad data quality fully exclude picture may simple uninterpretable hypothesis also back use microsoft emotion api significant amount picture classify since face rectangle found algorithm reason bad performance fine tune approach might structure inappropriate give data quality look model prediction precision fig reveals class seem almost randomly assign large number mi classify observation additionally potential problem dataset described previous section lead conclusion fine tune model probably inac curately specify give problem picture within original dataset already rather small size pixel stepwise dimension reduction layer fine tune approach require filter step sufficiently small run complete network case filter size convolutional layer throughout network mainly size see tab inhibits useful desire feature translation invariance convolutional layer may able detect large meaningful feature improvement estimation result could achieve two way either increase amount training data improve model architecture modern dl method dataset size image fer example consider relatively small one way increase amount training data use pattern permutation method give label data pattern permutation method include mirror turn distortion random crop simard advantage method whole process run completely automatically machine label permutation already know dl library python kera offer implement procedure permutation second approach extend data use online image search respective search term need human attention selection process could result well picture quality increase variability side potential model improvement use ensemble model often use practice model ensemble estimate several different model vary size design combine result idea different model may capture different aspect data task facial expression recognition model combination allow state art prediction accuracy slightly pramerdorfer kampel face value follow section us deep learn technique previous chapter interpret facial expression live stream presentation host company official institution analyzes respective effect stock market underlie hypothesis nonver bal communication might indicator value information present honest general textual analysis potential value derive facial expression denote face value contrast value speech word textual analysis due new nature topic analysis rather exploratory character today best found knowledge research do link biometrical facial expression data finance facial expression classifier previous section serve face measure automatically quantifies change person expression microsoft emotion api show comparable result reliable estimation procedure give task since self built train model restriction small input size must exactly pixel follow analysis make use emotion api order get emotional score emotion api self selects face rectangle within picture reduces amount time data preparation image manipulation massively requirement data early idea measure company face value link company live event real time stock price movement first investigation two different live stream video use emotional evaluation compare movement stock price tick data use order achieve high enough frequency important since change facial expression happen fraction second one chosen example stage analysis jahresgepr ach volkswagen ag th may achieve less continuous stream emotional score throughout press event second video five screen shot generate sent microsoft emotion api fvccall api return json output generate con taining emotion score eight basic emotion anger disgust contempt fear happiness neutrality sadness surprise since microsoft api black box procedure assume last stage image processing procedure do multinomial logit model softmax layer final emotion score emotion interpret proba bilities back property score eight emotion sum one picture throughout course press conference substantial variation emotional score could observe image sent api probable emotion predict neutral probability emotion range less interval fvcemo stock analysis correlation correlation show weak tendency price vw stock move emotional score positive happiness negative fear disgust others unclear evaluation positive correlation happiness negative surprise repeat procedure different example one apple keynote presenta tions yield different result ered press conference instantaneous real time influence press release support literature deeper research potential time lag analysis neglect due follow reason investigate face value generates two requirement data first necessary speech exists video otherwise information facial information obtain second event need news value news value mean moment therefore power affect stock price movement third hypothesis require informational value important also attracts real time large enough attention investor event otherwise reason video data stock movement make therefore event live broad cast tv livestream online consider begin analysis company live event speech tim cook ceo apple inc apple developer conference use another example press confer ence volkswagen present future strategy apple event attract lot public attention generally say press conference live stream moreover possible say direct link stock price movement today best knowledge explicit research do direction company live event investor reaction miss connection stock market link miss news value since kind event mainly advertisement part company public relation strategy contain enough value investor information relevant investor still rather base textual information event possible build large enough sample test kind hypothesis example apple host two three event variety topic per year major company even irregular company outside trading hour measurement instantaneous effect impossible building data set different company different event potential introduce unwanted uncontrollable noise since point definition might face value vague research direction rely use private company event reject european central bank press conference european central bank ecb offer publicly available major speech since write form since several year use press conference regular basis impor tant tool communication one important regular meeting ecb govern council responsible among others set key interest rate eurozone govern council meet since every week thursday approximately every four week variation weekday follow fix structure cet press release publish main result follow shortly cet ecb president currently mario draghi explains council decision live stream press conference pc follow question answer session tend journalist since decision announce trading hour european country sum information affect stock market example interest bond future still day pc hypothesis press release ecb significant influ ence stock market support schmeling wagner rosa hussain even across border europe hayo fig illustrates effect ecb council meeting press conference dax return peak right press release pc visible large volatility rest trading day ytilitalov egareva time cet solid press announcement dash vertical line council press release cet begin approximate end ecb pc cet fvcpressdays available pc available webcasts youtube com use follow analysis webcasts available since january september time thesis late available one make total unique pc since old webcasts cover part president statement begin pc part use every conference introductory statement cover minute impor tant result explanation introductory video unlike question answer session shot frontal close president face therefore predestine automate facial expression recognition reason feasibility two screen shot per second make analyze via microsoft cognitive service shallow analysis high frequency screen shot seem highly affect result average score overall picture analyze fig show aggregate average emo tion per pc january september neutrality left since overwhelmingly classify emotion score picture bar conference would add one fig also show measure person face facial expression dependent individual trait november presidency ecb change jean claude trichet mario draghi without know fact break visible average emotion score average level emotion per ecb press conference average aggregate emotional score ecb pc jan sep fvcbarplot estimation order determine value nonverbal communication ecb press conference compare study focus tonatility ecb press conference schmeling wagner use negativity speech derive financial dictionary find positive negative tone ecb press conference associate increase decrease face value nonverbal communication value form without assume specific model represent somewhat relationship emotion return day press conference emotion kind signal derive facial expression estimation parameter constant regression parameter general error term return index data euro stoxx euro stoxx consists large company within eurozone therefore chosen stoxx immediately affected monetary decision hussain find spillover effect ii iii iv vi vii viii ix const lag return anger contempt disgust fear happiness sadness surprise neutral adj result ols estimation emotional score return eurostoxx pc day siginificance level respectively fvceurostoxxols ecb announcement country like switzerland directly affected ecb de comparison sentiment analysis verbal communication use facial expression non verbal communication specific difficulty measure sentiment communication directly related content speech meaning much clear word human direction sentiment move moreover intuitive stand relationship exists negative price movement coincident negative sentiment use facial expression nonverbal communication lead problem relationship rather indirect make detour clear emotion might potentially indirectly affect price direction one main consideration signal facial expression construct capture price movement ordinary least square several model specification estimate detect potential influence emotional score end day return euro stoxx day ecb hold press conference use ordinary least square estimation fvceurostoxxols reference model model esti addingtheemotional score onebyone scoresequalzero tab show result different ols regression equation lag return parameter ret sad con hap neu ang dis fea sur correlation matrix emotion score return eursostoxx pc day fvccorr significant model emotional score always insignificant meaning relationship assume feasible estimate model contain emotional score since sum one observation lead problem perfect collinearity due property score sum one model ix interpret overall importance emotionality throughout press conference fig show correlation emotion score respective return day noreliablelinear connection return make partial least square due high correlation within different emotion score partial least square pls estimation perform partial least square use find essential relation two matrix wold idea find independent component fulfill follow relationship simultaneously zp zd xisthe data theemotionscores isthe dependent variable return euro stoxx index application matrix loading vector contains regression coefficient respective error term pls related principal component analysis principal component analysis pca component chosen maximize representational power covariance matrix without take dependence account pls covariance matrix decompose way component chosen represent data matrix order maximize representational power towards estimation fvceurostoxxpls ischosenbycross validation set four fig show circle correlation first two component pls estimation substantial part variation score fear surprise disgust anger explain first two component happiness variation weakly rep resent first two component main question interest pls much canbecaptured inthiscase euro stoxx pc day fig show explanatory power pls rather low calculate model overall yield value approximately component pls calculate base data model also evaluate therefore result interpret relationship detectable randomization split training test set yield result training test split result even low also due relatively small sample size discriminant analysis schmeling wagner show pc day negative change sentiment coinci dent systematically negative cumulative return day vice versa positive change discriminant analysis da perform emotional score data find poten cluster necessary prepocessing step separate positive negative return pc day two group binary encode negative return pc day positive return pc day sample consists day observation belong group observation belong group circle correlation pls estimation dependent variable fvccorrcircle linear da base bayes rule perform ardle simar distributional assumption make fvceurostoxxlda analyze result pre diction show due representation positive return false discovery rate group one high recall rate rise return low despite acceptable accuracy correct classification predictive value discriminant analysis highly doubt also support fig show histogram density arise analysis substantial difference result density visible group group group pc group pc day positive return fvceurostoxxlda conclusion broader topic thesis measure effect nonverbal communication financial data effect denote face value company nonverbal communication mea sured term classify image face several basic emotion several model built compare base kaggle dataset best single convolutional neural network accuracy correctly classify approximately image one seven class result comparable use microsoft cognitive service emotion api task measurement face value two major problem first valuable data scarce second since today research go direction unclear signal stock price generate emotion score could look like first problem overcome use data european central bank whose regular press conference pro vide good data base second problem treat use several different method ordinary least square partial least square discriminant analysis method chosen base current research effect speech sentiment ecb press conference stock market compare result proven effect speech sentiment lead result use method approach face value company detect side result research facial expression recognition use described method sensitive personal facial trait hinder generalization different example since main problem detect meaningful signal person face generate machine intelligence research could do access nonverbal communication lot potential direct comparison natural language processing example face use facial expression express emotion one way build metric base human face approach might consider example measure speaker pupil movement substantial knowledge range application could broaden take time lag signal financial response consideration also show research natural language processing effect limited stock price could extend financial statistical measure consider interest rate bond might special interest context ecb volatility bibliography rfou alain almahairi angermueller bahdanau ballas zhang theano python framework fast computation mathematical ex pressions arxiv print ab ba caruana deep net really need deep advance neural information processing system page bishop pattern recognition machine learn volume springer chollet others kera http github com fchollet kera cybenko approximation superposition sigmoidal function mathematics control signal system duda hart stork pattern classification volume wiley interscience ekman andfriesen journal personality social psychology feng xiang glass wang zhou apply deep learn answer selection study open task arxiv preprint arxiv fukushima neocognitron self organize neural network model mechanism pattern recognition unaffected shift position biological cybernetics goodfellow bengio courville deep learn mit press goodfellow erhan luc carrier courville mirza hamner bengio challenge representation learn report three machine learn contest neural network goodfellow warde farley mirza courville bengio maxout network arxiv preprint arxiv ardle simar apply multivariate statistical analysis springer heidelberg heidelberg edition hayo kutan neuenkirch impact central bank commu nication european pacific equity market economics letter zhang ren sun deep residual learn image recognition ieee conference computer vision pattern recognition cvpr page helbing societal economic ethical legal challenge digital revolution ssrn electronic journal hinton srivastava krizhevsky sutskever salakhutdinov improve neural network prevent co adaptation feature detector arxiv preprint arxiv page hochreiter bengio frasconi andschmidhuber net difficulty learn longterm dependency field guide dynamical recurrent network page john wiley son hornik approximation capability multilayer feedforward network neural network huang ramesh berg learn miller label face wild database study face recognition unconstrained environment massachusetts amherst technical report hubel wiesel receptive field binocular interaction functional architecture cat visual cortex journal physiology hussain simultaneous monetary policy announcement international stock market response intraday analysis journal banking finance ioffe szegedy batch normalization accelerate deep network training reduce internal covariate shift international conference machine learn page jarrett kavukcuoglu ranzato lecun best multi stage architecture object recognition proceeding ieee international conference computer vision page kingma ba adam method stochastic optimization arxiv preprint arxiv krizhevsky sutskever hinton imagenet classification deep convolutional neural network advance neural information processing system page kumar ravi survey application text mining financial domain knowledge base system kumari rajesh andpooja asurvey procedia computer science lawrence giles ah chung tsoi back face recognition convolutional neural network approach ieee transaction neural network lecun boser denker henderson howard hubbard jackel neural computation leshno lin pinkus schocken multilayer feed forward net work nonpolynomial activation function approximate function neural network manglik misra prashant andmaringanti ieee international conference system man cybernetics moniz de jong predict impact central bank communication financial market investor interest rate expectation european semantic web conference page springer nielsen neural network deep learn determination press usa parkhi vedaldi zisserman deep face recognition procedings british machine vision conference page vol porter ten brinke reading lie identify conceal falsify emotion universal facial expression psychological science pramerdorfer kampel facial expression recognition use convolutional neural network state art arxiv preprint arxiv qian momentum term gradient descent learn algorithm mo mentum term gradient descent neural network official journal interna tional neural network society rojas backpropagation algorithm neural network chapter page springer heidelberg rosa word shake trader stock market reaction central bank com munication real time journal empirical finance ruder overview gradient descent optimization algorithm arxiv preprint arxiv rumelhart hinton williams learn internal representation error propagation technical report california univ san diego la jolla inst cognitive science schmeling wagner central bank tone move asset price ssrn electronic journal schmidhuber deep learn neural network overview neural network shergill diegel sarrafzadeh shekar computerize sale sistants application computer technology measure consumer interest conceptual framework journal electronic commerce research silva marque de alexandre data classification multilayer perceptrons use generalize error function neural network simard steinkraus andplatt apply visual document analysis seventh international conference document analysis recognition proceeding srivastava hinton krizhevsky sutskever andsalakhutdinov dropout simple way prevent neural network overfitting journal machine learn research tetko livingstone luik neural network study comparison overfitting overtraining journal chemical information computer science tian kanade cohn facial expression recognition handbook face recognition page springer london london torrey andshavlik transferlearning handbook research machine learn application trend algorithm method technique urban geras kahou aslan wang caruana mohamed philipose richardson deep convolutional net really need deep convolutional arxiv preprint arxiv wold partial least square encyclopedia statistical science john wiley son inc wolf hassner maoz face recognition unconstrained video match background similarity proceeding ieee computer society conference computer vision pattern recognition page ieee yosinski clune bengio lipson transferable feature deep neural network advance neural information processing system proceeding nip yu zhang image base static facial expression recognition multi ple deep network learn proceeding acm international conference multimodal interaction page zeiler adadelta adaptive learn rate method arxiv preprint arxiv zhan li ogunbona safaei real time facial expression recog nition system online game international journal computer game technology declaration authorship hereby certify thesis submit entirely original work except otherwise indicate aware regulation concern plagiarism include regulation concern disciplinary action may result plagiarism use work author form properly acknowledge point use november sophie burgard