deep neural network cryptocurrencies price prediction bruno spilak thesis submit degree master science prof dr wolfgang karl ardle reviewer prof dr stefan lessmann lead may content deep neural network deep feedforward network nonlinear neuron multilayer perceptron back propagation recurrent neural network architecture back propagation recurrent neural network problem long term dependency long short term memory network lstm architecture back propagation lstm network learn neural network statistical learn maximum likelihood estimation loss function optimization neural network gradient descent adaptive optimization algorithm batch normalization generalization method model selection regularization technique neural network time series forecasting cryptocurrencies nancial asset stock price underlie assumption time series modelization nlarx model cryptocurrencies price movement analyze data analysis application example btc return application cryptocurrency portfolio data analysis cryptocurrencies price technical indicator fundamental indicator model architecture data preprocessing content baseline model best model selection optimization method tune hyper parameter evaluation nal model evaluation di erent trading strategy buy hold strategy long short strategy discussion future work discussion method data model architecture discussion result towards real portfolio management strategy model metric loss function risk management reinforcement learn conclusion cryptocurrencies cryptocurrencies price cryptocurrencies quarterly return model architecture long short portfolio price prediction one main challenge quantitative nance paper present neural network framework provide deep machine learn solution price prediction problem framework realize three instant multilayer perceptron mlp simple recurrent neural network rnn long short term memory lstm learn long dependency describe theory neural network deep learn order able build reproducible method application cryptocurrency market since price prediction use order make nancial decision trade signal com pare di erent approach prediction problem explore supervise learn method classi cation task study model predict sample price direction height major cryptocurrencies roll window regression method goal build classi cation problem predicts price cryptocurrency increase decrease considerably asabasisforthree webuilddi base long long short position build prediction compare performance trimbornand ardle cryptocurrencies bitcoin famous electronic money base blockchain technology use decentralize alternative currency thanks numerous application cryptocurrency market experienced expo nential growth compare di erent weight portfolio test investor bene fundamental indicator market capitalization nd lstm best accuracy predict directional movement important cryptocur rstquartersof keywords deep learn multilayer perceptron recurrent neural network long short term memory network cryptocurrencies crix acknowledgment would like express gratitude supervisor professor ardle continuous help thesis support help understand cryptocurrency market essential research would also like thank second supervisor professor lessmann introduce machine learn neural network help forecasting theory besides advisor would like thank simon trimborn provide crix data crucial thesis victor cluzel help latex alla petukhina give advice quantlet format faruk musta camil humajick support fortheircomments remark privatissimum seminar ladislaus von bortkiewicz chair statistic universit introduction ondecember th year sell dollar rapid growth predictable stock price forecasting one important task quantitative nance indeed pro guide force behind investment choice stock market investor need know appropriate time buy sell stock order maximize investment return however major discussion topic finance cient market hypothesis state price prediction useless pro maximization attempt give de nite answer however appearance behavioral finance many nancial economist believe stock price whichreinvigorate fundamental particularly technical analysis tool price prediction deep learn model particularly deep feedforward neural network already found numerous application quantitative nance volatility forecasting supervise learningscheme suchasarima extension moreover deep learn architecture catch pattern important generalization power recent lstm network seem appropriate sequential data time series nevertheless deep learn frequently criticize lack fundamental theory could crack open black box thesis explain neural network theory investigate lstm network outperform term prediction accuracy former deep neural network architecture mlp rnn cryptocurrency market recently spark interest new investor nancial market order ect trading decision show supervise learn deep neural network successfully predicts price movement within classi cation task build simple investment strategy base long term portfolio comprise important cryptocurrencies crix cryptocurrency index trimborn ardle result may use basis trading strategy rst overview deep mlp rnn lstm model explain basic concept neural network element architecture explain deep learn method neural network finally present application model cryptocurrency market building simple trading strategy chapter deep neural network chapter discus di erent architecture deep neural network explain corresponds di erent representation nonlinear function static relation thanks deep feedforward network section dynamic process thanks recurrent neural network see section necessarily deep architecture explain also model built de ning element discuss essentially notion neuron activation function deep feedforward network deep feedforward network also call feedforward network multilayer perceptron mlp extreme importance machine learn form basis extend neural network architecture many commercial application idea behind feedforward neural network non linear system goal ap proximate function neural network map input output function approximation goal learn parameter mapping get best approximation target output call feedforward information ow unidirectional input thefunction andtotheoutput therearenocycle feedbackconnection wheretheoutput fed back network network cycle call recurrent network present section call network see model direct graph whose node grouped several level call layer example show feedforward network one input layer one hidden layer one output layer compose chain two function network map output follow hidden layer output layer number hidden layer length chain give depth network finally role node analogous brain neuron call network neural node neuron number neuron hidden layer give width model rst explain neuron neural network work nonlinear neuron basic idea nonlinear model start linear model apply transform input instead nonlinear transformation goal deep learn determine choose function practice know piori function chapter deep neural network thus need learn reformulate model follow use parameter learn parameter map desire output goodfellow famous primary example xor function exclusive function take binary variable input produce output follow either approximate need learn parameter function easy show linear model rosenblatt perceptron rosenblatt learn xor function perceptron us linear combiner follow threshold function note weight vector perceptron bias vector parameter model perceptron map input output follow easily see linear classi er map object two class output variable respectively nevertheless latter true two class linearly separable de nition linearly separable two subset rp call linearly separable rp exists perceptron corresponds hyperplane de ned separate thetwoclasses however inthecaseofthe xorclassi er exit make linear perceptron incapable learn xor function example let input output matrix follow equation see equation contradictory equation indeed input linearly separable xor problem learn linear perceptron solve problem use di erent function map input another feature space represent classi er deep feedforward network let create simple feedforward network one hidden layer two unit see vector hidden unit compute function output layer compute function use put thus network represent chain function must choose function use linear func tions otherwise network would linear neural network us nonlinear perceptrons calledanactivation function orsquashing function appliedtorosenblatt slinearperceptron weight hidden layer bias activation function chosen input hidden output vector layer layer simple neural network two hidden unit clari cation bias represent de nition activation function activation function hidden neuron applies element wise operation vector weight connect neuron bias neural network use di erent activation function one common function classi cation task logistic function often call sigmoid function exp use sigmoid function activation function hidden output neuron complete network exp exp give solution xor problem let chapter deep neural network let explain model process batch input let set input call training set represent matrix form one example per column rst step neural network multiply input matrix rst layer weight matrix add bias vector obtain get value input example apply sigmoid function transformation input single line anymore thus nd hyperplane let process input nal output layer multiply weight vector add bias apply sigmoid function get solve xor problem di erent neural network use activation func tions modern application default use rectus ed linear unit relu jarrett de ned activation function max neural network one hidden relu unit linear perceptron output layer solves xor problem multilayer perceptron see latter classi cation problem determine architecture neural network great importance architecture refers structure network de ned depth width connection unit neural network asinequation layer input next layer multilayer perceptron mlp multilayer neural network represent multilayer perceptron question depth network determine architecture neural network important hornik show feedforward network single hidden layer su ciently many hidden unit arbitrary bound nonconstant activation function sigmoid activation function universal approximators nite input thus multilayer perceptron approximate continuous function mapping nite dimensional discrete space another deep feedforward network input hidden hidden output vector layer layer layer multilayer perceptron mlpwith hiddenlayerswith unit respectively output unit correspond output variable universal approximation theorem also prove wider class activation function relu leshno theorem mean su ciently large mlp represent function mean necessarily learn see next chapter large network slow train often prefer deep rather large network back propagation introduction statistical learn know implement neural network represent function input output need de ne nd network parameter base training set output target xor function example give speci solution classi cation problem error real application training set billion observation want classify model us many parameter case need learn parameter model way produce small error possible get good perfect classi cation franke training set de ne error loss function parameter want minimize respect parameter machine learn problem di erent example use neural network regression classi cation problem de ne di erent loss function one correspond speci problem see section let consider training set input vector target vector respectively estimate target neural network function estimate parameter minimum thanks optimization algorithm seesection call error gradient respect parameter rq require layer order understand change weight input layer impact loss function chapter deep neural network compute output layer computation complicate deep neural network need particular numerical method back propagation algorithm forward pas whenwefeedaninputx forward hidden layer network produce output step call forward pas graf forward propagation information continue train network training set produce scalar loss able update weight network correctly need able propagate information loss backward network second step call backward pas backward pas back propagation algorithm backprop rumelheart allows feedback information enable computation gradient well know method supervise training mlp backprop learn algorithm computation method allows network learn use optimization algorithm gradient descent describe back propagation algorithm apply neural network function single output understand back propagation work represent neural network compu tational graph node applies nonlinear perceptron operation example computational graph linear regression give dot operation operation computational graph linear regression backpropagation algorithm express error gradient respect quantity give neuron function outgo neuron possible thanks chain rule calculus computes derivative composition several function let real number real function take composition get output chain rule state use chain rule easy explicit expression error gradient scalar respect node computational graph produce scalar represent part multilayer perceptron computational graph activation unit input unit weight unit bias perceptron loss function deep feedforward network graph write error back propagation explicitly use partial deriva tives ui wij give backward equation back propagation algorithm compute gradient parameter follow zi give generalize implementation back propagation algorithm multilayer perceptron use matrix form activation function error back propagation back propagation matrix form let consider multilayer perceptron vector neuron activation layer matrix weight connect layer layer vector bias layer function applies elementwise activation function unit forward equation linear perceptron chapter deep neural network backward equation element wise product weight update recurrent neural network whichmakesmlpa static model sense input output pair mutually independent back tsoi seem ine cient want modelize sequential data input feature interdependent allow cyclic connection obtain recurrent neural network rnn modelize dynamic process example time series indeed interested dynamic process need model represent relation previous input output pair next one output function previous output model need process example one time retain memory represent contextual information reuse next time step arecurrent neural network share weight across several time step goodfellow language music processing architecture jordan presenteda cialneuralnetworks one cycle cycle make possible follow path neuron back allow feedback information cycle recurrent edge allow network hidden unit see previous output give network memory elman introduce notion time model recurrent neuron sometime refer context neuron state neuron structure simple recurrent neural network show input hidden output layer layer layer simple recurrent neural network one hidden layer jordan temporal context output information produce input state unit recurrent neural network hidden unit finally recurrent connection state unit output unit state unit allow output information fed back network follow time step recurrent connection constitute jordan network memory input hidden output layer layer layer state unit jordan recurrent neural network let explain rnn process sequence input back propagation recurrent neural network use algorithm calculate weight derivative rnn slightly modi ed version back propagation algorithm section call back propagation time bptt idea behind bptt unfold recurrent neural network time rojas stack identical copy rnn apply standard back propagation algorithm indeed unfold process result fact recurrent neural network share weight across time step comparison traditional mlp separate parameter input feature recurrent neural network lie core deep learn unfold network obtain deep architecture unfolded graph simple recurrent neural network give back propagation time multilayer perceptron bptt two step forward pas forward pas rnn mlp single hidden layer activation hidden layer come current input hidden layer activation previous time step graf let consider input sequence length present rnn input unit one hidden layer hidden unit output unit let also denote index interval input hidden layer connection index interval hidden chapter deep neural network output layer hidden layer input layer unfolded recurrent neural network time step hidden layer connection recurrent connection index interval hidden output layer connection consider follow notation xt value time input unit ut network input unit zt activation unit time weight network belongs indi erently weight connection input unit hidden unit ih weight connection hidden unit hidden unit nally weight connection hidden unit output unit hk notation inspire williams zipser graf make equation algorithm easy regular mlp see section hidden unit ut xt zt ih activation function apply exactly mlps get output hidden unit zt output unit network ut time zt ut ut zt hk atuniti byapplyingequations every get activation hidden layer nevertheless must choose initializer correspond network state receives information input dataset often set work well sequence sequence learn case use non zero noisy initial state zimmermann backward pas compute backward pas need derivative respect weight use back propagation algorithm applies standard back propagation recurrent neural network unfolded rnn thanks chain rule see section case rnn output layer current time step hidden layer next time step uenced activation hidden layer current time step graf give follow back propagate error ut ut ut hk ut rst compute apply equation recursively get ut term finally unfolded graph repeat weight duplicate get unique one across time step hidden layer use standard back propagation usual di erence sum gradient time step thus weight network get follow update rule pt xt xt ut ptt qu zti ut ut pt zt ut problem long term dependency saw rnn useful want modelize present input thanks previous information butinpractice indeed need explain current task must nd deep time step rather previous one computational graph recurrent network would deep simple rnn unlikely understand long term dependency need explain computation gradient back propagation algorithm let illustrate phenomenon simple rnn let suppose activation function logistic sigmoid rewrite equation use chain rule apply chain rule ht get qt hj qt diag sigmoid activation function hj jacobian matrix norm inferior indeed use sigmoid activation function squash output value derivative see pascanu complete demonstration thus sigmoid layer easily squash input small output region happens repeatedly stack multiple sigmoid layer even large change parameter rst layer nally really small impact output indeed accord equation long term contribution large go exponentially fast order matrix multiplication absolute value large eigenvalue recurrent weight matrix inferior boundary chapter deep neural network pascanu show tanh sigmoid activation function respectively moreover long term component explode instead vanish invert latter condition get necessary condition explode gradient solution vanish gradient problem williams zipser built approximation bptt algorithm truncate backward propagation information xed number prior time step call truncate back propagation time tbptt show vanish gradient occurs tbptt algorithm give good approximation true error gradient use weight penalty recurrent weight also solution pascanu etal back propagate gradient neither increase decrease much magnitude pascanu propose gradient clip deal explode gradient rescale gradient whenever norm kgk go threshold gv ifkgk kgk glorot propose use rectus er unit relu activation function rectifier max function compute neuron linear part linearity vanish gradient due nonlinear activation like tanh sigmoid indeed relu property squash input space small region also change structure model cope vanish gradient prob lem introduce new set unit call long short term memory unit lstm network hochreiter schmidhuber enforce constant error ow con stant error carousel thanks input output gate long short term memory network section chapter discus one use approach cope di culty learn long term dependency lstm unit problem remains important challenge deep learn could chosen solution echo state network leaky unit one refer goodfellow general introduction chose discus lstm network one ective sequence model gate recurrent unit gru gate recurrent neural network idea behind gru lstm unit create connection time constant error ow thus gradient neither explodes vanishes lstm network explicitly design avoid long term dependency problem remember information long period time practically default behavior popular neural network model sequence learn lstm architecture architecture lstm similar rnn di erence rnn supersets mlp lstm network supersets recurrently connect subnets know memory block graf basic memory block three single neural network layer interact instead one long short term memory network one memory cell call cell state central feature refer constant error carrousel cec hochreiter schmidhuber cell state produce minor linear transformation achieve constant error ow memory block linear unit xed recurrent self connection control cell state gate cell add memory cell one multiplicative input gate unit introduce protect current memory content store perturbed previous state currently irrelevant memory content gate give lstm ability control information ow cell state sigmoid activation function give amount information let trough close activation close open activation close thus input gate decides keep override information memory cell architecture cell state update base current state three source input net input sell recurrent connection net net input input output gate gers time step forward pas unit update error signal weight compute backward pas lstm found large success numerous application gers identi ed weakness process continual input stream without explicitly reset network state case cell state tends grow linearly learn make lstm cell degenerate ordinary recurrent network gradient vanishes see gers gers propose add memory block forget gate allows memory block reset thanks sigmoid activation function illustrates lstm cell inputgate outputgate cell forgetgate lstm cell forget gate source graf let one observation time input vector lstm cell implement follow equation graf chapter deep neural network xi hi ci xf hf cf tanh xc hc xo ho co tanh logistic sigmoid function respectively intput gate forget gate output gate memory cell activation xi hi xf hf cf xc hc xo index intuitive input input gate weight matrix hidden input gate ho co weight matrix etc back propagation lstm network like previous neural network lstm train gradient descent require error gradient section present computation exact lstm gradient bptt use follow notation weight connection unit ut network input unit time zt activation unit time index refer input gate forget gate output gate respectively refers one memory cell st state fo cell time activation function gate respectively cell input output activation function number input number output number cell hidden layer refer hidden hidden unit connection forward backward pass calculate section detail reader refer graf take equation forward pas let introduce ut follow equation gate long short term memory network input gate ut xt zt st hi ci zt ut forget gate ut xt zt st hf cf zt ut cell ut xt zt ic hc st ztst ztg ut output gate ut xt zt st io ho co zt ut cell output zt zth st backward pas let introduce notation zt st cell output ck ch output gate ut xc st ut state st zt zt st ci cf co cell ztg ut chapter deep neural network forget gate ut st ut input gate ut xc ut ut thanks complex architecture lstm easy train rnn nevertheless need de ne training occurs neural network chapter learn neural network previous chapter discuss architecture neural network represent mapping input output approximates target outputy wheref isunknown todothat chapter explain learn get best approximation deep learn procedure consists choose speci cation dataset loss function corresponds task optimization algorithm give model goodfellow explain three step statistical learn machine learn distinguish mainly three learn method duda supervise learn input dataset associate label target teacher provide cost input want reduce task classi cation regression problem thesis discus supervise learn algorithm unsupervised learn cluster try learn structure dataset explicit teacher model form cluster input pattern reinforcement learn interacts environment lie supervise unsupervised learning similar supervise learn feedback critic get real target category label improve classi er maximum likelihood estimation goal supervise learn algorithm estimate unknown probabilistic distribution distribution call true probability estimate use family forexample future price stock assume price follow nonlinear autoregressive process nar estimate rnn maximum likelihood estimation parameter give best approximation true probability modern neural network train use maximum likelihood goodfellow de nition maximum likelihood estimator true probability consider training set independently dis tributed let parametric family probability distribution space chapter learn neural network estimate true probability maximum likelihood estimator de ned argmaxp mv argmax take logarithm incidence value argmax transform product sum much easy deal argmin lnp mv de ne log likelihood function lnp lnp lnp likelihood pn theusualapproachto nd lnp mv leiblerdistance kl distance model probability empirical distribution de nition kullback leibler distance kl divergence two probability distribution give lnp lnp kl however aslnp andp de nition cross entropy cross entropy two probability distribution give lnp empirical distribution true probability training set problem simple linear regression compute maximum likelihood estimator easy use normal equation however practice much challenge neural network indeed analytical solution optimal weight must search use log likelihood loss function previous section discuss speci cation dataset present network estimate consider training set input vector target vector respectively estimate target neural network function must de ne loss function measure quality estima tion input training set common loss function negative log likelihood equivalently cross entropy network output target output saw minimize loss function brings maximum likelihood estima tion statistical learn per example loss function measure distance network output target output need get measure performance training set sum elementary loss function get loss function whole training set also call risk function lnp choice loss function directly related type output unit neural network depend task linear regression linear regression simply use linear perceptron output give hidden termediate activation output linear perceptron case maximize log likelihood similar minimize root mean square error rmse de ned follow rmse classi cation want represent class classi er standard approach use output unit softmax activation function obtain estimation class probability probability need vector need sum output exp softmax pk exp hot element equal zero except element correspond correct class equal one target probability loss function xx xx lnp lny particular case binary classi er network need predict one class probability case use single output unit sigmoid activation function output sigmoid function de ned equation loss function chapter learn neural network lnp ln lny ln optimization neural network previous section described compute derivative loss function network train gradient descent however need ensure training ective need choose optimization algorithm loss function learn parameter model gradient descent simplest algorithm know gradient descent repeatedly take small step direction negative error gradient parameter get follow update give iteration rq learn rate rq gradient cost function respect parameter gradient descentisdi culttouseinpractice moreover slow converge follow gradient entire training set technique call batch learn deep learn algorithm use stochastic gradient descent sgd indeed instead follow gradient whole training set sgd computes gradient minibatches small sample training set randomly chosen size minibatches determine batch size see batch size control update network weight training one important hyper parameter model estimate whole gradient average minibatches gradient unbiased estimator goodfellow chapter get follow update give iteration fx mini batch example training set sgd mini batch learn algorithm also use online learn algorithm update weight every sample present network batch size set large network redundant information training set know well use mini batch learn algorithm information online learn see bottou sgd popular sometimes slow qian propose method accelerates sgd convergence help sgd take right direction introduces contribution previous gradient current gradient variable hyper parameter determine fraction contribution update rule iteration optimization neural network rq adaptive optimization algorithm sgd basis many learn algorithm adagrad rmsprop dif ference algorithm adaptive learn rate indeed learn rate direction parameter space example gradient get stuck local minimum region rmsprop algorithm modi ed version adagrad algorithm goal perform well non convex function hinton change gradient divide learn rate exponentially decay average square gradient update learn rule give iteration follow pe adam kingma ba another adaptive algorithm nowadays one use optimization algorithm combination rmsprop momentum sgd algorithm give parameter value estimate rst moment mean second moment vector gradient estimation bias author compute bias correction time step finally formulate update rule give iteration batch normalization batch normalization optimization strategy algorithm method adaptive reparametrization developed io szegedy make training deep model easy indeed training deep neural network complicate fact distribution layer input change training parameter previous layer change io szegedy refer phenomenon internal covariate shift reduce normalize layer input batch normalization layer network train method performs normalization training mini batch allow high learn rate batch normalization also act regularizer prevent need penalty loss function goodfellow let de ne batch normalization transformation io szegedy chapter learn neural network de nition batch normalization layer consideramini batchofsizem fx value mini batch let pm mini batch mean pm mini batch variance let normalize value bm pxi constant finally let linear transformation bn learn know train neural network need explain measure learn indeed ective generalization method ability model perform well previously unobserved input call generalization challenge machine learn model good generalization power indicates model perform well new input section present problem deep neural network face generalization solution model selection establish superiority learn machine must make data generate assumption know optimal parameter assumption ever di cult know advance learn machine select nd best model correspond data de ned parameter need establish clear reproducible procedure base evaluation criterion holdout selection procedure holdout selection procedure consists partition available data three set train ing set validation set test set use training set consider di erent learn machine mlp rnn lstm di erent parameter apply model validation set select model best performance validation set apply model test set order measure generalization error generalization error evaluates prediction power model unknown data test set use training model generalization error indeed essential tting phenomenon described next paragraph tting tting problem tting tting problem main challenge machine learn tting occurs model learn underlie structure data implies large training error tting phenomenon appear com plexity model de ned next paragraph small tting occurs model learns exactly pattern training data generalize pattern prediction term tting happens train model learns also noise sample thus model good performance train set test set tting implies large gap training test error appear complexity model large generalization method model selection select best model use generalization error validation set select one low error respect rst occam razor domingo give two model generalization error simpler one prefer simplicity desirable di erent possible measure complexity number parameter model choice variable model receives input property function continuity slope vc dimension provide bound generalization error number training iteration tting phenomenon essentially related complexity model generalization error measure measure generalization error statistical learn theory give famous bias variance decomposition de ned follow error bias variance random variable equation see bias variance con tribute di erent proportion error depend complexity invite use training model famous method early stop developed yao described section yao show number training iteration connect tting phenomenon stop training early may reduce variance enlarge bias stop late may enlarge variance though reduce bias thus solve bias variance trade lead early stop rule metric model evaluation need look generalization power model give scalar loss need also evaluation metric performance measure give goal metric use depend task classi cation task usual metric accuracy model computes fraction correct prediction accuracy give formula accuracy fy yig however ectimbalancedclass next chapter indeed let consider binary classi er data point point correspondtoclass iftheclassi erpredict pointsinclass point class perfect classi er achieve optimal performance accuracyof however iftheclassi erpredict pointsinclass theaccuracyisthen classi er look achieves nearoptimal performance butin fact fails predict point class naive classi er thus need use alternative performance measure high robustness class skew example measure score introduce notion precision measure exactness binary classi cation task fraction correctly classi ed positive among example classi ed positive intuitively ability classi er label positive sample negative also introduce notion recall measure completeness binary classi cation task fraction correctly classi ed positive among positive intuitively ability classi er nd positive sample let introduce notation tp true positive number point assign correctly class fp falses positive number point belong class chapter learn neural network assign class incorrectly classi er fn false negative number point assign class classi er actually belong class recall precision measure class zgu tp tp tp fp tp fn take macro average measure compute locally category rst average category take macro take account imbalanced class uenced classi er macro performance rare category use weight measure calculates metric class nd average weight support number true instance label alternative take account label imbalance weight weight latter introduce metric give performance measure order compute generalization error model model low generalization error low loss good metric train test set regularization technique early stop see early stop cient hyper parameter selection algorithm number training step epoch another hyper parameter goodfellow early stop method stop training monitor quantity stop improv ing indeed tthedata training loss sometimes test set error begin increase training step indicates tting case well generalization power stop training test set error increase early stop method monitor example loss metric validation set training step stop training loss start increase accuracy start decrease case metric use accuracy early stop one use regularization technique deep learn simplicity ectiveness reduce training time dropout common way avoid tting deep neural network use dropout layer propose srivastava key idea randomly drop unit along connection neural network training force weight unit equal reduces number parameter model dropout also thought ective bagging method many large neural network goodfellowetal el test example dropout method train ensemble sub network construct remove input unit underlie base network generalization method explain di erent architecture neural network reproducible objective method train select best model correspond speci task let give example application deep neural network nancial data chapter neural network time series forecasting cryptocurrencies nancial asset rst cryptocurrency bitcoin create satoshi nakomoto bitcoin invent rst unregulated digital currency design work medium exchange thanks blockchain technology distribute ledger base decentralize peer peer network con rm transaction ten year later alternative cryptocurrencies call altcoins identi ed coinmarketcap prove real cryptocurrency market emerge indeed cryptocurrency market experience strong growth last year infer crix developed trimborn ardle economy become digital natural think role digital asset cryptocurrencies investment decision also grow indeed investor former exist nancial market interested cryptocurrencies new nancial product cryptocurrency market create apparition option future bitcoin eisl analyze bictoins return successfully show add bitcoin portfolio optimal diversi cation ect elendner catione ect finally trimborn show mix cryptocurrencies stock could improve risk return trade portfolio formation elendner investigate cryptocurrencies alternative investment asset study return follow section focus application di erent model present chapter time series model especially cryptocurrencies price stock price underlie assumption time series modelization numerous time series present autoregressive structure past observation use explain present future observation equation autoregressive structure unknown numerous model di erent assumption try represent structure common linear model arima sarima model simple model move average seasonality model considerable explanation power stock price underlie assumption nevertheless challenge indeed arima model strong assumption practice hard meet constant parameter white noise residual homoscedasticity example stock price often present heteroscedastic residual face problem econometrician built wide variety di erent model arch garch model however nd model explain structure stock price questionable actually predict future value especially long term horizon fact random walk theory stock price implies predictive model stock price useless stock price time series nance itisparticularly challenge property stock price behave simple time series indeed random walk theory suggest stock price return independently identically distribute time past value stock return explain future value direct result best prediction tomorrow price price today theory lead cient market hypothesis emh asserts price stock ect relevant information available imply investor outperform market trading strategy base decision make available information short fundamental analysis try evaluate intrinsic value security would irrelevant nancial market emh true moreover technical analysis would also irrelevant oppose fundamental analysis technical analysis aim forecast fu ture price movement base statistic technical market indicator move average bollinger interval underlie variable price movement volume market capitaliza tion global economic variable technical analysis forbid use linear autoregressive model arima price movement modelization arima model use past observation time series regressors suppose relation linear past year cient market hypothesis weaken credibility among eco nomics specially appearance behavioral economics revitalize fundamental technical analysis thesis use technical fundamental analysis suppose stock linear autoregressive exogenous variablesmodel nlarx nlarx model let rst de ne nlarx model de nition non linear autoregressive exogenous variable model letfs real value time series fx vector exogenous variable independent real random variable nlarx process represent mapping rp xd rq de ned follow example obtain follow nlarx model mapping nlarx represent neural network function rp xd rq parameter de ned follow chapter neural network time series forecasting structure network depends nature example mlp network would xd input neuron output neuron architecture network challenge represent di erent time lag autoregressive part model exogenous part see de nition nlarx process give great liberty forecasting procedure indeed distinguish two main procedure predict time window length time series recursive strategy assume represent nlarx network sbt sbt sbt sbt sbt strategy advantage parsimonious number parameter esti mate thus network minimum complexity however crucial weakness accumulate estimation error forecasting step thus forecast window large performance quickly degrade estimation price step ahead really unreliable moreover model represent relation past value learn structure output value itisamany sequence produce scalar nevertheless recursive strategy apply small forecast window indeed small forecast window might need small look back window lead less parameter estimate simpler model easy faster train multiple output strategy assume represent nlarx network sbt sbt prevent error forecast increase forecast window grows model complex need parameter allow learn dependence input output well output emany many sequence model nevertheless complexity implies cost longer training time large amount need data avoid tting problem cryptocurrencies price movement analyze data analysis selection important cryptocurrencies base market capitalization thanks crix dataset http crix hu de disposal daily data cryptocurrencies year since cryp cryptocurrencies price movement analyze important cryptocurrencies select cryptocurrencies large market capitalization period without miss value avoid miss data imputation problem time start study february cryptocurrencies bitcoin btc dash dash ripple xrp monero xmr litecoin ltc dogecoin doge nxt nxt namecoin nmc data nmc nxt doge xmr dash ltc xrp btc min max mean median standard deviation market capitalization statistic million dollar import cryptocur rencies observe btc dominates market market capitalization time big average xrp second important cryptocurrency time large nmc small cryptocurrency consider structure cryptocurrency market change everyday time write paper january cryptocurrencies large market capitalization di erent one select still important cryptocurrencies still top cryptocurrencies large market capitalization average period consider say cryptocur rencies dominate market old cryptocurrencies term market capitalization consider also recent cryptocurrencies market structure di erent result hold see nmc nxt doge xmr dash ltc xrp btc position market capitalization importance cryptocurrencies average period nevertheless btc still dominate market period xrp ltc dash major cryptocurrencies term market capitalization last month march october say eth bch neo xem miota etc major new cryp tocurrencies rank top cryptocurrencies large market capitalization average march october would interest add future analysis application example btc return section show di erent step build reproducible method time series forecasting neural network let imagine forecast problem want predict btc price two day future apply nlarx model btc price use technical fundamental indicator discus part choice parameter let btc price time estimate nlarx process follow mlp function dimensional vector exogenous variable chapter neural network time series forecasting suitable transformation daily return day day day move average respective bollinger band de ned section crix daily return time euribor interest rate di erent horizon year month month di erent exchange rate euro uk euro usd jpy cryptocurrencies price available everyday replace miss value weekend last value observe closing exchange friday training neural network need preprocess data indeed input represen tation important neural network price stationary saw previous section consider logarithm di erenced price also call log return take di erenced logarithm price series eliminate trend seasonality get logarithm return hold btc period consider one day ln standardize input variable avoid scale problem respectively mean standard deviation one input variable training set get standardize prediction test set use mean standard deviation training set get prediction original scale estimate nlarx follow mlp funtion rscaled rscaled rscaled rscaled rscaled xscaled rscaled xscaled represent scale daily return btc scale exogenous variable respectively network give prediction btc return however want predict btc return order make simple trading decision buy btc price expect grow sell btc price expect fall thus actual value price irrelevant interested future price movement reformulate problem binary classi cation future trend ln ppt ln pt build mlp network correspond task function rscaled rscaled rscaled xscaled thesis implementation di erent model use kera python library withtensor ow backend use data train set data test set represent di erent mlp architecture term trading strategy performance measure network accuracy model see equation since measure number correct prediction use tanh activation function hidden layer nal output layer two neuron correspond number class target variable softmax activation function order get probability class rst build baseline model two hidden layer neuron train network rmsprop algorithm see section epoch default batch size default learn rate evaluate baseline model fold cross validation see next paragraph train set obtain accuracy score variance cryptocurrencies price movement analyze comparison di erent mlp architecture hyper parameter tune model tune important step model construction consists building various model di erent hyper parameter value nd one best data ing evaluation criterion model di erent hyper parameter focus architecture neural network width depth select best model tune number neuron layer scikit learn python library order realize tune respect hold procedure realize fold cross validation train set let explain brie method split train set subsamples chronologically order train model rst sample test last sample repeat operation time rst train model rst sample validate retrain model test repeat operation train model test nal score average score select model best average score finally score test set give generalization error example realize fold cross validation train set model best average accuracy nal model select grid search test mlp architecture two three four ten hidden layer hiddenlayers neuronsperlayers beststep beststep perlayer grid search hyper parameter tune rst realize tune maximum number epoch epoch follow present accuracy nal model step epoch hidden layer neuron per layer accuracy model model model model model model model model grid search hyper parameter tune btctuning see architecture equivalent performance especially add layer necessarily improve accuracy model particular deep mlp architecture model model low performance baseline model nevertheless architecture three four hidden layer perform least good model two hidden layer indicate add layer baseline architecture chapter neural network time series forecasting improve generalization power finally model best model cross validation score nal measure generalization error obtain compute accuracy test set unseen model obtain nal accuracy score chapter application cryptocurrency portfolio application inspire pilot study carry cooperation com merzbank ag franke goal develop trading strategy portfolio make important stock dutch cbs index base predict return transpose strategy portfolio make cryptocurrencies dom inates crix trimborn ardle early franke study restrict buy hold strategy time horizon quarter year trading day cryptocurrency exchange open weekend portfolio create begin quarter held three month without alteration include cryptocurrencies whose price predict rise signi cantly hold end end quarter sell afterwards end three month value portfolio large possible basis trading strategy three month forecast cryptocurrencies use modelize time series use nlarx process see de nition represent price cryptocurrency see build one model cryptocurrency order predict price three month ahead let rst present input data use model data analysis cryptocurrencies price price height important cryptocurrencies di erent distribution follow trend btc clearly dominate cryptocurrency market average price almost usd last three year time large dash second cryptocurrency value time large xrp cryptocurrency second large market capitalization see cryptocurrency market experienced incredible growth last year see evolution crix price result consider price evolution cryptocurrencies within two di erent period july rst month price di erent cryptocur rencies almost constant close minimum begin year october price experienced rapid growth reach pick end summer start decline see fact price range change time cryptocurrency problem modelization see next section need preprocess data chapter application cryptocurrency portfolio btc dash xrp xmr ltc doge nxt nmc min st quartile median mean rd quartile max cryptocurrencies price statistic crix price technical indicator saw section technical indicator useful variable time series model forecast cryptocurrencies price decide use three major technical indicator bollinger band di erent time horizon two week one month three month day corresponds day traditional market let price stock time de ne simple move average order roll standard deviation order three component bollinger band corresponds central band upper band low band xed parameter historically equal width band direct indicator volatility stock example observe bitcoin price volatility increase price grow rst month model architecture bictoin price red day move average green low upper bollinger band light blue crixbtcbb cryptocurrency model consider bollinger band cryptocurrency price three different horizon reflect long term trend direction price volatility fundamental indicator reflect intrinsic value price chose include also fundamental indicator exogenous variable franke study use international interest rate euribor different horizon portfolio make cryptocurrencies use different exchange rate able catch inner relation cryptocurrency market also use value cryptocurrencies consider cryptocurrency index crix represent variable use input modelize cryptocurrency price technical indicator bollinger band day day day crypto crix euribor month month year fundamental indicator open low high close price exchange uk euro euro usd jpy usd input variable model model architecture goal build portfolio maximum three month return forecast point time series large lag notoriously unreliable franke goal find good forecast price cryptocurrencies maximize return cryptocurrency portfolio build direct forecast strategy nlarx model rather predict price movement actual price thus try predict trend correctly cryptocurrency price increase significantly morethan lessthan change chapter application cryptocurrency portfolio let consider price cryptocurrency time let assume nlarx estimate function de nition di erent neural network architecture sbt sbt data preprocessing log return transformation feed input variable network need preprocess data indeed saw cryptocurrencies price easy handle us log return moreover interest forward movement price use return multiple step return rather daily return de ne log return follow ln sequence learn deal time series go use sequence learn algorithm thatis want able predict price cryptocurrency rise within sequence also rise sequence fed network thus use many many sequence model input variable input use sequence past value multiple log return cryptocurrencies past value exogenous variable write matrix dimension br log return cryptocurrency time vector exogenous variable technical indicator cryptocurrency fundamental indica tor time cryptocurrency model include bollinger band cryptocurrency consider order reduce complexity model avoid tting problem thus height model share input variable except bollinger band output variable output variable depends objective xed activation function output layer output use sequence trend classi cation future need code log return three dimensional variable ect price movement accordingly objective st st st st otherwise model architecture write sequence sequence learn problem follow scale finally neural network us activation function squash input variable output interval need scale input data neuron learn faster see lecun scaler use depends activation function use default lstm activation function tanh scale input variable baseline model architecture timesteps number feature inner structure output layer dimension time step number class indeed apply future time step softmax layer three neuron one class output variable change inner structure model test di erent architecture neural network loss function since problem classi cation task use cross entropy loss function saw market quite stable begin period experienced rapid growth end expect unbalanced class dataset training balance loss function class weight clearly see except ltc cryptocurrencies unbalanced class cryptocurrency btc dash xrp xmr ltc doge nxt nmc class repartition thus need use modi ed version cross entropy de ned equation use weight cross entropy xx lny class weight frequence class metric performance metric model selection use accuracy weight measure introduce section chapter application cryptocurrency portfolio baseline model performance let rst present three di erent baseline model two hidden layer ten neuron correspond three type neuron lstm simple rnn perceptron reduce training time use well know method early stop de ned section apply early stop validation accuracy validation loss patience epoch stop model training validation accuracy increase every epoch validation loss decrease every epoch repeat training model ten time order obtain average performance robust present result cryptocurrency network epoch loss accuracy measure lstm btc rnn mlp lstm dash rnn mlp lstm xrp rnn mlp lstm xmr rnn mlp lstm ltc rnn mlp lstm doge rnn mlp lstm nxt rnn mlp lstm nmc rnn mlp performance baseline model test set forbtc dash power validation set minimum measure lstm model dash maximum measure mlp model xmr cryptocurrency result mixed mlp lstm model good measure return xrp doge respectively since generalization power well pure random forecast would measure model seem network experienced real di culties training process example nmc best performance three model occur rst iteration since validation loss accuracy improve eleventh epoch large loss minimum really poor prediction performance maximum measure network incapable extract general information training set see phenomenon occur model architecture ltc doge nxt problem maybe due overfitting problem see section diagnosis training process observe loss accuracy curve training validation set present curve first epoch training nmc training blue curve validation orange curve accuracy training blue curve validation orange curve loss loss accuracy curve see model performs well train set nevertheless indeed improve bestmodelselection parameter regularization aswesawinsection improve baseline model first need apply one dropout one batch normalization layer hidden layer avoid overfitting make training faster easy finally find best architecture model tune hyper parameter model depth width model also learn rate batch size chapter application cryptocurrency portfolio general architecture model present shape di erent layer one baseline model except last dimension depends number neuron layer explain tune number number hidden layer next paragraph model tune saw section model tune important step model construction realize tune width depth model previous chapter also tune training parameter learn rate batch size epoch rst tune batch size grid search make computation easy kera chose batch size reduce variance evaluation metric give robust result large batch size also allow reduce considerably training time finally test grid search learn rate realize small learn rate improve training make complexity model high need epoch thus select potential learn rate avoid large number trainable parameter model prefer use less neuron layer example mlp one hidden layer neuron input variable trainable parameter mlp two hidden layer neuron trainable parameter prefer second architecture nevertheless realize add fourth layer improve measure model thus establish grid search parameter width neuron correspond number feature model depth layer select parameter correspond model high measure also could test much value hyper parameter model computational resource limited study evaluation nal model present evaluation metric test set tune model lstm network best generalization power btc dash ltc nxt nmc mlp network best generalization power xrp xmr doge rnn always low performance except xmr quarterly return perfectly predict rnn mlp network measure cryptocurrency result di erent see three di erent model btc dash xmr achieve good performance minimum measure xmr lstm network result important since three cryptocurrencies expensive cryptocurrencies must underline rnn network seem fail extract information necessary perform good generalization training set explainable tting problem ne tune batch size learn rate necessary could also increase complexity model add training epoch allow reproducible method realize ne tune compare performance latter model performance baseline model select one best measure nally obtain nal model use trading strategy prefer rnn baseline model dash xrp doge lstm baseline model xmr doge nally mlp baseline model nxt latter see easy outperform mlp baseline model indicate hyper parameter mlp network model architecture cryptocurrency model loss accuracy score lstm btc rnn mlp lstm dash rnn mlp lstm xrp rnn mlp lstm xmr rnn mlp lstm ltc rnn mlp lstm doge rnn mlp lstm nxt rnn mlp lstm nmc rnn mlp tune model performance test set present architecture metric nal model select average lstm rnn mlp network achieve performance measure respectively show lstm best model term prediction accuracy worth noticeable mlp network two hidden layer achieve well performance mlp network three hidden layer rnn lstm network tend deeper prefer three hidden layer four cryptocurrencies eight respectively number hidden unit seem major impact perfor mance finally lstm network prefer small learn rate xrp whereas high learn rate select rnn network consider low performance rnn network fact small learn rate rnn network achieve low per formance high learn rate could maybe adopt technique reduce learn rate training consists begin training network high learn reduce plateau monitoring evolution metric consider multi training split train test set amount data train set may uence result indeed train set may include special pattern data useful generalize purpose may exclude general information improve performance test set nally test stability forecasting method new information becomes available retrain model quarter cut test set three set correspond three consecutive quarter chapter application cryptocurrency portfolio cryptocurrency model first layer second layer third layer learn rate score lstm btc rnn mlp lstm dash rnn mlp lstm xrp rnn mlp lstm xmr rnn mlp lstm ltc rnn mlp lstm doge rnn mlp lstm nxt rnn mlp lstm nmc rnn mlp final model architecture test set experiment three step predict rst quarter test set add original pred train set rst quarter test set update weight model new model repeat operation get new train set pred last prediction finally get prediction whole test set thanks three pred model pred compute measure three step pred pred pred prediction whole test set get nal performance wecompareintable shotandathree stepsprediction next quarter either model new train set data special pattern useful generalization mlp rnn lstm cryptocurrency onetraining multitraining onetraining multitraining onetraining multitraining btc dash xrp xmr ltc doge nxt nmc average comparison measure one shot multi training prediction evaluation different trading strategy see lstm network multi training experiment improve cryptocurrency performance model underline ability lstm network capture new pattern data necessity retrain model quarter nmc nxt ltc obtain mixed result mlp rnn network indicates maybe tendency new data measure decline one shot three step prediction nevertheless average improve generalization power point mlp rnn lstm network respectively evaluation di erent trading strategy evaluate di erent model term trading strategy need look another measure indeed thef accuracy model performance trading strategy base model prediction compare model use section nancial return trading strategy base prediction model let rst de ne return portfolio compose cryptocurrencies de nition portfolio return let consider portfolio asset pi price asset time day return asset pi pi ri pi day return portfolio de ned rn add constraint weight capital invest portfolio divide asset include aswecansee indeed whenwe take short position cryptocurrency sell margin cryptocurrency implies borrow amount broker risk free interest rate free return long short portfolio write rl jr free contains long short position respectively moreover need benchmark compare strategy test set use three benchmark portfolio replicate crix crix quarterly return portfolio base prediction baseline model type neural network portfolio base perfect prediction true signal observe training signal test set benchmark ect accuracy predictive model finally apply strategy begin quarter apply everyday create new portfolio day would new investor way measure performance portfolio begin test set overall performance strategy whole test set measure overall performance di erent strategy cumulative quarterly return regardless date investment give two way evaluate strategy chapter application cryptocurrency portfolio ifattime crix return date say portfolio beat cryptocurrency market quarter say quarter pro invest strategy rather crix use indicator number day investment give high quarterly return crix cumulative quarterly return portfolio high cumulative return crix end test set say portfolio beat cryptocurrency market whole period consider conclude always well invest strategy rather crix test period use indicator cumulative return end test set buy hold strategy price weight portfolio first use buy hold strategy period consider quarter year buy cryptocurrency return predict increase signi cantly close position portfolio end quarter consider compare return portfolio portfolio replicate crix suppose buy one coin cryptocurrency compare value portfolio begin end quarter consider let ensemble cryptocurrencies buy time number coin buy cryptocurrency quarterly return portfolio time pi pi pi since case buy one coin cryptocurrencies include portfolio pi pi pi portfolio correspond price weight portfolio return uenced expensive cryptocurrency indeed cryptocurrencies high price give weight great uence performance portfolio pi pi pi pi pi pi ti ti ti pi pi pi ri pi formula corresponds de nition example build portfolio cryptocurrencies consider thesis get average test set weight build three trading strategy base trading signal obtain prediction di erent model result three price weight portfolio base evaluation different trading strategy cryptocurrency weight btc dash doge ltc nmc nxt xmr xrp weight price weight portfolio average test set lstm rnn mlp network prediction respectively result present see lstm portfolio clearly outperforms mlp portfolio march equivalent date indeed see mlp portfolio successively predict wrong trading signal march mai imply also rnn portfolio outperforms mlp portfolio april nevertheless three strategy perfect prediction portfolio beat crix mid may see quarterly return indeed know trimborn ardle crix base market capitalization index give high weight large cryptocurrencies term market capitalization see next section detail marketcap index thus structure crix return uenced performance cryptocurrencies consider nevertheless see crix return two time large portfolio end march result say cryptocurrencies low marketcap height cryptocurrencies consider large return february june since small weight crix cryptocurrencies must also remind reader ethereum value multiply factor period include yet ethereumisthe second cryptocurrency term market capitalization thus second important uence crix return test set explain see cumulativereturns portfolio whole period nevertheless lstm portfolio always beat crix mid may indicates expensive cryptocurrencies best investment solution last quarter similarly mlp portfolio beat crix mid june since return price weight portfolio uenced expensive cryp tocurrency say latter strategy highly uenced btc change one model wrong predict btc return low performance wrong predict cheap cryptocurrencies however good performance fromthis tfromhighreturns cheap cryptocurrencies yet xrp nxt doge cheapest cryptocurrencies study seetable also build portfolio base market capitalization weight market capitalization weight portfolio market capitalization marketcap cryptocurrency de ned ki si pi pi si respectively price supply number coin chapter application cryptocurrency portfolio price weight quarterly return top cumulative quarterly return bottom crix blue perfect prediction orange lstm green rnn red mlp portfolio purple crixportfolio market cryptocurrency time marketcap weight portfolio weight proportional market capitalization itscryptocurrencies return portfolio define ki ri ki forexample get average test set weight expect portfolio diversified price weight portfolio present performance strategy rnn portfolio best performance beat crix whole test set thanks large return first quarter nevertheless overall performance strategy similar price weight evaluation different trading strategy cryptocurrency weight btc dash doge ltc nmc nxt xmr xrp weight marketcap weight portfolio average test set portfolio crix beat mlp lstm strategy indeed since market grow marketcap weight portfolio base accurate prediction corresponds approximatively basket portfolio contain height large constituent crix since crix weight de ned market capitalization trimborn ardle finally marketcap strategy pro high return cryptocurrencies low marketcap yet nxt doge low marketcap see experienced high return see order bene high return cryptocurrencies low price market capitalization build last trading strategy equally weight portfolio simplest beta strategy equally weight portfolio equally weight portfolio weight stock basket equally result port folio highly diversi ed oppose marketcap weight portfolio overweight overprice cryptocurrencies underweight underpriced cryptocurrencies implies overweight cryptocurrencies large risk indeed small cryptocurrency high risk failure nevertheless equally weight portfolio really easy construct since weight inversely proportional number cryptocurrencies basket quarterly return portfolio de ned follow number stock basket time ri present performance di erent strategy base portfolio aswecansee portfolio beat market whole test set since cumulative return much high crix portfolio end test period indeed portfolio bene high return cheap cryptocurrencies begin test set oppose crix however end test period di erent strategy similar performance indeed cheap currency example xrp nxt doge experienced low return close even negative return end may since cryptocurrencies overweighted compare crix weight crix beat equal weight portfolio day last quarter nevertheless overall performance strategy base predictive model much high crix almost time high mlp portfolio finally see strategy base prediction tune model always beat strategy base prediction baseline model type chapter application cryptocurrency portfolio marketcap weight quarterly return top cumulative quarterly return bot tom ofcrix blue perfectprediction orange lstm green rnn red andmlpportfolios purple crixportfolio neural network best strategy equally weight portfolio base prediction mlp model long short strategy latter subsection expose trading strategy base month buy hold negative return profit prediction short sell cryptocurrencies predict decrease significantly section build portfolio follow manner buy cryptocurrencies longposition buy hold strategy moreover predict cryptocurrency decrease significantly borrow cryptocurrency fiat money broker sell day end quarter close short position purchasing cryptocurrency borrow order give back broker price actually decrease make profit evaluation different trading strategy equally weight quarterly return cumulative quarterly return crix blue perfect prediction orange lstm green rnn red mlp portfolio pur ple crixportfolio see observe first strategy base final model large cumulative return end test period strategy base baseline model except strategy base marketcap portfolio mlp prediction equally weight portfolio rnn prediction price weight marketcap final portfolio beat crix quite similar performance whole test set see equal weight portfolio base mlp lstm prediction beat crix see lstm equal weight portfolio perform two time well crix final return lstm portfolio always beat rnn mlp portfolio long run mean wrong prediction rnn mlp network cost lot term return trading strategy indeed look return equally weight portfolio base rnn final prediction see strategy negative performance cause wrong prediction long instead short trading signal conversely indeed lstm chapter application cryptocurrency portfolio baseline model final model portfolio price marketcap equal price marketcap equal crix lstm rnn mlp maximum crix crix rnn crix crix mlp best nn lstm mlp rnn lstm rnn mlp long strategy cumulative return end test period baseline model final model portfolio price marketcap equal price marketcap equal crix lstm rnn mlp maximum crix crix crix crix crix lstm best nn lstm mlp mlp lstm lstm lstm long short strategy cumulative return end test period strategy well performance lstm model well measure comparison buy hold long short strategy model would low misclassi cation rate long short trading signal bene long short strategy compare overall performanceofabuy holdandalong shortstrategy always beat long short strategy indeed long shortstrategy ismore costlyto predict long trading signal instead short conversely predict nothing instead short long trading signal also see nal performance price weight portfolio base long long short strategy quite similar di er compare marketcap equally weight portfolio especially portfolio base mlp rnn prediction long long short portfolio price marketcap equal price marketcap equal crix lstm rnn mlp maximum crix crix mlp crix crix lstm best nn lstm rnn mlp lstm lstm lstm long long short strategy cumulative return end test period ingeneral fromtheseresults weprefertoapplyabuy short position model predict many trading signal short instead long evaluation different trading strategy position conversely however know hard beat index market grow year cryptocurrency market know exponential growth see average strategy portfolio beat crix test set quite remarkable moreover equally weight portfolio always beat crix except rnn long short portfolio maximum cumulative return mlp long portfolio time high crix long long short portfolio average best strategy price marketcap equal price marketcap equal mlp equal lstm equal rnn equal average equal best strategy lstm rnn mlp lstm mlp lstm mlp mlp equal number day percentage high return crix portfolio chapter discussion future work discussion method data data quality thesis use deep learn method require large amount data order estimate parameter model correctly especially rst solution tting problem collect data since cryptocurrency market quite new could get daily data cryptocurrencies consider without introduce miss value follow thesis would work data high frequency example every minute reduce hold period portfolio generalization cryptocurrency market dynamic market structure change comparison january start study cryptocurrencies consider part large market capitalization one year later time write major cryptocurrencies emerge ethereum bitcoincash iota many would necessary always update initial basket relation evolution market structure model architecture first could use random grid search tune width depth di erent neural network indeed show good performance literature could also go tune hyper parameter model low performance test set indeed model selection involves multiple trial error need important computation power study could also test type model convolutional neural network want underline ability lstm outperform former simple rnn mlp discussion result question prediction nancial market problematic especially cryp tocurrency market problem liquidity wale scam raise many time moreover new example gigantic bubble cryptocurrency market bitcoin price reach dollar result take great care since question prediction bubble quite problematic least expect towards real portfolio management strategy model perform well since much easy predict long position highly grow market thus even deep learn methodology always try avoid tting would interest see model perform bearish market towards real portfolio management strategy model metric loss function chose build height di erent model predict independently price trend cryptocurrencies franke study would interest predict weight cryptocurrency inside weight portfolio maximize return model could use custom loss function model learns tp penalize misclassi cation cost trading strategy indeed satis ed built loss metric function kera library sparse categorical crossentropy accuracy example order nd strategy high return could chosen return trading strategy base prediction test data selection criterion nal model nevertheless strategy maybe work short term manner long run without take account risk measure would eventually fails risk management indeed strategy base quarterly investment decision return portfolio management system realistic indeed take consider ation volatility portfolio order ect real investor behavior economic agent sharperatio maxdrawdown optimaldiver si cation see franke early application neural network portfolio management strategy nevertheless portfolio management solution strong assumption distribution return assumption hard meet real life particular cryptocurrency market highly volatile reinforcement learn study predict future price movement consider trading signal con verting directly trend action investor however price prediction market action moreover model predict amount buy sell cryptocurrency build real portfolio management strategy need consider another type machine learn reinforcement learn simulates real arti cial intelligence reinforcement learn study reaction agent towards environment thanks policy reward function jiang present nancial model free reinforcement learn framework provide deep machine learn solution portfolio management problem apply cryptocurrencies chapter conclusion thesis present general introduction deep neural network theory apply nancial time series focus analysis recurrent neural rnnand lstm architecture deep learn methodology order open researcher practitioner call black box neural network reader basic problem train order build prediction unseen data lem nancialdecisions withthisthesis provide rst cryptocurrencies portfolio base deep learn asset selection strategy bytuningthedi erenthyper wepresented show methodology necessary since hyper parameter tune always improves prediction accuracy model performance result cryptocurrency market take great care since market experience abnormal positive return typical bubble situation especiallylstm high prediction accuracy strategy succeed beat crix index term nancial return show index funding outperform ai base buy hold strategy even highly grow market nevertheless predict market risky realistic investment system implement take account active environment evolve neural network trend prediction able understand nancial cost misclassi cation would interest study strategy would perform add nancial policy risk measure learn process reinforcement learn manner cryptocurrencies cryptocurrencies price cryptocurrencies quarterly return cryptocurrencies cryptocurrencies price dollar cryptocurrencies quarterly return cryptocurrency quarterly return test set model architecture lstm model model architecture mlp model rnn model long short portfolio long short price weight quarterly return cumulative quarterly return crix blue perfect prediction orange lstm green rnn red mlp portfolio pur ple crixportfolio long short marketcap weight quarterly return cumulative quarterly return crix blue perfect prediction orange lstm green rnn red mlp portfolio purple crixportfolio long short portfolio long short equally weight quarterly return cumulative quarterly return crix blue perfect prediction orange lstm green rnn red mlp portfolio purple crixportfolio bibliography back tsoi fir iir synapsis new neural network architecture time series model neural computation pp bottou line learn stochastic approximation saad ed line learn neural network cambridge press new york ny usa pp domingo role occam razor knowledge discovery data mining knowledge discovery duda hart stork pattern classi cation nd edition wiley interscience eisl gasser andweinmayer caveatemptor diversi cation ssrn scholarly paper id rochester ny social science research network elendner trimborn ong andlee thecross nancial asset lee kuo chen deng ed handbook digital finance financial inclusion cryptocurrency fintech insurtech regulation chinatech mobile security distribute ledger st edition elman find structure time cognitive science franke nonlinear nonparametric method analyze nancial time series kall lu thi ed operation research proceeding select paper international conference operation research zurich august september springer heidelberg heidelberg pp franke ardle hafner statistic financial market springer gers schmidhuber cummins learn forget continual prediction lstm neural computation glorot bordes bengio deep sparse rectus er neural network gor dunson dud ed proceeding th international conference arti cial intelligence statistic vol proceeding machine learn research pmlr fort lauderdale fl usa pp goodfellow bengio courville deep learn mit press cambridge usa http www deeplearningbook org graf supervise sequence label recurrent neural network springer graf generate sequence recurrent neural network hinton overview mini batchngradient descent unpublished lecture bibliography hochreiter schmidhuber long short term memory neural computation hornik stinchcombe white multilayer feedforward network uni versal approximators neural network io szegedy batch normalization accelerate deep network training reduce internal covariate shift corr ab jarrett kavukcuoglu ranzato lecun best multi stage architecture object recognition ieee th international conference computer vision pp jiang xu lian deep reinforcement learn framework nancial portfolio management problem arxiv print jordan serial order parallel distribute processing approach technical report institute cognitive science california san diego kingma ba adam method stochastic optimization international conference learn representation lecun bottou orr mu ller cient backprop orr mu ller ed neural network trick trade springer heidelberg heidelberg pp leshno ya lin pinkus schocken multilayer feedforward network nonpolynomial activation function approximate function neural network zgu zgu gu ng text categorization class base corpus base keyword selection yolum gu ng gu rgen zturan ed computer information science iscis th international symposium istanbul turkey october proceeding springer heidelberg heidelberg pp pascanu mikolov bengio di culty training recurrent neural network dasgupta mcallester ed proceeding th international conference machine learn vol proceeding machine learn research pmlr atlanta georgia usa pp qian momemtum term gradient descent learn algorithm neural network rojas neural network systematic introduction springer rosenblatt perceptron probabilistic model information storage orga nization brain psychological review rumelheart hinton williams learn internal representation error propagation rumelheart mcclelland ed parallel distribute processing exploration microstructure cognition mit press cambridge usa chapter pp vol srivastava hinton krizhevsky sutskever andsalakhutdinov dropout simple way prevent neural network tting journal machine learn search bibliography trimborn ardle crix index blockchain base currency sfb econmic risk revise resubmit journal empirical finance trimborn li ardle invest cryptocurrencies liquidity constrain investment approach sfb discussion paper williams andzipser gradient computational complexity rumelheart mcclelland ed back propagation theory hillsdale chapter pp yao rosasco caponnetto onearly stoppingingradient descentlearning constructive approximation zimmermann chappelier bunke ine grammar base recognition handwritten sentence ieee transaction pattern analysis machine intelligence pp