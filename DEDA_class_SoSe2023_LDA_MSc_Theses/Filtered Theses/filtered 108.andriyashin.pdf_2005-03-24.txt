financial application classification regression tree master thesis present anton andriyashin prof dr wolfgang ardle case center apply statistic economics partial fulfillment requirement degree master art march declaration authorship hereby confirm author master thesis independently without use others indicate resource passage literally general matter take publication resource marked anton andriyashin march study give outline modern theory classification regression tree cart show advantage cart application finance practical issue regard cart application core implementation present second part work mainly concentrate dax use cart forecasting system realm comparison technical fundamental approach perform finally information age effect context learn sample construction analyze keywords cart decision tree financial application information age effect simulation content introduction classification regression tree cart cart cart advantage decision tree work impurity measure classification tree gini index twoing rule practice optimal size decision tree overview available method cross validation method optimal decision tree prune cost complexity function cross validation regression tree difference cart practice regression tree classification tree financial application cart challanges business application recursive dax stock portfolio creation example technical fundamental analysis example continued effect information age influence financial performance conclusion introduction classification regression tree cart classification regression tree relatively new method discriminant analysis developped group american scientist last year discriminant analysis aim cart classify group observation single observation subset know class compare classical parametric discriminant analysis cart offer number particular benefit like high degree result interpretability high precision fast computation cart non parametric tool discriminant analysis design represent decision rule form call binary tree binary tree split learn sample data start root node whole learn sample end relatively homogenous small group observation terminal node class tag forecast value assign hence result tree structure interpret decision rule recall traditional discriminant analysis operates discrete dependent variable class enumerate represent way imagine instead class learn sample numerical data continuous variable moreover connection continuous variable subset continuous disctere variable situation classical discriminant analysis apply cart able produce decision tree conduct classification tree continuous dependent variable call regression tree fact estimator non parametric regression model describe possible reltionship different variable learn sample follow section provide outlook decision tree create challenge arise practical application course number example illustrate power cart financial application cart classification regression tree special method create decision rule distinguish cluster observation determine class new observation particular feature cart decision rule represent via binary tree extremely easy apply rule practice consider follow real life example high risk patient survive least day heart attack admit identify san diego med ical center california basis initial hour data classification rule use three natural question produce see compare classical discriminant analysis precision cart classification even high rule require additional calculation easily use virtually anybody observation class obviously binary variable low risk high risk consider however different situation would interested expect decision tree example amount day patient able survive obviously decision tree change structure approach remains terminal node contain data mean expect number day patient survive get decision tree new patient diagnose assign one two possible group everything basis three simple question obviously example imply assumption relationship set patient characteristic expect durance life relationship estimate use cart apply learn sample information patient long live decision rule explicitely present via binary tree let denote th factor factor space learn sample consist variable let dependent variable binary continuous assume exists mapping whichis estimatedin thestep function class thatis decision tree berepresented appear example obviously estimator unique learn sample data split numerous way hence main question find good way splitting data future observation classify correctly cart advantage learn sample data split order find best node question iterative computer procedure initiate possible split process hence one may question efficiency method nonetheless cart appear well tune procedure numerous benefit cart non parametric important even analyze dependency know nature important perform explorative data mining complex high dimensional structure data strucuture hypothesis available non parametric analysis becomes merely single effective data mining tool moreover building model one make additional assumption concern model error distribution becomes substantial obstacle sample error distribution match require one cart require variable select advance mean give subset variable constitute learn sample cart automatically select significant one sense hence even learn sample hold irrelevant information due measurement error misspecification model choose correct split hence account disturbance automatically cart efficient computational term although possible data split analyze cart architecture flexible account quite quickly result invariate respect monotone transformation independent variable hence relatively easy researcher rescale properly input variable intrepretability result could even high cart handle datasets complex structure becomes important priori information dataset avail able hand peculiarity allows consider big variety possible model specification cart extremely robust effect outlier due data splitting nature decision rule creation possible distiguish datasets different characteristic hence neutralize outlier separate node cart use combination continuous categorical data researcher limited particular class data create model able capture real life effect make prediction accuracy high hence cart easily apply variety field especially financial applica method decision tree work order create decision tree two major question answer firstly proper variable proper question value must determine tree node instance example question patient age great year hence age variable somehow significant particular tree node threshold also determine due particular criterion secondly right configuration size decision tree must found since possible split learn sample data absolutely class homogenous group observation left whereas kind structure obviously lack predictive power let th variable learn sample arbitrary decision tree node pose question like constant since kind restriction univariate data splitting always orthogonal fig ure example similar question may work simulated two dimensional data two question sufficient split data reasonably optimal value determine first let analyze question existance split arbitrary data set always possible filter data way cluster class homogenous datasets answer positive since every coordinate dimensional observation xp space bound follow way ap xp bp arbitrary constant bj hand quite obvious constant unique since always possible find dimensional vector ap xp bp hence learn sample splitting algorithm alowing separate class homoge nous cluster observation always exists since limit case one split sample set single observation solution non unique introduce effective way splitting data hence criterion measure data homogeneity impurity introduce different feasible data split compare accord measure simulated data splitting via orthogonal cluster thus possible find optimal split give tree node measure determine posse good property also important point criterion number feasible split always finite moreover always possible split data question value take variable value learn sample two one dimensional observation obviously filter form cluster obser vation note however similar filter result split hence always sufficient make grid base observation value learn sample order find best give criterion reason robustness symmetric grid usually consider mean different observation use construct filter depend type decision tree precisely type depen dent variable different splitting approach base call impurity function available start classification tree proceed regression one suppose next split determine optimally learn sample cluster absolutely class homogenous group tree represent structure call maximum tree since impossible make additional data split without violate condition base node call parent node observation differ ent class next important question arise tree construction stop splitting determine optimal decision tree configuration different available approach share logic type tree classi fication regression try analyze advantage disadvantage method especially realm financial application impurity measure classification tree let introduce basic learn sample data characteristic operate future suppose observation learn sample overall number observation belonging class define class probability follow proportion observation belonging particular class relative overall number observation let number observation node number observa tions belonging th class node joint probability event observation th class fall node hence condtional probability observation belong node give class compute follow proprtion class node obvious let introduce new measure tree show degree class ho mogeneity give node call characteristic impurity measure able represent class homogeneity indicator give tree node hence help find optimal question value well proper variable number node equestion first define impurity function determine subset unique maximum point symmetric function function satisfy condition call impurity function give function define impurity measure node important point give definition follow possible define multiple impurity measure node denote arbitrary data split give tode call parent node two child node arise represent correspondingly observation subset meeting meeting filter fraction data fall left child node share data parent child node hierarchy hence quality measure well split filter data accord class hetero geneity define arbitrary obviously high value well split since possible reduce data impurity significantly since value represent change data impurity solely due split find optimal question value proper variable question find optimal natural maximize different node hence kind procedure enable one build decision tree configuration maximum tree search optimal value value fact remains constant hence equivalent argmax argmax argmin implicit function result node class homogenous procedure loop decision tree becomes require configuration class assign terminal node use follow rule maxp maximum unique class assign arbitrary pool argument take maximum value momement function define hence propose algorithm fact quite versatile proceed specification worth point follow maximize increment impurity function mean two level decision tree take account whereas part tree influence optimal split procedure characterize locally optimal possible build globally optimal algorithm data splitting imagine locally optimal maximum tree terminal node build globally optimal decision tree one check every possible tree structure number terminal node rocket amount necessary computation moreover estimate many terminal node globally optimal tree fact different hence procedure loop possible combination take account course make amount computation overwhelm thus practice locally optimal variant use one keep mind locally optimal tree necessarily globally optimal probably im portant globally optimal tree necessarily locally optimal financial sphere computation sometimes require do virtually online least conduct quickly speed matter becomes crucial reasonable apply locally optimal procedure enhance precision require credit score portfolio optimization computation speed recedes background sufficient compute power available possible construct globally optimal decision rule nonetheless best knowledge author nowadays commercial version software capable produce globally optimal structure present market let get back question impurity function specification perhaps natural way define data impurity use variance measure assign observation node belonging class others sample variance estimate node observation sum class get call gini index thus gini index consider function turn second degree polynom non negative coefficient convex function hold since conclude function convex since isaconvexfunction get inequality becomes equality case recall since conclude unlessp even best available univariate filter decrease class heterogeneity give way gini index compute becomes obvious impurity measure quite effective first relatively cheap term computation speed second mention gini index relatively robust effect outlier outlier drastically change value hence affected course impurity measure define number different way practical application call twoing rule also consider idea completely different instead look maximization impurity mea way learn sample two class reason algorithm decision rule base twoing criterion able distinguish observation gen eral factor top level tree take account specific data characteristic low level set learn sample class let divide two subset observation belonging get dummy class rest dummy class next step calculate different two dummy class since actually depends value maximize get two step procedure first find maximize second find superclass maximize word idea twoing criterion two find combination super class node impurity increment maximize data two class method provide one big advantage find call strategic node node filter observation way different maximum feasible extent although apply twoing rule may seem desirable especially data big number class another challenge arise namely computational speed let assume learn sample class set split way class data learn sample create combination fortunately result help reduce drastically amount necessary computation proven classification task two class impurity measure arbitrary split superclass determine follow max hence twoing rule easily apply practice well gini index although first criterion work bit slow gini index twoing rule practice section conclude overview two popular impurity measure classification tree look practical issue use two rule consider learn dataset wih observation characterize automobile make type color technical parameter age etc aim build decision tree splitting different car make base feasible relevant parameter look classification tree construct use gini index classification tree construct gini index particular feature node observation belonging one make filter observation strike characteristic separate result decision tree able pick autmobile make quite easily example take critical feature high performance decision tree salford system adver tisement leaflet next tree data somewhat different classification tree construct twoing index instead specify particular car make node application twoing rule result demonstration strategic node question distinguish different car class maximum extent feature vital high dimensional datasets big number class process typical example speech recognition problem every word cod new class twoing rule apply classification tree probably split different word number syllable one one syllable next step similar word characteristic probably take account financial application researcher claim gini index preferable twoing rule impossible course derive absolute dominance theoretically simulation carry dax data show dominance exist least particular datasets optimal size decision tree overview available method moment interested determine best split particular node next perhaps important question determine optimal tree size stop splitting important consider application impurity measure recursively dataset limit case possible stop splitting node observation class advantage splitting data anymore terminal node class homogenous dataset kind tree account data variation every point learn sample flawlessly classify use maximum tree approach fruitful financial practice classification procedure term underspecification overspecification fre quently use direct use maximum tree obvious case overspecifica tion implies learn sample characterize absolutely decision tree without error however case application maximum tree real data result severe error reason maximum classification regression tree account even small insignificant data variation cause random shock measurement error unclassified observation process use maximum tree high probability follow terminal node describe kind disturbance hence recommendation rule obviously bias maximum tree usually try explain purely random effect use factor space learn sample explanation usually spurious moreover nobody could guarantee future random distirbances account way small probabiltity classify new observation follow fundamental part tree case course subset terminal node way regularize event hand small simple decision rule panacea case significant relationship probably could reveal since iteration use split dataset hence decision rule become rough possibly account fundamental data relationship thus special criterion require stop data splitting since tree building mostly dependent easy way involve variable rule data splitting becomes useless term answer quite obvious case usually mean limit case approach maximum tree hence primary criterion could follow form stop data splitting threshold value subset data possible split try recall always finite number split max splitting conduct worth point set equivalent building maximum tree value unknown could determine data simulation unfor tunately drawback method empirical simulation conduct many research show impurity increment frequently non monotone even small decision tree may underparametrized set even small value probably remedy situation cost tree overparametriza tion set high value significantly increase risk explicitely since high value impurity increment precede value small situation become even bad another possible way determine adequate shape decision tree set restriction minimum number observation terminal node terminal node number observation high node also split data still suppose cluster well enough although dynamic always monotone due way question value select problem estimation still remain hand use historical simulation financial data artificial sample sometimes possible estimate less robust level case result well productivity even overall efficiency compare advanced demand method difficulty estimation suggest usage criterion would share described drawback particularly would require priori information like thus procedure call cross validation usually employ cross validation method optimal decision tree prune usually cross validation implies procedure us available data way big part employ training set rest test set process loop different part data become learn training set end datapoint employ member test learn set aim procedure extract maximum information learn sample especially situation data scarceness procedure implement follow way first learn sample randomly divide part refer training set one part test set use training set decision tree construct rest data test set use verify tree quality since actual class response value know learn sample next step pool data use test set becomes part learn set whereas another th part data becomes test set loop stop data point employ way maximum information data extract aim cross validation compare quality tree different configura tions tree different size define training set test set learn sample give classification rule base learn set possible estimate quality follow way ln jn lv test set observation class one iteration estimate since none observation engage construction decision rule possible define cross validation measure tree quality ecv althoughv parameter like still quite important parameter key factor speed precision trade worth note cross validation extremely slow big hence adequate balance require usually value specify give precise task formulation time constraint becomes extremely important imagine online classification system require classify different high frequency stock exchange operation one classification take say several minute time constraint one second cross validation obviously wrong algorithm apply nonetheless usually time constraint extremely tough choice mainly dependent feasible compute power unfortunately small value cross validation estimate unstable since iteration cluster data select randomly number iteration relatively small thus overall estimation result somewhat random empirical simulation show dax stock selection classification problem see next part study individual stock average yield devia tion cause kind randomness course absolutely inadequate since big way increase limit case randomness obviously disappears cost overwhelmingly increase amount computation practical financial application high dimensional datasets significant increase feasible unless su percomputers employ nowadays cross validation industry standard many ap plication different research claim result robustness acceptable level hence conclude cross validation recommend primary method decision tree optimization employ cross validation method possible tree configuration also feasible due computational constraint question arises possible check subtrees maximum tree special key subtrees result introduce appear possible cost complexity function cross validation idea method introduce new measure would able take account tree complexity size estimate number terminal node maximum tree get penalty big size hand able make perfect sample prediction small tree course get much low penalty size predict ability naturally limited optimization procedure base trade criterion could determine best decision tree size define internal misclassification error arbitrary observation node maxp define also internal misclassification tree error set terminal node estimate call internal base solely learn sample contrary cross validation artificially introduces learn test set may seem use tree quality measure sufficient unfortunately consider case maximum tree tree max best configuration discuss maximum tree represent optimal decision rule quite rare case subtree define number terminal node measure max complexity follow cost complexity function could use optimize decision tree size complexity parameter cost component complex tree high number terminal node low time high penalty vice versa alhough infinite number value number subtrees max sulting minimization finite hence prune lead creation max subtrees sequence decrease number terminal node since sequence finite optimal subtree arbitrary remain optimal complexity parameter change becomes new optimal subtree complexity parameter value main question optimal subtree forum give minimize max always exists unique moreover reason computational efficiency usually crucial financial application interested sequence optimal subtrees different value nest root node learn sample case number subtrees check obviously reduce drastically show exists optimal tree sense min min tmax tmax result proof existance also proof uniqueness consider another optimal subtree minimize nest exist accordance second condition idea introduce cost complexity function stage check subset different subtrees optimal subtrees different value start max point define first optimal subtree sequence max size minimum among subtrees cost level get terminal node necessary verify condition max max fulfil node prune process loop extra prune available result tree becomes define node ancestor descendant connect path tree lead decision tree hierarchy example node descendant node descendant although position low since possible connect path node without engage analogously node ancestor ancestor define branch tree subtree base node descen dants example marked area represent branch brach consider separate tree prune branch tree mean delete descendant node denote transform tree example prune branch result new tree branch define internal misclassification estimate branch original tree prune tree set terminal node hence arbitrary node true consider cost complexity misclassification estimate branch single node define single node estimator subtree consist signle node branch estimate branch prefer single node accord cost complexity misclassification estimation critical value become equal critical value determine follow inequality equivalent since get next member optimal subtrees sequence special node call weak link determine purpose function define tt node weak link ming new value define new tree sequence obviuosly define prune branch process loop root node final member sequnce reach multiple weak link detect instance branch prune way possible get sequence optimal subtrees max possible prove sequence increase practically tell implement search algorithm first maximum tree take found weak link detect branch max prune calculate process loop algorithm apply number prune node usually quite significant instance consider follow typical empirical evidence tree typical prune speed tree become small difference number terminal node also get small finally worth mention sequence optimally prune subtrees subset tree might construct use direct method internal misclassi fication estimator minimization give fix number terminal node consider example tree terminal node subtree terminal node low otherwise min tmax impossible definition apply method fold cross validation sequence max optimal tree determine hand frequently point choice tree minimum value ecv always adequate since ecv robust whole range value ecv satisfy ecv ecv small moreover min simpe change random generator seed definitely result change value minimize hence call one standard error empirical rule apply state tree minimize ecv sequence value correspondent tree max select argmaxe denotes sample estimate standard error relevant sample estimator dot line show area value slightly differ mine left edge roughly equivalent terminal node show application one standard error rule use one standard error rule allows achieve robust result also get tree low complexity give error comparable mine regression tree difference moment mainly concentrate decision tree structure cover aspect classification tree creation although regression tree share similar logic important peculiarity cover forget course technical aspect regression tree building recall difference classification regression tree type dependent variable discrete decision tree call classification regression tree decision tree continuous dependent variable example relationship number terminal node main stage classification tree creation remain regression one first foremost criterion work learn sample data efficiently split second stage maximum regression tree efficiently prune well gini index twoing rule discuss previous section assume number class finite hence introduce measure base mainly arbitrary class node since case continuous dependent variable class approach use anymore unless group continuous value effectively substitute artificial class since class anymore maximum regression tree de termined analogously discrete case absolute homogeneity described adequate impurity measure regression tree introduce recall idea gini index becomes quite natural use variance impurity indicator since node data variance easily compute splitting criterion arbitrary node write argmax var var emerge child node course directly dependent choice hence maximum regression tree easily define structure node predict value important point since continuous data much high chance take different value compare discrete one size maximum regression tree usually big maximum regression tree properly define problem get optimally size tree like classification tree maximum regression tree usually suppose upwardly prune help cost complexity function cross validation majority result present apply regression tree well cart practice regression tree classification tree financial application may seem plausible use regression classification tree accordance type dependent variable however empirical result necessarily support point view many occasion classification tree prefer regression one usually happens sufficiently high degree regression tree robustness term classification rule structure mention possible extremely even optimally derive size regression tree big simplest example consider look two linearly separable datasets though obviously separate orthogonal hyperplanes even true decision rule linear inequality regression tree reproduce cost high complexity case learn sample data relatively noisy add obstacle see itcanbeadirect classification like crm customer relationship management insurance class diagnostics etc task may say construct profitable portfolio stock base extra evidence like technical fundamental data last case investor obviously interested future stock price forecast make since stock price continuous variable obvious way use regression tree represent future stock price estimate reason mention characteristic estimate quite far optimal one one way overcome problem somehow substitute regression tree classification one effect probably even big number class modify dataset relatively moderate achieve introduce artificial class class base dependent variable value stock price example three class introduce finally investor get decision long short neutral type rule base current information set one period relative price increment use denotes stock price important value rt rt since new class set rt difened arbitrary non negative constant indicator function word implies expect price growth long position non siginificant future one period price fluctuation neutral position short position constant set accord investor risk aversion prefences historical simulation priori information empirical simulation dax data show approach dramatically improve model performance hence classification tree effectively substitute regres sion structure even case continuous dependent variable cart challanges business application recall model assumption decision tree construct use univariate orthogonal split although case data seem linearly separable lot situation true several reason instance kind effect may stand nonlinear fundamental dependency learn sample data measure error thus lot random disturbancies could occur anyway situation quite difficult construct efficient decision rule let examine case non linearity first decision tree normally try represent kind structure non linear separable data hence conclude decision tree quite effectively capture even non linear dependency mean step approximation real application majority situation follow similar case see decision tree still apply question may possible substitute linear filter non linear univariate multivariate although property result decision rule may become well cost high leave alone technical issue kind algo rithm problem infinite number different variable combination even one restrict certain class function linear still extremely hard determine correct filter particular node filter proper dimension brute force approach probably help low dimensional data sitution similar derivation globally optimal tree supercomputer course solve problem serious issue existance measurement error highly noise data consider follow example data significant random disturbance fundamental relationship situation quite typical financial application majority data highly noise although exists cluster quite homogenous surely represent fundamental dependency cluster appear bottom tree apparently investor would like get explanation data portion really feasible use standard method described kind data pattern surely result final rule small number node tree present use se rule conjunction cost complexity function cross validation typically produce tree constitute node hence information node lose traditional approach necessary appear versatile obvious need develop alternative approach surmount obstacle like alternative method decision tree prune developped author though detailed description overview theoretical property scope ofthiswork traditional solution provide inadequate result recursive dax stock portfolio creation example example continue implication start show successful classification tree apply even case noisy data non linear dependency problem framework follow investor operating dax market posse database technical fundamental data refer dax company also time series historical stock close price investor task create recursive portfolio available dax stock spossibledeviation buy sell hold particular stock transaction lead supplementary cost rate basis point moreover investor supposes certain relationship technical fundamental parameter future stock price idea form class mapping function formally task follow let fix amount cash available investor time period money invest different dax stock time period let yield particular th stock due one period change stock price denote weight th stock portfolio investor construct long short neutral position stock take control variable discrete take one three possible value position set order reduce portfolio risk investor close active position end period hence transaction cost arise price quantity th stock portfolio th period time moreover reduce risk return investor suppose make equally weight portfolio investor assumes relationship information set technical fundamental historical data relevant company determine mapping precise form mapping unknown investor nonetheless set empirical rule concern pattern relationship history stock price movement available give assumption investor problem state max itp itq subset described asaresultofa prioriinvestor mapping carry basis function estimate class binary decision rule represent classification tree hence result investor get decision rule class long short neutral dependent historical technical fundamental data dax company use actual data possible simulate kind strategy unfortunately direct application classical approach result even positive profit model use instead june proper calibration make investment activity imitate ba ing calculate optimal strategy simulation implement matlab mean create program complex active strategy implies solution use capital dynamic record contrary passive strategy stand hold equally weight dax portfolio model performance evaluation could carry important point equally weight dax portfolio dax index different yield dax compute complicate way another comparison ground wealth curve risk free bond yield annually present picture clearly see active strategy outperforms others fact efficient simulation result next two show weekly return distribution active passive strategy active strategy distribution clearly implies positive skewness yield average portfolio yield sharp index since yield standard deviation value express annual term similar study citygroup aim classify stock technological company produce average standard deviation sharp index risk free rate another study jpmorgan aim classification stock information available produce similar result average yield standard deviation sharp index study employ traditional approach classification tree course directly compare result since different stock market involve etc hand timeline hence conclude least last two year propose active strategy clearly outperforms term active strategy weekly return passive strategy weekly return next issue analyze modify cart algorithm affected result purpose implementation classical set modify method allow use special calibration criterion derive best algorithm moment result follow situation modify algorithm acknowledge well compare conjuction cost complexity function cross validation moreover force application classical approach result much low precision almost case hence one conclude modify cart core play prominent role show financial result technical fundamental analysis example continued although performance may vital financial engineering numerous situation classification tree provide another valuable information con tinue work dataset try find variable appear signficant construct tree result affect impression technical fundamental analysis relevance next clue available variable use analysis variable name variable type regulariry estimate close price fund tech day momentum technical day stohastic technical day technical day macd technical day standard error technical day roc technical day trix technical day bv fundamental month cf fundamental month dividend paid fundamental month ebitda fundamental month eps fundamental month number stock outstanding fund tech month sale fundamental month list available variable match different company scale variable name variable type regulariry estimate pt pt fund tech day pt salesit fundamental day pit cfit fundamental day pit epsit fundamental day pit epsit fundamental day pit roe technical day momentum technical day stohastic technical day mait technical day pit macd technical day mait technical day pit roc technical day trix technical day list transform variable use simulation roe return equity estimate use market price pit pi pi roe itisworthexamining tree different angle kind algorithm use supplementary say decision tree non parametric tool could give insight fundamental relationship nowadays different approach evaluate market efficiency hence different group method try reveal fundamental relationship like one described usually one distinguishes technical fundamental analysis method specific assumption tool time widely spread modern financial application particular interest analyze significant variable appear decision tree particular point view since root node filter significant one variable use proxy important factor since simulation perform dynamic period decision rule reevaluate stock possible collect large dataset distribution root node variable refer get characteristic different variable result quite promising root node filter variable distribution distribution technical fundamental variable dominance one methodology another hand feasible investor strategy simulation imply also combination two type variable empirical result show case use type variable simultaneously acknowledge superior fundamental variable together last period yield result best performance finally case use single variable last period yield lead high result although result valid specific choice important specific dataset conclusion make separate application either technical fundamental variable subset appear rational strategy effect information age influence financial performance section analyze another important issue different layer infor mation may affect empirical property investor strategy implication make perform simulation definition information age effect observation measure different time period different impact model forecasting power recent obser vations play important role sample whereas use old data could even deteriorate model prediction dynamic imitation system three major way create learn sample specify first learn sample static size remains constant new observation influence instance set constitute first observation even new observation become available change structure course naive way construct learn sample case may adequate another approach suggests add new observation learn sample date information account sample becomes dynamically expand size constantly grow unless new observation available obviously creation method implies observation significant forecasting power usually apply data dimension low negative influence old observation produce forecast avoid information age effect case learn sample least specify number observation new observation add old one neglect dynamically stable sample create since size constant learn sample information age effect important point best author knowledge majority financial study us first second approach instance state increase learn sample size lead well financial result since actual information account simulation dax stock allow three type learn sample creation optimal one chosen calibration stage evidence frequently different approach use case dynamically stable learn sample acknowledge best dynamically expand one notice static version appear distribution ex post simulation analysis also show case kind calibration decision right hence state one possible reason average simulated financial result probably proper accounting information age effect conclusion classification tree appear quite powerful versatile tool modern finance application empirical result show simulated financial performance average level author modify core cart apply dax data conclude cart mere effective non parametric classification gression tool also powerful mean explorative data analysis excep tional interpretation capability property allow demonstrate power pure financial application like recursive portfolio creation also shed light problem technical fundamental analysis application although lot promising result get study least two major direction towards future research particular interest firstly two step hybrid cart logit model could built first cart analyzes dataset extract valuable information logit model built account possible data peculiarity second direction enhance core cart instance try construct algorithm alowing build efficient tree compare one level optimal one course could quite useful test built system apply different financial datasets leo breiman jerome friedman richard olshen charles stone classifica tion regression tree wadsworth statistic probability series neill brennan method select stock within sector electronic version schroder salomon smith barney equity research europe quantitative strategy april cern classification standard single classification structure cern pur chase inventory cern european organization nuclear research admin istrative information service http ai web cern ch ai projs cc project spec html center tracon automation system nasa ames research center http ctas arc nasa gov project description sw overview html current earth environment analogue extraterrestrial environment gen eral meeting nasa astrobiology institute http nai arc nasa gov library downloads annual abstract book pdf escape danger risk drag bernstein investment research manage ment http www bernstein com public story aspx eugene fama kenneth french multifactor explanation asset pricing anomaly journal finance volume issue mar eugene fama kenneth french cross section expect stock return journal finance volume issue jun juergen franke wolfgang haerdle christian hafner introduction statistic financial market electronic version graham harman priya paramenswaran martine witt share bond cash asset allocation new economy use cart electronic version salomon smith barney equity research australia quantitative analysis september roger hertog mark gordon active management beat index fund bern stein investment research management http www bernstein com public story aspx nid inna kolyshkina richard brooke data mining approach model insurance risk electronic version october tjen sien lim wei yin loh yu shih comparison prediction ac curacy complexity training time thirty three old new classification algorithm electronic version john mahedi balance sheet accrual signal future stock performance overlook data bernstein investment research management http www bernstein com cmsobjectpc pdfs jpm pdf lakshmi seshadri jpmorgan quantitative factor model august stock list electronic version jpmorgan quantitative equity derivative strategy new york august eric sorensen chee ooi keith miller decision tree approach stock selection electronic version salomon smith barney equity research united state global quantitative research november dan steinberg scott cardell hybrid cart logit model classification data mining http www salford system com dan steinberg scott cardell improve data mining new hybrid method present dci database client server world boston